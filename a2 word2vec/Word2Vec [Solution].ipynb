{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "#make sure you got these folder\n",
    "from utils.gradcheck import gradcheck_naive, grad_tests_softmax, grad_tests_negsamp\n",
    "from utils.utils import normalizeRows, softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 2: Word2Vec\n",
    "\n",
    "### Estimated Time: ~10 hours\n",
    "\n",
    "**Quick note**:  This assignment may be overwhelming for some of you.  It may be wise to set aside some significant amount of time so you can slowly go over this assignment.  The objective of this assignment is for you to understand the math behind <code>word2vec</code>, which will be a good fundamental background to understand any other NLP embedding algorithms.  We will also attempt to implement those maths into code to further enhance our understandings.\n",
    "\n",
    "Let’s have a quick refresher on the word2vec algorithm. For full details, you may want to rewatch the zoom video we did in our first two lectures.  \n",
    "\n",
    "The key insight behind word2vec is that *a word is known by the company it keeps*. Concretely, suppose we have a **center** word $c$ and a contextual window. We shall refer to words that lie in this contextual window as **outside words** denoting $o$. For example, in Figure 1 we see that the center word $c$ is *banking*. Since the context window size is 2, the outside words are *turning*, *into*, *crises*, and *as*.\n",
    "\n",
    "The goal of the skip-gram word2vec algorithm is to accurately learn the probability distribution $P(O|C)$. Given a specific word $o$ and a specific word $c$, we want to calculate $P (O = o|C = c)$, which is the probability that word $o$ is an *outside* word for $c$, i.e., the probability that $o$ falls within the contextual window of $c$.\n",
    "\n",
    "<img src = \"img/word2vec.png\" width=400>\n",
    "\n",
    "In word2vec, the conditional probability distribution is given by taking vector dot-products and applying the softmax function:\n",
    "\n",
    "$$P (O = o|C = c) = \\displaystyle\\frac{\\exp({u_o^{T} v_c})}{\\sum_{w \\in V} \\exp({u_w^{T} v_c})}$$\n",
    "\n",
    "Here, $u_o$ is the *outside* vector representing outside word $o$, and $v_c$ is the *center* vector representing center word $c$. To contain these parameters, we have two matrices, $U$ and $V$ . The columns of $U$ are all the *outside* vectors $u_w$. The columns of $V$ are all of the *center* vectors $v_w$. Both $U$ and $V$ contain a vector for every $w \\in \\text{Vocabulary}$.\n",
    "\n",
    "Recall from lectures that, for a single pair of words $c$ and $o$, the loss is given by:\n",
    "\n",
    "$$\\mathbf{J}_\\text{naive-softmax}(v_c, o, U) = -\\log P(O=o |C=c)$$\n",
    "\n",
    "We can view this loss as the cross-entropy2 between the true distribution $y$ and the predicted distribution $\\hat{y}$. Here, both $y$ and $\\hat{y}$ are vectors with length equal to the number of words in the vocabulary. \n",
    "\n",
    "Furthermore, the $k$th entry in these vectors indicates the conditional probability of the $k$th word being an *outside word* for the given $c$. The true empirical distribution $y$ is a one-hot vector with a 1 for the true outside word $o$, and $0$ everywhere else. The predicted distribution $\\hat{y}$ is the probability distribution $P (O|C = c)$ given by our model in above equation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1:  Math behind word2vec\n",
    "\n",
    "### Question 1 (1pt)\n",
    "\n",
    "#### <font color=\"red\">Answer the following questions</font> \n",
    "\n",
    "1. What is $U$ and the shape of $U$?\n",
    "2. What is $V$ and the shape of $V$?\n",
    "3. What is $u_o$ and the shape of $u_o$?\n",
    "4. What is $v_c$ and the shape of $v_c$?\n",
    "5. What is $y$ and the shape of $y$?\n",
    "6. What is $\\hat{y}$ and the shape of $\\hat{y}$?\n",
    "7. What is the numeric range of the softmax function P (O = o|C = c)?\n",
    "8. Why use $\\log$ after the softmax function?\n",
    "\n",
    "Solution:\n",
    "\n",
    "1. $U$ is the outside words matrix of shape <code>(vocab_size, embedding_dim)</code>\n",
    "2. $V$ is the center word matrix of shape <code>(vocab_size, embedding_dim)</code>\n",
    "3. $u_o$ is the embedding vector holding a particular outside word $o$ of shape <code>(embedding_dim, )</code>\n",
    "4. $v_c$ is the embedding vector holding a particular center word $c$ of shape <code>(embedding_dim, )</code> \n",
    "5. $y$ is the true distribution one-hot vector with 1 for the true outside word $o$ and 0 everywhere else; has shape of <code>(vocab_size, )</code>\n",
    "6. $\\hat{y}$ is the predicted distribution one-hot vector given by our model $P(O|C = c)$; has shape of <code>(vocab_size, )</code>\n",
    "7. 0 to 1\n",
    "8. $\\log$ has many nice properties; (1) helps numerically because the product of a large number of small probabilities can easily underflow; this is resolved by computing instead the sum of the log probabilities; (2) cancel out nicely with $\\exp$, (3) we can use $\\log$ because it is a monotically increasing function, thus it won't affect the objective function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2 (1pt)\n",
    "\n",
    "Show that the naive-softmax loss is the same as the cross-entropy loss between $y$ and $\\hat{y}$; i.e., show that\n",
    "\n",
    "$$-\\sum_{w \\in V}y_w \\log(\\hat{y}_w) = -\\log(\\hat{y}_o)$$\n",
    "\n",
    "#### <font color=\"red\">Write your answer here.</font> \n",
    "*(you may need to study latex to write your answers)*\n",
    "\n",
    "Because $y$ is a one-hot encoder vector with zeros everywhere except at the index $w=o$  where $y_o = 1$, the sum is actually just\n",
    "\n",
    "$$-\\sum_{w \\in V} y_w \\log(\\hat{y}_w) = -(y_1 \\log(\\hat{y}_1) + \\cdots + y_o \\log(\\hat{y}_o) + \\cdots + y_{|V|} \\log (\\hat{|V|}) = \\log(\\hat{y}_o)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3 (1pt)\n",
    "\n",
    "Compute the partial derivative of $\\mathbf{J}_{\\text{naive-softmax}}$ with respect to $v_c$.\n",
    "\n",
    "#### <font color=\"red\">Write your answer here.</font> \n",
    "\n",
    "$$\\begin{align*}\n",
    "\\partial \\frac{J_{\\text{naive_softmax}}}{\\partial v_c} &= \\frac{\\partial}{\\partial v_c}[-\\log(\\hat{y}_o)]\\\\\n",
    "&= \\frac{\\partial}{\\partial v_c}[-\\log \\displaystyle\\frac{\\exp({u_o^{T} v_c})}{\\sum_{w \\in V} \\exp({u_w^{T} v_c})}]\\\\\n",
    "&= -\\frac{\\partial}{\\partial v_c}[\\log{\\exp({u_o^{T} v_c})} - \\log(\\sum_{w \\in V} \\exp({u_w^{T} v_c}))]\\\\\n",
    "&= -\\frac{\\partial}{\\partial v_c}[\\log{\\exp({u_o^{T} v_c})}] + \\frac{\\partial}{\\partial v_c} [\\log(\\sum_{w \\in V} \\exp({u_w^{T} v_c}))]\\\\ \n",
    "&= -\\frac{\\partial}{\\partial v_c}[u_o^{T} v_c] + \\frac{\\partial}{\\partial v_c} [\\log(\\sum_{w \\in V} \\exp({u_w^{T} v_c}))]\\\\\n",
    "&= -(u_o) + (\\frac{1}{\\sum_{w \\in V} \\exp({u_w^{T} v_c})}\\sum_{x \\in V}u_x \\dot \\exp({u_x^T v_c}))\\\\\n",
    "&= -u_o + \\sum_{x \\in V}\\frac{\\exp({u_x^T v_c})}{\\sum_{w \\in V} \\exp({u_w^{T} v_c})}u_x\\\\\n",
    "&= -u_o + \\sum_{x \\in V} p(u_x | v_c) u_x \\\\\n",
    "&= -u_o + \\sum_{x \\in V} \\hat{y}_x u_x\n",
    "\\end{align*}$$\n",
    "\n",
    "This says that the gradient of the loss function w.r.t. the center word is equal to the difference between the observed representation of the outside context word and the expected word according to our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4 (1pt)\n",
    "\n",
    "Compute the partial derivative of $\\mathbf{J}_{\\text{naive-softmax}}$ with respect to each of the outside word vectors $u_w$'s.  There will be two cases:  when $w = o$ , the true outside word vector, and $w \\neq o$ for all other words.\n",
    "\n",
    "#### <font color=\"red\">Write your answer here.</font> \n",
    "\n",
    "**Case 1** - the outside word vector is the true context word vector:\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\partial \\frac{J_{\\text{naive_softmax}}}{\\partial u_{w=o}} &= \\frac{\\partial}{\\partial  u_{w=o}}[-\\log(\\hat{y}_o)]\\\\\n",
    "&= \\frac{\\partial}{\\partial u_{w=o}}[-\\log \\displaystyle\\frac{\\exp({u_o^{T} v_c})}{\\sum_{w \\in V} \\exp({u_w^{T} v_c})}]\\\\\n",
    "&= -\\frac{\\partial}{\\partial u_{w=o}}[u_o^Tv_c] + \\frac{\\partial}{\\partial u_{w=o}}[\\log \\sum_{w \\in V} \\exp({u_w^{T} v_c})]\\\\\n",
    "&= -(v_c) + (\\frac{1}{\\sum{w \\in V \\exp({u_w^{T} v_c})}}(\\exp({u_o^{T} v_c}) \\cdot v_c))\\\\\n",
    "&= -(v_c) + (\\frac{\\exp({u_o^{T} v_c})}{\\sum{w \\in V \\exp({u_w^{T} v_c})}}\\cdot v_c)\\\\\n",
    "& = -(v_c) + \\hat{y}_o \\cdot v_c\\\\\n",
    "\\end{align*}$$\n",
    "\n",
    "**Case 2** - the outside word vector is the NOT true context word vector:\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\partial \\frac{J_{\\text{naive_softmax}}}{\\partial u_{w\\neq o}} &= \\frac{\\partial}{\\partial  u_{w \\neq o}}[-\\log(\\hat{y}_o)]\\\\\n",
    "&= \\frac{\\partial}{\\partial u_{w \\neq o}}[-\\log \\displaystyle\\frac{\\exp({u_o^{T} v_c})}{\\sum_{w \\in V} \\exp({u_w^{T} v_c})}]\\\\\n",
    "&= \\frac{\\partial}{\\partial u_{w \\neq o}}[u_o^T v_c] +  \\frac{\\partial}{\\partial u_{w \\neq o}}[\\log \\sum_{w \\in V} \\exp{(u_w^T v_c)}]\\\\\n",
    "&= 0 + (\\frac{1}{\\sum{w \\in V \\exp({u_w^{T} v_c})}}(\\exp({u_{w \\neq o}^{T} v_c}) \\cdot v_c))\\\\\n",
    "&= v_c \\cdot \\hat{y}_{w \\neq o}\n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5 (1pt)\n",
    "\n",
    "Compute the derivatives of the sigmoid function given by \n",
    "\n",
    "$$ g(x) = \\frac{1}{1+e^{-x}} $$\n",
    "\n",
    "#### <font color=\"red\">Write your answer here.</font> \n",
    "\n",
    "The derivative of sigmoid function is\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\frac{dg}{dx} &= \\frac{0(1 + e^{-x}) - (-1)(e^{-x}))}{(1 + e^{-x})^2} \\\\\n",
    "    &= \\frac{e^{-x}}{(1 + e^{-x})^2}  = \\frac{e^{-x} + 1 - 1}{(1 + e^{-x})^2} \\\\\n",
    "    &= \\frac{1}{(1 + e^{-x})} - \\frac{1}{(1 + e^{-x})^2} \\\\\n",
    "    &= \\frac{1}{(1 + e^{-x})} \\big(1 - \\frac{1}{(1 + e^{-x})}\\big)\\\\\n",
    "    &= g(1 - g)\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6 (1pt)\n",
    "\n",
    "Now we shall consider the Negative Sampling loss, which is an alternative to the Naive Softmax loss. Assume that $K$ negative samples (words) are drawn from the vocabulary. For simplicity of notation we shall refer to them as $w_1,w_2,\\cdots,w_K$ and their outside vectors as $u_1,\\cdots,u_K$ For this question, assume that the $K$ negative samples are distinct. In other words, $i \\neq j$ implies $w_i \\neq w_j$ for $i,j \\in \\{1,\\cdots,K\\}$. Note that $o \\notin \\{w_1,\\cdots,w_K\\}$. For a center word $c$ and an outside word $o$, the negative sampling loss function is given by:\n",
    "\n",
    "$$\\mathbf{J}_{\\text{neg-sample}}(v_c, o, U) = -\\log(\\sigma(u_o^Tv_c)) - \\sum_{k=1}^K\\log(\\sigma(-u_k^Tv_c))$$\n",
    "\n",
    "Compute the partial derivatives of $\\mathbf{J}_{\\text{neg-sample}}$ with respect of $v_c, u_o, \\text{ and } u_k$.  Please write your answers in terms of the vectors $u_o, v_c, \\text{and } u_k$.  \n",
    "\n",
    "After this, explain with one sentence why this loss function is much more efficient to compute than the naive-softmax loss.\n",
    "\n",
    "#### <font color=\"red\">Write your answer here.</font> \n",
    "\n",
    "(1) **w.r.t. $v_c$**:\n",
    "\n",
    "$$\\frac{\\partial \\mathbf{J}_{\\text{neg-sample}}}{\\partial v_c} = \\frac{\\partial}{\\partial v_c}[-\\log(\\sigma(u_o^Tv_c))] - \\frac{\\partial}{\\partial v_c}[\\sum_{k=1}^K \\log(\\sigma(-u_k^Tv_c))]$$\n",
    "\n",
    "For the first term, let $S = \\sigma(u_o^Tv_c)$:\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\frac{\\partial}{\\partial v_c}[-\\log(\\sigma(u_o^Tv_c))]  &= (\\frac{\\partial}{\\partial v_c}[-\\log(S)])(\\frac{\\partial}{\\partial v_c}[S])\\\\\n",
    "&= (-\\frac{1}{\\sigma(u_o^Tv_c)})(u_o\\sigma(u_o^Tv_c)(1 - \\sigma(u_o^Tv_c)))\\\\\n",
    "&= u_o(1 - \\sigma(u_o^Tv_c))\n",
    "\\end{align*}$$\n",
    "\n",
    "For the second term, let $S = \\sigma(-u_k^Tv_c)$:\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\frac{\\partial}{\\partial v_c}[\\sum_{k=1}^K \\log(\\sigma(-u_k^Tv_c))] &= \\sum_{k=1}^{K} \\frac{\\partial}{\\partial v_c}[\\log(S)] \\frac{\\partial}{\\partial v_c}[S]\\\\\n",
    "&= \\sum_{k=1}^{K} \\frac{-u_k\\sigma(-u_k^Tv_c)(1 - \\sigma(-u_k^Tv_c))}{\\sigma(-u_k^Tv_c)}\\\\\n",
    "&= -\\sum_{k=1}^{K} u_k(1 - \\sigma(-u_k^Tv_c))\n",
    "\\end{align*}$$\n",
    "\n",
    "Combining the two answers, we get:\n",
    "\n",
    "$$ \\frac{\\partial \\mathbf{J}_{\\text{neg-sample}}}{\\partial v_c} = u_o(1 - \\sigma(u_o^Tv_c)) + \\sum_{k=1}^{K} u_k(1 - \\sigma(-u_k^Tv_c))$$\n",
    "\n",
    "(2) **w.r.t. $u_o$**:\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\frac{\\partial \\mathbf{J}_{\\text{neg-sample}}}{\\partial u_o} &= \\frac{\\partial}{\\partial u_o}[-\\log(\\sigma(u_o^Tv_c))] - \\frac{\\partial}{\\partial u_o}[\\sum_{k=1}^K \\log(\\sigma(-u_k^Tv_c))]\\\\\n",
    "&= \\frac{\\partial}{\\partial u_o}[-\\log(\\sigma(u_o^Tv_c))] - 0\\\\\n",
    "&= -[\\frac{1}{\\sigma(u_o^Tv_c)}][\\sigma(u_o^Tv_c)(1 - \\sigma(u_o^Tv_c))v_c]\\\\\n",
    "&= -v_c(1 - \\sigma(u_o^Tv_c))\n",
    "\\end{align*}$$\n",
    "\n",
    "(3) **w.r.t. $u_k$**:\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\frac{\\partial \\mathbf{J}_{\\text{neg-sample}}}{\\partial u_k} &= \\frac{\\partial}{\\partial u_k}[-\\log(\\sigma(u_o^Tv_c))] - \\frac{\\partial}{\\partial u_k}[\\sum_{k=1}^K \\log(\\sigma(-u_k^Tv_c))]\\\\\n",
    "&= 0 - \\frac{\\partial}{\\partial u_k}[\\sum_{k=1}^K \\log(\\sigma(-u_k^Tv_c))]\\\\\n",
    "&= -\\frac{-v_c\\sigma(-u_k^Tv_c)(1 - \\sigma(-u_k^Tv_c))}{\\sigma(-u_k^Tv_c)}\\\\\n",
    "&= v_c(1 - \\sigma(-u_k^Tv_c))\n",
    "\\end{align*}$$\n",
    "\n",
    "This loss function is much more efficient because it only takes $O(K)$, whereas the naive method requires us to normalize looking all word vectors, taking $O(|V|)$\n",
    "\n",
    "### Question 7 (1pt)\n",
    "\n",
    "Suppose the center word is $c = w_t$ and the context window is $[w_{t−m}, \\cdots, w_{t−1}, w_t, w_{t+1}, \\cdots, w_{t+m}]$, where $m$ is the context window size. Recall that for the skip-gram version of word2vec, the total loss for the context window is:\n",
    "\n",
    "$$\\mathbf{J}_{skip-gram}(v_c, w_{t-m}, \\cdots, w_{t+m}, U) = \\sum_{\\substack{-m \\leq j \\leq m \\\\ j \\neq 0}} \\mathbf{J} (v_c, w_{t+j}, U)$$\n",
    "\n",
    "Here, $\\mathbf{J}(v_c, w_{t+j}, U)$ represents an arbirtary loss term for the center word $c=w_t$ and outside word $w_{t+j}$.  $\\mathbf{J}(v_c, w_{t+j}, U)$ could be $\\mathbf{J}_{\\text{naive-softmax}}$ or $\\mathbf{J}_{\\text{neg-sample}}$ depending on your implementation.\n",
    "\n",
    "Write down three partial derivatives:\n",
    "\n",
    "- (i) $\\displaystyle\\frac{\\partial {\\mathbf{J}_{\\text{skip-gram}}} (v_c, w_{t-m}, \\cdots w_{t+m}, U)}{\\partial U}$\n",
    "- (ii) $\\displaystyle\\frac{\\partial {\\mathbf{J}_{\\text{skip-gram}}} (v_c, w_{t-m}, \\cdots w_{t+m}, U)}{\\partial v_c}$\n",
    "- (iii) $\\displaystyle\\frac{\\partial {\\mathbf{J}_{\\text{skip-gram}}} (v_c, w_{t-m}, \\cdots w_{t+m}, U)}{\\partial v_w} \\text{ where } w \\neq c$\n",
    "\n",
    "Write your answers in terms of $\\partial \\mathbf{J}(v_c, w_{t+j}, U)/\\partial U$ and $\\partial \\mathbf{J}(v_c, w_{t+j}, U)/\\partial v_c$.  This is very simple - don't overthink - each solution should be one line.  We just want you to write so that you are more clear when you implement.\n",
    "\n",
    "#### <font color=\"red\">Write your answer here.</font> \n",
    "\n",
    "- (i) $\\displaystyle\\sum_{-m \\leq j \\leq m, j \\neq 0} \\partial \\mathbf{J}_{skip-gram}(v_c, w_{t+j}, U)/\\partial U$\n",
    "- (ii) $\\displaystyle\\sum_{-m \\leq j \\leq m, j \\neq 0} \\partial \\mathbf{J}_{skip-gram}(v_c, w_{t+j}, U)/\\partial v_c$\n",
    "- (iii) 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2:  Code\n",
    "\n",
    "Now you are done, you are ready to implement <code>word2vec</code>!  Please complete the implementation below.\n",
    "\n",
    "### Question 1 Implement the sigmoid function (1pt)\n",
    "\n",
    "This should be fairly easy.  Recall that sigmoid function is given by:\n",
    "\n",
    "$$ g(x) = \\frac{1}{1+e^{-x}} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Compute the sigmoid function for the input here.\n",
    "    Arguments:\n",
    "    x -- A scalar or numpy array.\n",
    "    Return:\n",
    "    s -- sigmoid(x)\n",
    "    \"\"\"\n",
    "\n",
    "    # ------------------\n",
    "    # Write your implementation here (~1 line).\n",
    "    \n",
    "    s = 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    # ------------------\n",
    "\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_sigmoid():\n",
    "    \"\"\" Test sigmoid function \"\"\"\n",
    "    print(\"=== Sanity check for sigmoid ===\")\n",
    "    assert sigmoid(0) == 0.5\n",
    "    assert np.allclose(sigmoid(np.array([0])), np.array([0.5]))\n",
    "    assert np.allclose(sigmoid(np.array([1,2,3])), np.array([0.73105858, 0.88079708, 0.95257413]))\n",
    "    print(\"Tests for sigmoid passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Sanity check for sigmoid ===\n",
      "Tests for sigmoid passed!\n"
     ]
    }
   ],
   "source": [
    "test_sigmoid() #turn on when you are ready to test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2 Implement the gradient computation of naive softmax (1pt)\n",
    "\n",
    "Here, this is a function that will return the loss, the gradient with respect to $v_c$ and to $U$.\n",
    "\n",
    "1. For **loss**, recall that the loss is given by \n",
    "\n",
    "$$\\mathbf{J}_\\text{naive-softmax}(v_c, o, U) = -\\log P(O=o |C=c)$$\n",
    "\n",
    "where \n",
    "\n",
    "$$P (O = o|C = c) = \\displaystyle\\frac{\\exp({u_o^{T} v_c})}{\\sum_{w \\in V} \\exp({u_w^{T} v_c})}$$\n",
    "\n",
    "*Implementation consideration* - use dot product to avoid unnecessary <code>for</code> loop, i.e., <code>yhat = softmax (outsideVectors @ centerWordVec)</code> should give you the dot product of all outside word vectors with the particular center word vector.  To calculate the loss for a specific $u_o$, simply put the <code>outsideWordIdx</code> as index after the softmax function.   For the softmax function, we have provided so please use it. Last, make sure that the loss is simply a scalar, i.e., shape of (1, ).\n",
    "\n",
    "2. For **gradient with respect to $v_c$**, the gradient that you have calculated should be something like this:\n",
    "\n",
    "$$\\partial \\frac{J_{\\text{naive_softmax}}}{\\partial v_c} = -u_o + \\sum_{x \\in V} \\hat{y}_x u_x$$\n",
    "\n",
    "*Implementation consideration* - since the shape of $v_c$ is <code>(embedding_dim, )</code>, its gradient will also has the same shape.  For people who is struggling, it should look something like this <code>-trueOutsideVec + np.sum(outsideVectors * y_hat, axis=0)</code> where <code>trueOutsideVec</code> is simply <code>outsideVectors[outsideWordIdx]</code>\n",
    "\n",
    "3. For **gradient with respect to $U$**, the gradient for true outside vector that you have calculated should be something like this:\n",
    "\n",
    "$$\\partial \\frac{J_{\\text{naive_softmax}}}{\\partial u_{w=o}} = -(v_c) + \\hat{y}_o \\cdot v_c$$\n",
    "\n",
    "For not true outside vector, it is quite similar\n",
    "\n",
    "$$v_c \\cdot \\hat{y}_{w \\neq o}$$\n",
    "\n",
    "*Implementation consideration* - note that the equation above is simply for one outside word, anyhow, as long as you use dot product, it will handle everything for you, i.e., <code>gradOutsideVecs = np.dot(y_hat, centerWordVec[:, np.newaxis].T)</code> should give you the gradient for all words except the true outside word vector.  By further subtracting it like this <code>gradOutsideVecs[outsideWordIdx] -= centerWordVec</code>, you will obtain the gradient for the true outside word vector.  Similarly above, since the shape of $U$ is <code>(vocab_size, embedding_dim)</code>, its gradient will also has the same shape.\n",
    "\n",
    "Last, you can run <code>test_naiveSoftmaxLossAndGradient()</code> to see whether your work can pass the test.  Note that gradient checking is a sanity test that only checks whether the gradient and loss values produced by your implementation are consistent with each other. Gradient check passing on its own doesn’t guarantee that you have the correct gradients. It will pass, for example, if both the loss and gradient values produced by your implementation are 0s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naiveSoftmaxLossAndGradient(\n",
    "    centerWordVec,\n",
    "    outsideWordIdx,\n",
    "    outsideVectors,\n",
    "    dataset\n",
    "):\n",
    "    \"\"\" Naive Softmax loss & gradient function for word2vec models\n",
    "\n",
    "    Implement the naive softmax loss and gradients between a center word's \n",
    "    embedding and an outside word's embedding. This will be the building block\n",
    "    for our word2vec models. For those unfamiliar with numpy notation, note \n",
    "    that a numpy ndarray with a shape of (x, ) is a one-dimensional array, which\n",
    "    you can effectively treat as a vector with length x.\n",
    "\n",
    "    Arguments:\n",
    "    centerWordVec -- numpy ndarray, center word's embedding\n",
    "                    in shape (embedding_dim, )\n",
    "                    (v_c in our part 1)\n",
    "    outsideWordIdx -- integer, the index of the outside word\n",
    "                    (o of u_o in our part 1)\n",
    "    outsideVectors -- outside vectors is\n",
    "                    in shape (vocab_size, embedding_dim) \n",
    "                    for all words in vocab (tranpose of U in our part 1)\n",
    "    dataset -- needed for negative sampling, unused here.\n",
    "\n",
    "    Return:\n",
    "    loss -- naive softmax loss\n",
    "    gradCenterVec -- the gradient with respect to the center word vector\n",
    "                     in shape (embedding_dim, )\n",
    "                     (dJ / dv_c in part 1)\n",
    "    gradOutsideVecs -- the gradient with respect to all the outside word vectors\n",
    "                    in shape (vocab_size, embedding_dim) \n",
    "                    (dJ / dU)\n",
    "    \"\"\"\n",
    "    \n",
    "    # ------------------\n",
    "    # Write your implementation here.\n",
    "    ### Please use the provided softmax function\n",
    "    \n",
    "    \n",
    "    scores = outsideVectors @ centerWordVec  # UT @ v_c : [vocab_size, embed] @ [embed, 1] = [vocab_size, ]\n",
    "    y_hat = softmax(scores)[:, np.newaxis]  #y_hat: [vocab_size, 1]\n",
    "    loss = float(-np.log(y_hat[outsideWordIdx])) #naive-softmax loss: scalar (1, )\n",
    "\n",
    "    #dJ / dv_c : [embed, ]\n",
    "    trueOutsideVec = outsideVectors[outsideWordIdx] #trueOutsideVec:  [embed, ]\n",
    "    gradCenterVec = -trueOutsideVec + np.sum(outsideVectors * y_hat, axis=0) # [embed, ] + [embed, ] = [embed, ]  (refer to broadcasting rule if you don't understand here)\n",
    "\n",
    "    #dJ / dU : [vocab_size, embed]\n",
    "    gradOutsideVecs = np.dot(y_hat, centerWordVec[:, np.newaxis].T) # y_hat @ centerWordVec : [vocab_size, 1] @ [1, embed] = [vocab_size, embed]\n",
    "    gradOutsideVecs[outsideWordIdx] -= centerWordVec #[vocab_size, embed]\n",
    "    \n",
    "    \n",
    "    # ------------------\n",
    "\n",
    "    return loss, gradCenterVec, gradOutsideVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_naiveSoftmaxLossAndGradient():\n",
    "    \"\"\" Test naiveSoftmaxLossAndGradient \"\"\"\n",
    "    dataset, dummy_vectors, dummy_tokens = getDummyObjects()\n",
    "\n",
    "    print(\"==== Gradient check for naiveSoftmaxLossAndGradient ====\")\n",
    "    def temp(vec):\n",
    "        loss, gradCenterVec, gradOutsideVecs = naiveSoftmaxLossAndGradient(vec, 1, dummy_vectors, dataset)\n",
    "        return loss, gradCenterVec\n",
    "    gradcheck_naive(temp, np.random.randn(3), \"naiveSoftmaxLossAndGradient gradCenterVec\")\n",
    "\n",
    "    centerVec = np.random.randn(3)\n",
    "    def temp(vec):\n",
    "        loss, gradCenterVec, gradOutsideVecs = naiveSoftmaxLossAndGradient(centerVec, 1, vec, dataset)\n",
    "        return loss, gradOutsideVecs\n",
    "    gradcheck_naive(temp, dummy_vectors, \"naiveSoftmaxLossAndGradient gradOutsideVecs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDummyObjects():\n",
    "    \"\"\" Helper method for naiveSoftmaxLossAndGradient and negSamplingLossAndGradient tests \"\"\"\n",
    "\n",
    "    def dummySampleTokenIdx():\n",
    "        return random.randint(0, 4)\n",
    "\n",
    "    def getRandomContext(C):\n",
    "        tokens = [\"a\", \"b\", \"c\", \"d\", \"e\"]\n",
    "        return tokens[random.randint(0,4)], \\\n",
    "            [tokens[random.randint(0,4)] for i in range(2*C)]\n",
    "\n",
    "    dataset = type('dummy', (), {})()\n",
    "    dataset.sampleTokenIdx = dummySampleTokenIdx\n",
    "    dataset.getRandomContext = getRandomContext\n",
    "\n",
    "    random.seed(31415)\n",
    "    np.random.seed(9265)\n",
    "    dummy_vectors = normalizeRows(np.random.randn(10,3))\n",
    "    dummy_tokens = dict([(\"a\",0), (\"b\",1), (\"c\",2),(\"d\",3),(\"e\",4)])\n",
    "\n",
    "    return dataset, dummy_vectors, dummy_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Gradient check for naiveSoftmaxLossAndGradient ====\n",
      "Gradient check passed!. Read the docstring of the `gradcheck_naive` method in utils.gradcheck.py to understand what the gradient check does.\n",
      "Gradient check passed!. Read the docstring of the `gradcheck_naive` method in utils.gradcheck.py to understand what the gradient check does.\n"
     ]
    }
   ],
   "source": [
    "test_naiveSoftmaxLossAndGradient() #turn on when you are ready to test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3 Implement the gradient computation using negative sampling loss (1pt)\n",
    "\n",
    "1. For **loss**, recall that the negative sampling loss is\n",
    "\n",
    "$$\\mathbf{J}_{\\text{neg-sample}}(v_c, o, U) = -\\log(\\sigma(u_o^Tv_c)) - \\sum_{k=1}^K\\log(\\sigma(-u_k^Tv_c))$$\n",
    "\n",
    "*Coding implementation*:  indices are given where the first index belongs to the true outside word, while the remaining $K$ number of indices belong to the negative samples.  For negative sampling, we have provided the function <code>getNegativeSamples</code> so please use it.  One good way to do this is first to calculate the dot product between all relevant outside word vectors within the selected indices and the center word vector like this <code>scores = (outsideVectors[indices] @ centerWordVec)[:, np.newaxis]</code>.  Then for the left side of the equation, use <code>scores[0]</code> as part of the calculation, and for the right side, use <code>-scores[1:]</code>.  The remaining should be easy, applying the already implemented <code>sigmoid</code> function, and <code>log</code> and <code>np.sum</code> accordingly.  Final reminder that the loss is of scalar (1, ) shape.\n",
    "\n",
    "2. For **gradient with respect to $v_c$**, the gradient that you have calculated should be something like this:\n",
    "\n",
    "$$ \\frac{\\partial \\mathbf{J}_{\\text{neg-sample}}}{\\partial v_c} = u_o(1 - \\sigma(u_o^Tv_c)) + \\sum_{k=1}^{K} u_k(1 - \\sigma(-u_k^Tv_c))$$\n",
    "\n",
    "*Coding implementation*: for the left side of the equation, you may want to use <code>outsideVectors[outsideWordIdx]</code>, and for the right side of the equation, use <code>outsideVectors[negSampleWordIndices]</code>.  Other than that, this should be fairly simple.  Remind that the output shape is <code>(embedding_dim, )</code>\n",
    "\n",
    "3. For **gradient with respect to $U$**, there are two parts, the gradient for true outside vector that you have calculated should be something like this:\n",
    "\n",
    "$$\\frac{\\partial \\mathbf{J}_{\\text{neg-sample}}}{\\partial u_o} = -v_c(1 - \\sigma(u_o^Tv_c))$$\n",
    "\n",
    "The gradient for negative vector that you have calculated should be something like this:\n",
    "\n",
    "$$\\frac{\\partial \\mathbf{J}_{\\text{neg-sample}}}{\\partial u_k} = v_c(1 - \\sigma(-u_k^Tv_c))$$\n",
    "\n",
    "*Coding implementation*:  Both of the gradient should be simple to implement using the indexing approach we have done before.  There is some technicality, i.e., the same word may be negatively sampled multiple times. For example if an outside word is sampled twice, you shall have to double count the gradient with respect to this word. Thrice if it was sampled three times, and so forth.  A good way to do this is to first count the occurrences of indices like this:  <code>indexCount = np.bincount(indices)[:, np.newaxis]</code>, then loop through all distinct indices and multiply the gradients with the number of occurences like this: <code>for i in np.unique(indices): gradOutsideVecs[i] *= indexCount[i]</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we have provided the function for getting negative samples\n",
    "def getNegativeSamples(outsideWordIdx, dataset, K):\n",
    "    \"\"\" Samples K indexes which are not the outsideWordIdx \"\"\"\n",
    "\n",
    "    negSampleWordIndices = [None] * K\n",
    "    for k in range(K):\n",
    "        newidx = dataset.sampleTokenIdx()\n",
    "        while newidx == outsideWordIdx:\n",
    "            newidx = dataset.sampleTokenIdx()\n",
    "        negSampleWordIndices[k] = newidx\n",
    "    return negSampleWordIndices #[K, ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def negSamplingLossAndGradient(\n",
    "    centerWordVec,\n",
    "    outsideWordIdx,\n",
    "    outsideVectors,\n",
    "    dataset,\n",
    "    K=10\n",
    "):\n",
    "    \"\"\" Negative sampling loss function for word2vec models\n",
    "\n",
    "    Implement the negative sampling loss and gradients for a centerWordVec\n",
    "    and a outsideWordIdx word vector as a building block for word2vec\n",
    "    models. K is the number of negative samples to take.\n",
    "\n",
    "    Note: The same word may be negatively sampled multiple times. For\n",
    "    example if an outside word is sampled twice, you shall have to\n",
    "    double count the gradient with respect to this word. Thrice if\n",
    "    it was sampled three times, and so forth.\n",
    "\n",
    "    Arguments/Return Specifications: same as naiveSoftmaxLossAndGradient\n",
    "    \"\"\"\n",
    "\n",
    "    # Negative sampling of words is done for you.\n",
    "    negSampleWordIndices = getNegativeSamples(outsideWordIdx, dataset, K)\n",
    "    indices = [outsideWordIdx] + negSampleWordIndices\n",
    "\n",
    "    # ------------------\n",
    "    # Write your implementation here\n",
    "    \n",
    "    scores = (outsideVectors[indices] @ centerWordVec)[:, np.newaxis]  #u_ok @ v_c = [K+1, embed] @ [embed, ] = [K + 1, ] => (newaxis) => [K + 1, 1]\n",
    "    trueOutsideWordScore = scores[0] #scalar\n",
    "    probFromCorpus = sigmoid(trueOutsideWordScore) #scalar in the range of 0 to 1\n",
    "    negSampleScores = -scores[1:] #[K, 1]\n",
    "    probNotFromCorpus = sigmoid(negSampleScores) #[K, 1] in the range of 0 to 1\n",
    "\n",
    "    loss = float(-np.log(probFromCorpus) -\n",
    "                 np.sum(np.log(probNotFromCorpus), axis=0)) #scalar\n",
    "    \n",
    "    #dJ / dv_c : [embed, ]\n",
    "    gradFromCorpus = -outsideVectors[outsideWordIdx] * (1 - probFromCorpus) #[embed, ] * (1 - [1, ]) => #[embed, ] using broadcasting rule\n",
    "    gradNotFromCorpus = np.sum(\n",
    "        outsideVectors[negSampleWordIndices] * (1 - probNotFromCorpus), axis=0) #sum ( [K, embed] * (1 - [K, 1])  , axis = 0) => [embed, ]\n",
    "    gradCenterVec = gradFromCorpus + gradNotFromCorpus # [embed, ] + [embed, ] = [embed, ]\n",
    "    \n",
    "    #dJ / dU : [vocab_size, embed]\n",
    "    gradOutsideVecs = np.zeros(outsideVectors.shape) #[vocab_size, embed]\n",
    "    gradOutsideVecs[outsideWordIdx] = -centerWordVec * (1 - probFromCorpus) #[embed, ] * scalar = [embed, ]\n",
    "    gradOutsideVecs[negSampleWordIndices] += centerWordVec * (1 - probNotFromCorpus) # [K, embed] + ([embed, ] * [K, 1])  => [K, embed] + [K, embed]  => [K, embed] adding all gradients of the negative samples\n",
    "    \n",
    "    # Factor in repeatedly drawn negative samples.\n",
    "    indexCount = np.bincount(indices)[:, np.newaxis]  #count occurrences\n",
    "    for i in np.unique(indices):\n",
    "        gradOutsideVecs[i] *= indexCount[i]  #multiply the gradient according to the occurences\n",
    "\n",
    "    # ------------------\n",
    "\n",
    "    return loss, gradCenterVec, gradOutsideVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_negSamplingLossAndGradient():\n",
    "    \"\"\" Test negSamplingLossAndGradient \"\"\"\n",
    "    dataset, dummy_vectors, dummy_tokens = getDummyObjects()\n",
    "\n",
    "    print(\"==== Gradient check for negSamplingLossAndGradient ====\")\n",
    "    def temp(vec):\n",
    "        loss, gradCenterVec, gradOutsideVecs = negSamplingLossAndGradient(vec, 1, dummy_vectors, dataset)\n",
    "        return loss, gradCenterVec\n",
    "    gradcheck_naive(temp, np.random.randn(3), \"negSamplingLossAndGradient gradCenterVec\")\n",
    "\n",
    "    centerVec = np.random.randn(3)\n",
    "    def temp(vec):\n",
    "        loss, gradCenterVec, gradOutsideVecs = negSamplingLossAndGradient(centerVec, 1, vec, dataset)\n",
    "        return loss, gradOutsideVecs\n",
    "    gradcheck_naive(temp, dummy_vectors, \"negSamplingLossAndGradient gradOutsideVecs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Gradient check for negSamplingLossAndGradient ====\n",
      "Gradient check passed!. Read the docstring of the `gradcheck_naive` method in utils.gradcheck.py to understand what the gradient check does.\n",
      "Gradient check passed!. Read the docstring of the `gradcheck_naive` method in utils.gradcheck.py to understand what the gradient check does.\n"
     ]
    }
   ],
   "source": [
    "test_negSamplingLossAndGradient() #turn on when you are ready to test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: Implement the skipgram model (5pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def skipgram(currentCenterWord, windowSize, outsideWords, word2Ind,\n",
    "             centerWordVectors, outsideVectors, dataset,\n",
    "             word2vecLossAndGradient=naiveSoftmaxLossAndGradient):\n",
    "    \"\"\" Skip-gram model in word2vec\n",
    "\n",
    "    Implement the skip-gram model in this function.\n",
    "\n",
    "    Arguments:\n",
    "    currentCenterWord -- a string of the current center word\n",
    "    windowSize -- integer, context window size\n",
    "    outsideWords -- list of no more than 2*windowSize strings, the outside words\n",
    "    word2Ind -- a dictionary that maps words to their indices in\n",
    "              the word vector list\n",
    "    centerWordVectors -- center word vectors (as rows) is in shape \n",
    "                        (num words in vocab, word vector length) \n",
    "                        for all words in vocab (V in pdf handout)\n",
    "    outsideVectors -- outside vectors is in shape \n",
    "                        (num words in vocab, word vector length) \n",
    "                        for all words in vocab (transpose of U in the pdf handout)\n",
    "    word2vecLossAndGradient -- the loss and gradient function for\n",
    "                               a prediction vector given the outsideWordIdx\n",
    "                               word vectors, could be one of the two\n",
    "                               loss functions you implemented above.\n",
    "\n",
    "    Return:\n",
    "    loss -- the loss function value for the skip-gram model\n",
    "            (J in the pdf handout)\n",
    "    gradCenterVec -- the gradient with respect to the center word vector\n",
    "                     in shape (word vector length, )\n",
    "                     (dJ / dv_c in the pdf handout)\n",
    "    gradOutsideVecs -- the gradient with respect to all the outside word vectors\n",
    "                    in shape (num words in vocab, word vector length) \n",
    "                    (dJ / dU)\n",
    "    \"\"\"\n",
    "\n",
    "    loss = 0.0\n",
    "    gradCenterVecs = np.zeros(centerWordVectors.shape)\n",
    "    gradOutsideVectors = np.zeros(outsideVectors.shape)\n",
    "\n",
    "    # ------------------\n",
    "    # Write your implementation here\n",
    "    \n",
    "    \n",
    "    currCenterWordIdx = word2Ind[currentCenterWord]\n",
    "    centerWordVec = centerWordVectors[currCenterWordIdx]\n",
    "\n",
    "    for outsideWord in outsideWords:\n",
    "        outsideWordIdx = word2Ind[outsideWord]\n",
    "        currLoss, currGradCenter, currGradOutside = word2vecLossAndGradient(\n",
    "            centerWordVec, outsideWordIdx, outsideVectors, dataset)\n",
    "        loss += currLoss\n",
    "        gradCenterVecs += currGradCenter\n",
    "        gradOutsideVectors += currGradOutside\n",
    "\n",
    "    # Clear out all non-center word gradients.\n",
    "    gradCenterVecs[np.arange(gradCenterVecs.shape[0]) != currCenterWordIdx] = 0\n",
    "    \n",
    "\n",
    "    # ------------------\n",
    "    \n",
    "    return loss, gradCenterVecs, gradOutsideVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_skipgram():\n",
    "    \"\"\" Test skip-gram with naiveSoftmaxLossAndGradient \"\"\"\n",
    "    dataset, dummy_vectors, dummy_tokens = getDummyObjects()\n",
    "\n",
    "    print(\"==== Gradient check for skip-gram with naiveSoftmaxLossAndGradient ====\")\n",
    "    gradcheck_naive(lambda vec: word2vec_sgd_wrapper(\n",
    "        skipgram, dummy_tokens, vec, dataset, 5, naiveSoftmaxLossAndGradient),\n",
    "        dummy_vectors, \"naiveSoftmaxLossAndGradient Gradient\")\n",
    "    grad_tests_softmax(skipgram, dummy_tokens, dummy_vectors, dataset)\n",
    "\n",
    "    print(\"==== Gradient check for skip-gram with negSamplingLossAndGradient ====\")\n",
    "    gradcheck_naive(lambda vec: word2vec_sgd_wrapper(\n",
    "        skipgram, dummy_tokens, vec, dataset, 5, negSamplingLossAndGradient),\n",
    "        dummy_vectors, \"negSamplingLossAndGradient Gradient\")\n",
    "    grad_tests_negsamp(skipgram, dummy_tokens, dummy_vectors, dataset, negSamplingLossAndGradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2vec_sgd_wrapper(word2vecModel, word2Ind, wordVectors, dataset,\n",
    "                         windowSize,\n",
    "                         word2vecLossAndGradient=naiveSoftmaxLossAndGradient):\n",
    "    batchsize = 50\n",
    "    loss = 0.0\n",
    "    grad = np.zeros(wordVectors.shape)\n",
    "    N = wordVectors.shape[0]\n",
    "    centerWordVectors = wordVectors[:int(N/2),:]\n",
    "    outsideVectors = wordVectors[int(N/2):,:]\n",
    "    for i in range(batchsize):\n",
    "        windowSize1 = random.randint(1, windowSize)\n",
    "        centerWord, context = dataset.getRandomContext(windowSize1)\n",
    "\n",
    "        c, gin, gout = word2vecModel(\n",
    "            centerWord, windowSize1, context, word2Ind, centerWordVectors,\n",
    "            outsideVectors, dataset, word2vecLossAndGradient\n",
    "        )\n",
    "        loss += c / batchsize\n",
    "        grad[:int(N/2), :] += gin / batchsize\n",
    "        grad[int(N/2):, :] += gout / batchsize\n",
    "\n",
    "    return loss, grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Gradient check for skip-gram with naiveSoftmaxLossAndGradient ====\n",
      "Gradient check passed!. Read the docstring of the `gradcheck_naive` method in utils.gradcheck.py to understand what the gradient check does.\n",
      "======Skip-Gram with naiveSoftmaxLossAndGradient Test Cases======\n",
      "The first test passed!\n",
      "The second test passed!\n",
      "The third test passed!\n",
      "All 3 tests passed!\n",
      "==== Gradient check for skip-gram with negSamplingLossAndGradient ====\n",
      "Gradient check passed!. Read the docstring of the `gradcheck_naive` method in utils.gradcheck.py to understand what the gradient check does.\n",
      "======Skip-Gram with negSamplingLossAndGradient======\n",
      "The first test passed!\n",
      "The second test passed!\n",
      "The third test passed!\n",
      "All 3 tests passed!\n"
     ]
    }
   ],
   "source": [
    "test_skipgram()  #turn on when you are ready to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_word2vec():\n",
    "    \"\"\" Test the two word2vec implementations, before running on Stanford Sentiment Treebank \"\"\"\n",
    "    dataset = type('dummy', (), {})()\n",
    "\n",
    "    def dummySampleTokenIdx():\n",
    "        return random.randint(0, 4)\n",
    "\n",
    "    def getRandomContext(C):\n",
    "        tokens = [\"a\", \"b\", \"c\", \"d\", \"e\"]\n",
    "        return tokens[random.randint(0, 4)], \\\n",
    "            [tokens[random.randint(0, 4)] for i in range(2*C)]\n",
    "    dataset.sampleTokenIdx = dummySampleTokenIdx\n",
    "    dataset.getRandomContext = getRandomContext\n",
    "\n",
    "    random.seed(31415)\n",
    "    np.random.seed(9265)\n",
    "    dummy_vectors = normalizeRows(np.random.randn(10, 3))\n",
    "    dummy_tokens = dict([(\"a\", 0), (\"b\", 1), (\"c\", 2), (\"d\", 3), (\"e\", 4)])\n",
    "\n",
    "    print(\"==== Gradient check for skip-gram with naiveSoftmaxLossAndGradient ====\")\n",
    "    gradcheck_naive(lambda vec: word2vec_sgd_wrapper(\n",
    "        skipgram, dummy_tokens, vec, dataset, 5, naiveSoftmaxLossAndGradient),\n",
    "        dummy_vectors, \"naiveSoftmaxLossAndGradient Gradient\")\n",
    "\n",
    "    print(\"==== Gradient check for skip-gram with negSamplingLossAndGradient ====\")\n",
    "    gradcheck_naive(lambda vec: word2vec_sgd_wrapper(\n",
    "        skipgram, dummy_tokens, vec, dataset, 5, negSamplingLossAndGradient),\n",
    "        dummy_vectors, \"negSamplingLossAndGradient Gradient\")\n",
    "\n",
    "    print(\"\\n=== Results ===\")\n",
    "    print(\"Skip-Gram with naiveSoftmaxLossAndGradient\")\n",
    "\n",
    "    print(\"Your Result:\")\n",
    "    print(\"Loss: {}\\nGradient wrt Center Vectors (dJ/dV):\\n {}\\nGradient wrt Outside Vectors (dJ/dU):\\n {}\\n\".format(\n",
    "        *skipgram(\"c\", 3, [\"a\", \"b\", \"e\", \"d\", \"b\", \"c\"],\n",
    "                  dummy_tokens, dummy_vectors[:5, :], dummy_vectors[5:, :], dataset)\n",
    "    )\n",
    "    )\n",
    "\n",
    "    print(\"Expected Result: Value should approximate these:\")\n",
    "    print(\"\"\"Loss: 11.16610900153398\n",
    "Gradient wrt Center Vectors (dJ/dV):\n",
    " [[ 0.          0.          0.        ]\n",
    " [ 0.          0.          0.        ]\n",
    " [-1.26947339 -1.36873189  2.45158957]\n",
    " [ 0.          0.          0.        ]\n",
    " [ 0.          0.          0.        ]]\n",
    "Gradient wrt Outside Vectors (dJ/dU):\n",
    " [[-0.41045956  0.18834851  1.43272264]\n",
    " [ 0.38202831 -0.17530219 -1.33348241]\n",
    " [ 0.07009355 -0.03216399 -0.24466386]\n",
    " [ 0.09472154 -0.04346509 -0.33062865]\n",
    " [-0.13638384  0.06258276  0.47605228]]\n",
    "    \"\"\")\n",
    "\n",
    "    print(\"Skip-Gram with negSamplingLossAndGradient\")\n",
    "    print(\"Your Result:\")\n",
    "    print(\"Loss: {}\\nGradient wrt Center Vectors (dJ/dV):\\n {}\\n Gradient wrt Outside Vectors (dJ/dU):\\n {}\\n\".format(\n",
    "        *skipgram(\"c\", 1, [\"a\", \"b\"], dummy_tokens, dummy_vectors[:5, :],\n",
    "                  dummy_vectors[5:, :], dataset, negSamplingLossAndGradient)\n",
    "    )\n",
    "    )\n",
    "    print(\"Expected Result: Value should approximate these:\")\n",
    "    print(\"\"\"Loss: 16.15119285363322\n",
    "Gradient wrt Center Vectors (dJ/dV):\n",
    " [[ 0.          0.          0.        ]\n",
    " [ 0.          0.          0.        ]\n",
    " [-4.54650789 -1.85942252  0.76397441]\n",
    " [ 0.          0.          0.        ]\n",
    " [ 0.          0.          0.        ]]\n",
    " Gradient wrt Outside Vectors (dJ/dU):\n",
    " [[-0.69148188  0.31730185  2.41364029]\n",
    " [-0.22716495  0.10423969  0.79292674]\n",
    " [-0.45528438  0.20891737  1.58918512]\n",
    " [-0.31602611  0.14501561  1.10309954]\n",
    " [-0.80620296  0.36994417  2.81407799]]\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Gradient check for skip-gram with naiveSoftmaxLossAndGradient ====\n",
      "Gradient check passed!. Read the docstring of the `gradcheck_naive` method in utils.gradcheck.py to understand what the gradient check does.\n",
      "==== Gradient check for skip-gram with negSamplingLossAndGradient ====\n",
      "Gradient check passed!. Read the docstring of the `gradcheck_naive` method in utils.gradcheck.py to understand what the gradient check does.\n",
      "\n",
      "=== Results ===\n",
      "Skip-Gram with naiveSoftmaxLossAndGradient\n",
      "Your Result:\n",
      "Loss: 11.16610900153398\n",
      "Gradient wrt Center Vectors (dJ/dV):\n",
      " [[ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [-1.26947339 -1.36873189  2.45158957]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]]\n",
      "Gradient wrt Outside Vectors (dJ/dU):\n",
      " [[-0.41045956  0.18834851  1.43272264]\n",
      " [ 0.38202831 -0.17530219 -1.33348241]\n",
      " [ 0.07009355 -0.03216399 -0.24466386]\n",
      " [ 0.09472154 -0.04346509 -0.33062865]\n",
      " [-0.13638384  0.06258276  0.47605228]]\n",
      "\n",
      "Expected Result: Value should approximate these:\n",
      "Loss: 11.16610900153398\n",
      "Gradient wrt Center Vectors (dJ/dV):\n",
      " [[ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [-1.26947339 -1.36873189  2.45158957]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]]\n",
      "Gradient wrt Outside Vectors (dJ/dU):\n",
      " [[-0.41045956  0.18834851  1.43272264]\n",
      " [ 0.38202831 -0.17530219 -1.33348241]\n",
      " [ 0.07009355 -0.03216399 -0.24466386]\n",
      " [ 0.09472154 -0.04346509 -0.33062865]\n",
      " [-0.13638384  0.06258276  0.47605228]]\n",
      "    \n",
      "Skip-Gram with negSamplingLossAndGradient\n",
      "Your Result:\n",
      "Loss: 16.15119285363322\n",
      "Gradient wrt Center Vectors (dJ/dV):\n",
      " [[ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [-4.54650789 -1.85942252  0.76397441]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]]\n",
      " Gradient wrt Outside Vectors (dJ/dU):\n",
      " [[-0.69148188  0.31730185  2.41364029]\n",
      " [-0.22716495  0.10423969  0.79292674]\n",
      " [-0.45528438  0.20891737  1.58918512]\n",
      " [-0.31602611  0.14501561  1.10309954]\n",
      " [-0.80620296  0.36994417  2.81407799]]\n",
      "\n",
      "Expected Result: Value should approximate these:\n",
      "Loss: 16.15119285363322\n",
      "Gradient wrt Center Vectors (dJ/dV):\n",
      " [[ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [-4.54650789 -1.85942252  0.76397441]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]]\n",
      " Gradient wrt Outside Vectors (dJ/dU):\n",
      " [[-0.69148188  0.31730185  2.41364029]\n",
      " [-0.22716495  0.10423969  0.79292674]\n",
      " [-0.45528438  0.20891737  1.58918512]\n",
      " [-0.31602611  0.14501561  1.10309954]\n",
      " [-0.80620296  0.36994417  2.81407799]]\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "test_word2vec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
