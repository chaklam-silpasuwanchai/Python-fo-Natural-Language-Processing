{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26c21c2d",
   "metadata": {},
   "source": [
    "# Chunking Methods\n",
    "Chunking methods refer to various strategies for breaking down large pieces of text into manageable, meaningful segments. These methods are essential for applications in natural language processing, such as summarization, semantic search, and text generation.\n",
    "\n",
    "- Chunk Size - The number of characters you would like in your chunks. 50, 100, 100,000, etc.\n",
    "- Chunk Overlap - The amount you would like your sequential chunks to overlap. This is to try to avoid cutting a single piece of context into multiple pieces. This will create duplicate data across chunks.\n",
    "\n",
    "\n",
    "https://www.chunkviz.com/\n",
    "\n",
    "<img src=\"../figures/chunking.png\" >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e005aacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain\n",
    "!pip install langchain-groq\n",
    "!pip install langchain-openai\n",
    "!pip install langchain-core\n",
    "!pip install langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e98e04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "\n",
    "llm = ChatGroq(model=\"meta-llama/llama-4-scout-17b-16e-instruct\")\n",
    "\n",
    "embedding_model = HuggingFaceBgeEmbeddings(\n",
    "    model_name = \"BAAI/bge-small-en-v1.5\",\n",
    "    model_kwargs = {'device':'cpu'},\n",
    "    encode_kwargs = {'normalize_embeddings':True}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f5b46c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2fc3afd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "One of the most important things I didn't understand about the world when I was a child is the degree to which the returns for performance are superlinear.\n",
    "\n",
    "Teachers and coaches implicitly told us the returns were linear. \"You get out,\" I heard a thousand times, \"what you put in.\" They meant well, but this is rarely true. If your product is only half as good as your competitor's, you don't get half as many customers. You get no customers, and you go out of business.\n",
    "\n",
    "It's obviously true that the returns for performance are superlinear in business. Some think this is a flaw of capitalism, and that if we changed the rules it would stop being true. But superlinear returns for performance are a feature of the world, not an artifact of rules we've invented. We see the same pattern in fame, power, military victories, knowledge, and even benefit to humanity. In all of these, the rich get richer.\n",
    "\n",
    "You can't understand the world without understanding the concept of superlinear returns. And if you're ambitious you definitely should, because this will be the wave you surf on.\n",
    "\n",
    "It may seem as if there are a lot of different situations with superlinear returns, but as far as I can tell they reduce to two fundamental causes: exponential growth and thresholds.\n",
    "\n",
    "The most obvious case of superlinear returns is when you're working on something that grows exponentially. For example, growing bacterial cultures. When they grow at all, they grow exponentially. But they're tricky to grow. Which means the difference in outcome between someone who's adept at it and someone who's not is very great.\n",
    "\n",
    "Startups can also grow exponentially, and we see the same pattern there. Some manage to achieve high growth rates. Most don't. And as a result you get qualitatively different outcomes: the companies with high growth rates tend to become immensely valuable, while the ones with lower growth rates may not even survive.\n",
    "\n",
    "Y Combinator encourages founders to focus on growth rate rather than absolute numbers. It prevents them from being discouraged early on, when the absolute numbers are still low. It also helps them decide what to focus on: you can use growth rate as a compass to tell you how to evolve the company. But the main advantage is that by focusing on growth rate you tend to get something that grows exponentially.\n",
    "\n",
    "YC doesn't explicitly tell founders that with growth rate \"you get out what you put in,\" but it's not far from the truth. And if growth rate were proportional to performance, then the reward for performance p over time t would be proportional to pt.\n",
    "\n",
    "Even after decades of thinking about this, I find that sentence startling.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309a714b",
   "metadata": {},
   "source": [
    "### 1. **Fixed Size (Character) Text Splitting**  \n",
    "This method involves dividing text into chunks based on character count. It’s straightforward and commonly used in text preprocessing.  \n",
    "\n",
    "* **Pros:** Easy & Simple\n",
    "* **Cons:** Very rigid and doesn't take into account the structure of your text\n",
    "* **Best Use Cases:** :Quick preprocessing or exploratory analysis. Word frequency or simple keyword matching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6ff62e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk_size = 200 # Characters\n",
    "\n",
    "# Run through the a range with the length of your text and iterate every chunk_size you want\n",
    "def fixed_size_chunking(text, chunk_size):\n",
    "    return [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]\n",
    "\n",
    "fixed_size_chunks = fixed_size_chunking(text, chunk_size)\n",
    "len(fixed_size_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cdbe720",
   "metadata": {},
   "source": [
    "### 2. **Recursive Character Text Splitting**  \n",
    "Recursive splitting is used when the text is too large for fixed-size chunks. It divides text progressively into smaller chunks, ensuring better segmentation.\n",
    "\n",
    "You can see the default separators for LangChain here. Let's take a look at them one by one.\n",
    "\n",
    "- \"\\n\\n\" - Double new line, or most commonly paragraph breaks\n",
    "- \"\\n\" - New lines\n",
    "- \" \" - Spaces\n",
    "- \"\" - Characters\n",
    "\n",
    "**Configurable Parameters**:  \n",
    "  - `chunk_size`: Size of the final chunks.  \n",
    "  - `chunk_overlap`: Overlap between chunks to maintain context.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6c9ed8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=0)\n",
    "recursive_chunks = splitter.split_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "789ac8aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(recursive_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c1e01eb",
   "metadata": {},
   "source": [
    "## 3. Document Specific Splitting\n",
    "\n",
    "Let's start to handle document types other than normal prose in a .txt. What if you have pictures? or a PDF? or code snippets?\n",
    "\n",
    "Our first two levels wouldn't work great for this so we'll need to find a different tactic.\n",
    "\n",
    "This level is all about making your chunking strategy fit your different data formats. Let's run through a bunch of examples of this in action\n",
    "\n",
    "The Markdown, Python, and JS splitters will basically be similar to Recursive Character, but with different separators.\n",
    "\n",
    "See all of LangChains document splitters [here](https://python.langchain.com/docs/modules/data_connection/document_transformers/text_splitters/code_splitter) and Llama Index ([HTML](https://docs.llamaindex.ai/en/stable/module_guides/loading/node_parsers/modules.html#htmlnodeparser), [JSON](https://docs.llamaindex.ai/en/stable/module_guides/loading/node_parsers/modules.html#jsonnodeparser), [Markdown](https://docs.llamaindex.ai/en/stable/module_guides/loading/node_parsers/modules.html#markdownnodeparser))\n",
    "\n",
    "### Markdown\n",
    "\n",
    "You can see the separators [here](https://github.com/langchain-ai/langchain/blob/9ef2feb6747f5a69d186bd623b569ad722829a5e/libs/langchain/langchain/text_splitter.py#L1175).\n",
    "\n",
    "Separators:\n",
    "* `\\n#{1,6}` - Split by new lines followed by a header (H1 through H6)\n",
    "* ```` ```\\n ```` - Code blocks\n",
    "* `\\n\\\\*\\\\*\\\\*+\\n` - Horizontal Lines\n",
    "* `\\n---+\\n` - Horizontal Lines\n",
    "* `\\n___+\\n` - Horizontal Lines\n",
    "* `\\n\\n` Double new lines\n",
    "* `\\n` - New line\n",
    "* `\" \"` - Spaces\n",
    "* `\"\"` - Character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b4b979a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "markdown_text = \"\"\"\n",
    "# Fun in California\n",
    "\n",
    "## Driving\n",
    "\n",
    "Try driving on the 1 down to San Diego\n",
    "\n",
    "### Food\n",
    "\n",
    "Make sure to eat a burrito while you're there\n",
    "\n",
    "## Hiking\n",
    "\n",
    "Go to Yosemite\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "38432796",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['# Fun in California\\n\\n## Driving',\n",
       " 'Try driving on the 1 down to San Diego',\n",
       " '### Food',\n",
       " \"Make sure to eat a burrito while you're\",\n",
       " 'there',\n",
       " '## Hiking\\n\\nGo to Yosemite']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import MarkdownTextSplitter\n",
    "splitter = MarkdownTextSplitter(chunk_size = 40, chunk_overlap=0)\n",
    "splitter.split_text(markdown_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8c79ee",
   "metadata": {},
   "source": [
    "### 4. **Semantic Chunking**  \n",
    "Semantic chunking uses embeddings to split text based on meaning rather than structure. This ensures that semantically similar content is grouped together.  \n",
    "\n",
    "Embeddings represent the semantic meaning of a string. They don't do much on their own, but when compared to embeddings of other texts you can start to infer the relationship between chunks. I want to lean into this property and explore using embeddings to find clusters of semantically similar texts.\n",
    "\n",
    "The hypothesis is that semantically similar chunks should be held together.\n",
    "\n",
    "I tried a few methods:\n",
    "1) **Heirarchical clustering with positional reward** - I wanted to see how heirarchical clustering of sentence embeddings would do. But because I chose to split on sentences, there was an issue with small short sentences after a long one. You know? (like this last sentenence). They could change the meaning of a chunk, so I added a positional reward and clusters were more likely to form if they were sentences next to each other. This ended up being ok, but tuning the parameters was slow and unoptimal.\n",
    "2) **Find break points between sequential sentences** - Next up I tried a walk method. I started at the first sentence, got the embedding, then compared it to sentence #2, then compared #2 and #3 and so on. I was looking for \"break points\" where embedding distance was large. If it was above a threshold, then I considered it the start of a new semantic section. I originally tried taking embeddings of every sentence, but this turned out to be too noisy. So I ended up taking groups of 3 sentences (a window), then got an embedding, then dropped the first sentence, and added the next one. This worked out a bit better.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6be0ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain-experimental langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa4d60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import FakeEmbeddings\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "\n",
    "\n",
    "embeddings = FakeEmbeddings(size=1352)\n",
    "splitter = SemanticChunker(embeddings)\n",
    "chunks = splitter.split_text(text)\n",
    "print(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a372a2c9",
   "metadata": {},
   "source": [
    "### 5. **Agentic Chunking**  \n",
    "This advanced method uses AI to extract propositions or logical segments from text. It’s ideal for complex documents requiring logical segmentation.  \n",
    "\n",
    "#### Tools:  \n",
    "- `AgenticChunker` or similar frameworks.  \n",
    "\n",
    "#### Example:  \n",
    "\n",
    "**Input Text**:  \n",
    "\"Climate change is a global issue. Reducing emissions is crucial. Governments should enforce stricter laws.\"  \n",
    "\n",
    "**Chunks**:  \n",
    "1. \"Climate change is a global issue.\"  \n",
    "2. \"Reducing emissions is crucial.\"  \n",
    "3. \"Governments should enforce stricter laws.\"  \n",
    "\n",
    "**Use Case**: Extracting logical arguments for debate or analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af879b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install agentic-chunker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37643ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agentic_chunker import AgenticChunker\n",
    "ac = AgenticChunker(llm_client=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfcd409a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ac.pretty_print_chunks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cda555a",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = ac.get_chunks(get_type='list_of_strings')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545be091",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
