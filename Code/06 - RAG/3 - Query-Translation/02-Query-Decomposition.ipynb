{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query Decomposition\n",
    "\n",
    "Sometimes a single question is multiple questions in disguise. For example: “Did Microsoft or Google make more money last year?”. To get to the correct answer for this seemingly simple question, we actually have to break it down: “How much money did Google make last year?” and “How much money did Microsoft make last year?”. Only if we know the answer to these 2 questions can we reason about the final answer.\n",
    "\n",
    "\n",
    "Related Paper \n",
    "- [Least-to-Most Prompting Enables Complex Reasoning in Large Language Models](https://arxiv.org/pdf/2205.10625)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain\n",
    "!pip install langchain-groq\n",
    "!pip install langchain-openai\n",
    "!pip install langchain-core\n",
    "!pip install langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "llm = ChatGroq(model=\"meta-llama/llama-4-scout-17b-16e-instruct\")\n",
    "embedding_model = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"You are a helpful ai assistant who helps users to find relevant products from \\\n",
    "online clothing store by decomposing their query into 4 smaller-queries each of which addresses a \\\n",
    "specific clothing condition which then be used to query of database for retrieving relevant products. \\\n",
    "Carefully evaluate the query and come up with sub-queries that helps in recommending the best clothing items for the user.\n",
    "\n",
    "Generate sub-queries related to: {question} \\n\n",
    "Stirctly respond with only the sub-questions separated by new line\n",
    "\"\"\"\n",
    "prompt_to_decompose = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# Example User Input\n",
    "user_input = \"I am having a baby for the first time and I \\\n",
    "dont know what cloths should I get for her. \\\n",
    "I live in which is both very cold in winters and sunny in summers and also planning to go to outdoor walks with my baby.\"\n",
    "\n",
    "l_to_m_decomposition_chain = (\n",
    "    prompt_to_decompose\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    "    | (lambda x: x.strip().split(\"\\n\"))\n",
    ")\n",
    "sub_queries = l_to_m_decomposition_chain.invoke(user_input)\n",
    "print(sub_queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Here is the question you need to answer for reccomending unique clothing items from our store:\n",
    "\n",
    "\\n --- \\n {question} \\n --- \\n\n",
    "\n",
    "Here is any available background question + answer pairs:\n",
    "\n",
    "\\n --- \\n {q_a_pairs} \\n --- \\n\n",
    "\n",
    "Here is additional context relevant to the question: \n",
    "\n",
    "\\n --- \\n {context} \\n --- \\n\n",
    "\n",
    "Use the above context and any background question + answer pairs to answer the question: \\n {question}\n",
    "\"\"\"\n",
    "\n",
    "decomposition_prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "\n",
    "def create_qa_pairs(question, answer):\n",
    "    return f\"Question: {question}\\nAnswer: {answer}\\n\\n\"\n",
    "\n",
    "q_a_pairs = \"\"\n",
    "\n",
    "for ques in sub_queries:\n",
    "    rag_chain = (\n",
    "        {'context': itemgetter(\"question\") | vstore_retriever,\n",
    "        \"question\": itemgetter(\"question\"),\n",
    "        \"q_a_pairs\": itemgetter(\"q_a_pairs\")}\n",
    "        | decomposition_prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    answer = rag_chain.invoke({\"question\":ques, \"q_a_pairs\":q_a_pairs})\n",
    "    q_a_pair = create_qa_pairs(ques, answer)\n",
    "    q_a_pairs += \"\\n---\\n\" + q_a_pair\n",
    "\n",
    "print(answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
