{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1891f8a0",
   "metadata": {},
   "source": [
    "### MultiVector Retriever\n",
    "It can often be beneficial to store multiple vectors per document. There are multiple use cases where this is beneficial. LangChain has a base MultiVectorRetriever which makes querying this type of setup easy. A lot of the complexity lies in how to create the multiple vectors per document. This notebook covers some of the common ways to create those vectors and use the MultiVectorRetriever.\n",
    "\n",
    "The methods to create multiple vectors per document include:\n",
    "\n",
    "Smaller chunks: split a document into smaller chunks, and embed those (this is ParentDocumentRetriever).\n",
    "Summary: create a summary for each document, embed that along with (or instead of) the document.\n",
    "Hypothetical questions: create hypothetical questions that each document would be appropriate to answer, embed those along with (or instead of) the document.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebaa49ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.vectorstores import FAISS\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094fc563",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = './docs/pdf'\n",
    "documents = [] \n",
    "pdf_list = []\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith('.pdf'):\n",
    "        pdf_list.append(folder_path + filename)\n",
    "\n",
    "pdf_loaders = [PyPDFLoader(pdf) for pdf in pdf_list]\n",
    "for loader in pdf_loaders:\n",
    "    documents.extend(loader.load())\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 700,\n",
    "    chunk_overlap = 100\n",
    ")\n",
    "docs = text_splitter.split_documents(documents) \n",
    "\n",
    "embedding_model = HuggingFaceInstructEmbeddings(\n",
    "        model_name = 'hkunlp/instructor-base',              \n",
    "        model_kwargs = {\n",
    "            'device': torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        },\n",
    "    )\n",
    "\n",
    "db_file_name = 'ml-andrew-ng/'\n",
    "\n",
    "vectordb = FAISS.load_local(\n",
    "        folder_path = db_file_name,\n",
    "        embeddings  = embedding_model\n",
    "    ) \n",
    "\n",
    "retreiver = vectordb.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3328c84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.storage import InMemoryStore\n",
    "# from langchain.document_loaders import TextLoader\n",
    "\n",
    "# The storage layer for the parent documents\n",
    "store = InMemoryStore()\n",
    "id_key = \"doc_id\"\n",
    "# The retriever (empty to start)\n",
    "retriever = MultiVectorRetriever(\n",
    "    vectorstore=vectordb,\n",
    "    docstore=store,\n",
    "    id_key=id_key,\n",
    ")\n",
    "import uuid\n",
    "\n",
    "doc_ids = [str(uuid.uuid4()) for _ in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec9c5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The splitter to use to create smaller chunks\n",
    "child_text_splitter = RecursiveCharacterTextSplitter(chunk_size=400)\n",
    "\n",
    "sub_docs = []\n",
    "for i, doc in enumerate(docs):\n",
    "    _id = doc_ids[i]\n",
    "    _sub_docs = child_text_splitter.split_documents([doc])\n",
    "    for _doc in _sub_docs:\n",
    "        _doc.metadata[id_key] = _id\n",
    "    sub_docs.extend(_sub_docs)\n",
    "\n",
    "retriever.vectorstore.add_documents(sub_docs)\n",
    "retriever.docstore.mset(list(zip(doc_ids, docs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86cf5b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_docs = vectorstore.similarity_search(\"What is Linear Regression\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
