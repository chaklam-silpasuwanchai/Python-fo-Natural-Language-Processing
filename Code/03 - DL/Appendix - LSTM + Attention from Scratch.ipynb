{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "c3b4170d-2358-464d-bdef-adc1b4309607"
   },
   "source": [
    "# Appendix - LSTM + Attention from Scratch\n",
    "\n",
    "In case any students want to deeply understand LSTM.  You can take a look at this.  \n",
    "\n",
    "We shall explore the following\n",
    "\n",
    "    1. LSTM and its variants\n",
    "        - Vanilla LSTM\n",
    "        - Coupled Gate LSTM\n",
    "        - Peephole LSTM\n",
    "        - BiLSTM\n",
    "    2. Attention Mechanism\n",
    "        - General Attention\n",
    "        - Self-Attention\n",
    "        \n",
    "To understand, we code from scratch using pytorch building units.\n",
    "\n",
    "Note that in reality, we don't do from scratch, because the built-in pytorch LSTM has some cudnn optimization that makes it faster than ours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92c31892",
   "metadata": {
    "id": "09aca745-714a-4604-9229-01cba31bf73b",
    "outputId": "057c6666-c88e-40f5-dd11-85d4d281838b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import torchtext\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a6fd6da",
   "metadata": {
    "id": "60359109-acba-4baa-a2fb-438ff10c34a5"
   },
   "outputs": [],
   "source": [
    "#some hyperparameters\n",
    "input_dim = 5000  #just for example\n",
    "hidden_dim = 256\n",
    "embed_dim = 300\n",
    "output_dim = 1\n",
    "\n",
    "batch_size = 32"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "1bb66181-ff2f-4d43-9bba-815ffa49acf5"
   },
   "source": [
    "## 1. LSTM and its variants"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "df43e65d-bd68-48dc-950b-09e53babd8d5"
   },
   "source": [
    "We have learned in class that LSTM was designed to avoid the long term dependency problem as well as to helps with the problem of vanishing and exploding gradients.\n",
    "\n",
    "The key to LSTM is the cell state and the 3 gates to 'protect' and 'control' the cell states.\n",
    "\n",
    "We will now look in to the components inside and implement them line by line :)\n",
    "\n",
    "**The expected shape of LSTM input is SHAPE : (bs, seq_len, input_dim)**\n",
    "\n",
    "For **EACH** time step of our sequence, these are the operations inside LSTM cell.\n",
    "\n",
    "The first step in our LSTM is to decide what information we’re going to throw away from the previous cell state. This decision is made by a sigmoid layer called the “forget gate layer.” It looks at $\\mathbf{h}_{t-1}$ and $\\mathbf{x}_t$, and outputs a number between 0 and 1. A 1 represents “completely keep this” while a 0 represents “completely get rid of this.”\n",
    "\n",
    "$\\mathbf{f}_t = \\sigma(\\mathbf{W}_f \\ \\mathbf{h}_{t-1} + \\mathbf{U}_f \\ \\mathbf{x}_t + \\mathbf{b}_f)$\n",
    "\n",
    "The next step is to decide what new information we’re going to store in the cell state. This has two parts.\n",
    "First, a sigmoid layer called the “input gate layer” decides which values we’ll update.\n",
    "\n",
    "$\\mathbf{i}_t = \\sigma(\\mathbf{W}_i \\ \\mathbf{h}_{t-1} + \\mathbf{U}_i \\ \\mathbf{x}_t + \\mathbf{b}_i)$\n",
    "\n",
    "Next, a tanh layer creates a vector of new 'candidate' values, $\\tilde{\\mathbf{c}}_t$ (aka. $\\mathbf{g}_t$), that could be added to the state. In the next step, we’ll combine these two to create an update to the state.\n",
    "\n",
    "$\\mathbf{g}_t = \\mathbf{tanh} \\ (\\mathbf{W}_g \\ \\mathbf{h}_{t-1} + \\mathbf{U}_g \\ \\mathbf{x}_t  + \\mathbf{b}_g)$\n",
    "\n",
    "It’s now time to update the old cell state, $\\mathbf{c}_{t-1}$, into the new cell state $\\mathbf{c}_t$. The previous steps already decided what to do, we just need to actually do it.\n",
    "We multiply the old state by f_t, forgetting the things we decided to forget earlier.\n",
    "Then we add $\\mathbf{i}_t \\circ \\mathbf{g}_t$. This is the new candidate values, scaled by how much we decided to update each state value.\n",
    "\n",
    "$\\mathbf{c}_t = \\mathbf{f}_t \\circ \\mathbf{c}_{t-1} + \\mathbf{i}_t \\circ \\mathbf{g}_t$\n",
    "\n",
    "Finally, we need to decide what we’re going to output. This output will be based on our cell state, but will be a filtered version. First, we run a sigmoid layer which decides what parts of the cell state we’re going to output. Then, we put the cell state through tanh (to push the values to be between −1 and 1) and multiply it by the output of the sigmoid gate, so that we only output the parts we decided to.\n",
    "\n",
    "$\\mathbf{o}_t = \\sigma( \\mathbf{W}_o \\ \\mathbf{h}_{t-1} + \\mathbf{U}_o \\ \\mathbf{x}_t + \\mathbf{b}_o)$\n",
    "\n",
    "$\\mathbf{h}_t = \\mathbf{o}_t \\circ \\tanh \\ (\\mathbf{c}_t)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "314e380e-5fe5-4931-84ea-13a0e6d1e02d"
   },
   "source": [
    "In conclusion, these are the formula that we need to implement in our LSTM_cell class :\n",
    "\n",
    "$\\mathbf{f}_t = \\sigma(\\mathbf{W}_f \\ \\mathbf{h}_{t-1} + \\mathbf{U}_f \\ \\mathbf{x}_t + \\mathbf{b}_f)$\n",
    "\n",
    "$\\mathbf{i}_t = \\sigma(\\mathbf{W}_i \\ \\mathbf{h}_{t-1} + \\mathbf{U}_i \\ \\mathbf{x}_t + \\mathbf{b}_i)$\n",
    "\n",
    "$\\mathbf{o}_t = \\sigma( \\mathbf{W}_o \\ \\mathbf{h}_{t-1} + \\mathbf{U}_o \\ \\mathbf{x}_t + \\mathbf{b}_o)$\n",
    "\n",
    "$\\mathbf{g}_t = \\tanh \\ (\\mathbf{W}_g \\ \\mathbf{h}_{t-1} + \\mathbf{U}_g \\ \\mathbf{x}_t  + \\mathbf{b}_g)$\n",
    "\n",
    "$\\mathbf{c}_t = \\mathbf{f}_t \\circ \\mathbf{c}_{t-1} + \\mathbf{i}_t \\circ \\mathbf{g}_t$\n",
    "\n",
    "$\\mathbf{h}_t = \\mathbf{o}_t \\circ \\tanh \\ (\\mathbf{c}_t)$\n",
    "\n",
    "where\n",
    "\n",
    "$\\mathbf{h}_{t-1}$ is the hidden state from the previous time step [SHAPE : (bs, hidden_dim)] << no seq_len because it is only at time step t-1\n",
    "\n",
    "$\\mathbf{x}_t$ is the input of the current time step [SHAPE : (bs, hidden_dim)] << no seq_len because it is only at time step t\n",
    "\n",
    "$\\mathbf{W}$ are the weights that would be multiply with the hidden states [SHAPE : (hidden_dim, hidden_dim)]\n",
    "\n",
    "$\\mathbf{U}$ are the weights that would be multiply with the inputs [SHAPE : (input_dim, hidden_dim)]\n",
    "\n",
    "$\\mathbf{b}$ are the biases that would be added to the values before they are passed to sigmoid or tanh [SHAPE : (hidden_dim)]\n",
    "\n",
    "$\\circ$ is the Hadamard product as known as element-wise multiplication\n",
    "\n",
    "** $\\tilde{\\mathbf{c}}_t$ and $\\mathbf{g}_t$ can be used interchangeably"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "1c336be2-bf7f-4736-80b3-ab6442d114d1"
   },
   "source": [
    "### 1.1 Vanilla LSTM from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "4c3f75d5-66c2-461b-9bd7-315a5dce12d9"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "class LSTM_cell(nn.Module):\n",
    "    def __init__(self, input_dim: int, hidden_dim: int):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # initialise the trainable Parameters\n",
    "        self.U_i = nn.Parameter(torch.Tensor(input_dim, hidden_dim))\n",
    "        self.W_i = nn.Parameter(torch.Tensor(hidden_dim, hidden_dim))\n",
    "        self.b_i = nn.Parameter(torch.Tensor(hidden_dim))\n",
    "        \n",
    "        self.U_f = nn.Parameter(torch.Tensor(input_dim, hidden_dim))\n",
    "        self.W_f = nn.Parameter(torch.Tensor(hidden_dim, hidden_dim))\n",
    "        self.b_f = nn.Parameter(torch.Tensor(hidden_dim))\n",
    "        \n",
    "        self.U_g = nn.Parameter(torch.Tensor(input_dim, hidden_dim))\n",
    "        self.W_g = nn.Parameter(torch.Tensor(hidden_dim, hidden_dim))\n",
    "        self.b_g = nn.Parameter(torch.Tensor(hidden_dim))\n",
    "        \n",
    "        self.U_o = nn.Parameter(torch.Tensor(input_dim, hidden_dim))\n",
    "        self.W_o = nn.Parameter(torch.Tensor(hidden_dim, hidden_dim))\n",
    "        self.b_o = nn.Parameter(torch.Tensor(hidden_dim))\n",
    "        \n",
    "        self.init_weights()\n",
    "    \n",
    "    def init_weights(self):\n",
    "        stdv = 1.0 / math.sqrt(self.hidden_dim)\n",
    "        for weight in self.parameters():\n",
    "            weight.data.uniform_(-stdv, stdv)\n",
    "    \n",
    "    def forward(self, x, init_states=None):\n",
    "        \"\"\"\n",
    "        x.shape =  (bs, seq_len, input_dim)\n",
    "        \"\"\"\n",
    "        bs, seq_len, _ = x.shape\n",
    "        output = []\n",
    "        \n",
    "        # initialize the hidden state and cell state for the first time step \n",
    "        if init_states is None:\n",
    "            h_t  = torch.zeros(bs, self.hidden_dim).to(x.device)\n",
    "            c_t  = torch.zeros(bs, self.hidden_dim).to(x.device)\n",
    "        else:\n",
    "            h_t, c_t = init_states\n",
    "        \n",
    "        # For each time step of the input x, do ...\n",
    "        for t in range(seq_len):\n",
    "            x_t = x[:, t, :] # get x data of time step t (SHAPE: (batch_size, input_dim))\n",
    "            \n",
    "            f_t = torch.sigmoid(    h_t @ self.W_f  +  x_t @ self.U_f  +  self.b_f)\n",
    "            i_t = torch.sigmoid(    h_t @ self.W_i  +  x_t @ self.U_i  +  self.b_i)\n",
    "            o_t = torch.sigmoid(    h_t @ self.W_o  +  x_t @ self.U_o  +  self.b_o)\n",
    "            g_t = torch.tanh(       h_t @ self.W_g  +  x_t @ self.U_g   + self.b_g)\n",
    "            c_t = (f_t * c_t) + (i_t * g_t)\n",
    "            h_t = o_t * torch.tanh(c_t)\n",
    "            \n",
    "            output.append(h_t.unsqueeze(0)) # reshape h_t to (1, batch_size, hidden_dim), then append to the list of hidden states\n",
    "\n",
    "        output = torch.cat(output, dim = 0) # concatenate h_t of all time steps into SHAPE :(seq_len, batch_size, hidden_dim)\n",
    "        output = output.transpose(0, 1).contiguous() # just transpose to SHAPE :(batch_size, seq_len, hidden_dim)\n",
    "        return output, (h_t, c_t)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "4a414477-0269-41b3-8454-d29dda4d6bd5"
   },
   "source": [
    "### Run this cell to check LSTM Cell can run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "a8dc8192-84fc-45a4-bb76-5d331fa15798"
   },
   "outputs": [],
   "source": [
    "my_LSTM_cell = LSTM_cell(embed_dim, hidden_dim).to(device)\n",
    "\n",
    "test_data = torch.ones((batch_size, 100, embed_dim)).to(device)\n",
    "output, (h_t, c_t) = my_LSTM_cell(test_data)\n",
    "\n",
    "assert output.shape == torch.Size([32, 100, 256])\n",
    "assert h_t.shape    == torch.Size([32, 256])\n",
    "assert c_t.shape    == torch.Size([32, 256])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "5df451c3-a5ce-47fe-aa55-e787896379f6"
   },
   "source": [
    "Many variants of LSTM have been developed which are slightly different from Vanilla/Basic LSTM that we have just implemented above"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "40ecc7c5-63dc-456c-899a-83ce8cad1cb5",
    "tags": []
   },
   "source": [
    "### 1.2 Peephole LSTM\n",
    "One popular LSTM variant, introduced by Gers & Schmidhuber (2000), is adding “Peephole Connections.” This means that we let all the gate layers look at the cell state.\n",
    "\n",
    "$\\mathbf{f}_t = \\sigma(\\mathbf{W}_f \\ \\mathbf{h}_{t-1} + \\mathbf{U}_f \\ \\mathbf{x}_t + \\mathbf{P}_f \\ \\mathbf{c}_t + \\mathbf{b}_f)$\n",
    "\n",
    "$\\mathbf{i}_t = \\sigma(\\mathbf{W}_i \\ \\mathbf{h}_{t-1} + \\mathbf{U}_i \\ \\mathbf{x}_t + \\mathbf{P}_i \\ \\mathbf{c}_t + \\mathbf{b}_i)$\n",
    "\n",
    "$\\mathbf{o}_t = \\sigma( \\mathbf{W}_o \\ \\mathbf{h}_{t-1} + \\mathbf{U}_o \\ \\mathbf{x}_t + \\mathbf{P}_o \\ \\mathbf{c}_t + \\mathbf{b}_o)$\n",
    "\n",
    "$\\mathbf{g}_t = \\text{tanh} \\ (\\mathbf{W}_g \\ \\mathbf{h}_{t-1} + \\mathbf{U}_g \\ \\mathbf{x}_t  + \\mathbf{b}_g)$\n",
    "\n",
    "$\\mathbf{c}_t = \\mathbf{f}_t \\circ \\mathbf{c}_{t-1} + \\mathbf{i}_t \\circ \\mathbf{g}_t$\n",
    "\n",
    "$\\mathbf{h}_t = \\mathbf{o}_t \\circ \\tanh \\ (\\mathbf{c}_t)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2f77c0e6-7199-410d-a625-804c281caf16"
   },
   "source": [
    "We can see that every gate now has $\\mathbf{c}_t$ as their input. And we also have 3 new parameters; $\\mathbf{P}_f$, $\\mathbf{P}_i$ and $\\mathbf{P}_o$ which has the same shape as $\\mathbf{W}$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "09fae08a-37be-4c58-9d4c-49adc9a9d811"
   },
   "source": [
    "### 1.3 Coupled LSTM\n",
    "\n",
    "Another variation is to use Coupled forget and input gates. Instead of separately deciding what to forget and what we should add new information to, we make those decisions together. We only forget when we’re going to input something in its place. We only input new values to the state when we forget something older. The difference is very simple. The input gate is now $(1 - \\mathbf{f}_t)$\n",
    "\n",
    "\n",
    "$\\mathbf{f}_t = \\sigma(\\mathbf{W}_f \\ \\mathbf{h}_{t-1} + \\mathbf{U}_f \\ \\mathbf{x}_t + \\mathbf{b}_f)$\n",
    "\n",
    "$\\mathbf{i}_t = (1 - \\mathbf{f}_t)$\n",
    "\n",
    "$\\mathbf{o}_t = \\sigma( \\mathbf{W}_o \\ \\mathbf{h}_{t-1} + \\mathbf{U}_o \\ \\mathbf{x}_t + \\mathbf{b}_o)$\n",
    "\n",
    "$\\mathbf{g}_t = \\text{tanh} \\ (\\mathbf{W}_g \\ \\mathbf{h}_{t-1} + \\mathbf{U}_g \\ \\mathbf{x}_t + \\mathbf{b}_g)$\n",
    "\n",
    "$\\mathbf{c}_t = \\mathbf{f}_t \\circ \\mathbf{c}_{t-1} + \\mathbf{i}_t \\circ \\mathbf{g}_t$\n",
    "\n",
    "$\\mathbf{h}_t = \\mathbf{o}_t \\circ \\text{tanh} \\ (\\mathbf{c}_t)$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "c5df99c3-ade2-404c-9c74-a6a91156407d"
   },
   "source": [
    "### Vanilla / Peephole / Coupled LSTM from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "556b9211-1d3b-4803-b6e6-e1422ff46f78"
   },
   "outputs": [],
   "source": [
    "class new_LSTM_cell(nn.Module):\n",
    "    def __init__(self, input_dim: int, hidden_dim: int, lstm_type: str):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.lstm_type = lstm_type\n",
    "        \n",
    "        # initialise the trainable Parameters\n",
    "        self.U_i = nn.Parameter(torch.Tensor(input_dim, hidden_dim))\n",
    "        self.W_i = nn.Parameter(torch.Tensor(hidden_dim, hidden_dim))\n",
    "        self.b_i = nn.Parameter(torch.Tensor(hidden_dim))\n",
    "        \n",
    "        self.U_f = nn.Parameter(torch.Tensor(input_dim, hidden_dim))\n",
    "        self.W_f = nn.Parameter(torch.Tensor(hidden_dim, hidden_dim))\n",
    "        self.b_f = nn.Parameter(torch.Tensor(hidden_dim))\n",
    "        \n",
    "        self.U_g = nn.Parameter(torch.Tensor(input_dim, hidden_dim))\n",
    "        self.W_g = nn.Parameter(torch.Tensor(hidden_dim, hidden_dim))\n",
    "        self.b_g = nn.Parameter(torch.Tensor(hidden_dim))\n",
    "        \n",
    "        self.U_o = nn.Parameter(torch.Tensor(input_dim, hidden_dim))\n",
    "        self.W_o = nn.Parameter(torch.Tensor(hidden_dim, hidden_dim))\n",
    "        self.b_o = nn.Parameter(torch.Tensor(hidden_dim))\n",
    "        \n",
    "        if self.lstm_type == 'peephole' :\n",
    "            self.P_i = nn.Parameter(torch.Tensor(hidden_dim, hidden_dim))\n",
    "            self.P_f = nn.Parameter(torch.Tensor(hidden_dim, hidden_dim))\n",
    "            self.P_o = nn.Parameter(torch.Tensor(hidden_dim, hidden_dim))\n",
    "            \n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        stdv = 1.0 / math.sqrt(self.hidden_dim)\n",
    "        for weight in self.parameters():\n",
    "            weight.data.uniform_(-stdv, stdv)\n",
    "    \n",
    "    def forward(self, x, init_states=None):\n",
    "        bs, seq_len, _ = x.shape\n",
    "        output = []\n",
    "        \n",
    "        # initialize the hidden state and cell state for the first time step \n",
    "        if init_states is None:\n",
    "            h_t  = torch.zeros(bs, self.hidden_dim).to(x.device)\n",
    "            c_t  = torch.zeros(bs, self.hidden_dim).to(x.device)\n",
    "        else:\n",
    "            h_t, c_t = init_states\n",
    "        \n",
    "        # For each time step of the input x, do ...\n",
    "        for t in range(seq_len):\n",
    "            x_t = x[:, t, :] # get x data of time step t (SHAPE: (batch_size, input_dim))\n",
    "            \n",
    "            if self.lstm_type in ['vanilla', 'coupled'] :\n",
    "                f_t = torch.sigmoid(    h_t @ self.W_f  +  x_t @ self.U_f  +  self.b_f)\n",
    "                o_t = torch.sigmoid(    h_t @ self.W_o  +  x_t @ self.U_o  +  self.b_o)\n",
    "                if self.lstm_type == 'vanilla':\n",
    "                    i_t = torch.sigmoid(    h_t @ self.W_i  +  x_t @ self.U_i  +  self.b_i)\n",
    "                if self.lstm_type == 'coupled':\n",
    "                    i_t = (1 - f_t)\n",
    "            if self.lstm_type == 'peephole' :\n",
    "                i_t = torch.sigmoid( h_t @ self.W_i + x_t @ self.U_i + c_t @ self.P_i + self.b_i) # SHAPE: (batch_size, hidden_dim)\n",
    "                f_t = torch.sigmoid( h_t @ self.W_f + x_t @ self.U_f + c_t @ self.P_f + self.b_f) # SHAPE: (batch_size, hidden_dim)\n",
    "                o_t = torch.sigmoid( h_t @ self.W_o + x_t @ self.U_o + c_t @ self.P_o + self.b_o) # SHAPE: (batch_size, hidden_dim)\n",
    "            \n",
    "            g_t = torch.tanh(       h_t @ self.W_g  +  x_t @ self.U_g   + self.b_g)\n",
    "            c_t = (f_t * c_t) + (i_t * g_t)\n",
    "            h_t = o_t * torch.tanh(c_t)\n",
    "            \n",
    "            output.append(h_t.unsqueeze(0)) # reshape h_t to (1, batch_size, hidden_dim), then append to the list of hidden states\n",
    "\n",
    "        output = torch.cat(output, dim = 0) # concatenate h_t of all time steps into SHAPE :(seq_len, batch_size, hidden_dim)\n",
    "        output = output.transpose(0, 1).contiguous() # just transpose to SHAPE :(seq_len, batch_size, hidden_dim)\n",
    "        return output, (h_t, c_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "92273dc5-957c-40b5-98e5-61c64bbcc869"
   },
   "source": [
    "### Run this cell to check if all types of your LSTM Cells can run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "8d11723c-88e2-4c16-bc13-5bf249e379f5"
   },
   "outputs": [],
   "source": [
    "Vanilla_LSTM_cell = new_LSTM_cell(embed_dim, hidden_dim, lstm_type = 'vanilla').to(device)\n",
    "test_data = torch.ones((batch_size, 100, embed_dim)).to(device)\n",
    "output, (h_t, c_t) = Vanilla_LSTM_cell(test_data)\n",
    "assert output.shape == torch.Size([32, 100, 256])\n",
    "assert h_t.shape    == torch.Size([32, 256])\n",
    "assert c_t.shape    == torch.Size([32, 256])\n",
    "\n",
    "Coupled_LSTM_cell = new_LSTM_cell(embed_dim, hidden_dim, lstm_type = 'coupled').to(device)\n",
    "test_data = torch.ones((batch_size, 100, embed_dim)).to(device)\n",
    "output, (h_t, c_t) = Coupled_LSTM_cell(test_data)\n",
    "assert output.shape == torch.Size([32, 100, 256])\n",
    "assert h_t.shape    == torch.Size([32, 256])\n",
    "assert c_t.shape    == torch.Size([32, 256])\n",
    "\n",
    "Peephole_LSTM_cell = new_LSTM_cell(embed_dim, hidden_dim, lstm_type = 'peephole').to(device)\n",
    "test_data = torch.ones((batch_size, 100, embed_dim)).to(device)\n",
    "output, (h_t, c_t) = Peephole_LSTM_cell(test_data)\n",
    "assert output.shape == torch.Size([32, 100, 256])\n",
    "assert h_t.shape    == torch.Size([32, 256])\n",
    "assert c_t.shape    == torch.Size([32, 256])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "6f923e60-2fb7-455d-af05-8aa95a64c816"
   },
   "source": [
    "### 1.4 BiLSTM model for sequence classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7a97361a-4b6a-46e7-9d0a-a19cbb8e614e"
   },
   "source": [
    "We now have the basic variants of LSTM cells. But what about Bidirectional LSTM. How do we implement that?\n",
    "\n",
    "The answer is simple. We create **2 LSTM cells** then pass our normal input to one of them, and pass the **flipped** input to the other. (reverse the order of sequence)\n",
    "\n",
    "Then we take the last hidden state from the 2 LSTM (one would be the hidden state at the last word of the sentence and another at the first word of the sentence) and concatenate them. Like this we have information of the sequence from both directions!\n",
    "\n",
    "Formally these are the formula\n",
    "\n",
    "$\\overrightarrow{\\mathbf{h}}_t = LSTM(\\mathbf{x}_t, \\overrightarrow{\\mathbf{h}}_{t-1})$\n",
    "\n",
    "$\\overleftarrow{\\mathbf{h}}_t = LSTM(\\mathbf{x}_t, \\overleftarrow{\\mathbf{h}}_{t+1})$\n",
    "\n",
    "$\\mathbf{h}_t = \\sigma(\\mathbf{W}_y[\\overrightarrow{\\mathbf{h}}_t ; \\overleftarrow{\\mathbf{h}}_{t}] + \\mathbf{b}_y )$\n",
    "\n",
    "Then we should pass $\\mathbf{h}_t$ to another Linear Layer to get the output for binary classification."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "f07808fe-4bd0-40d7-a88b-f9ebc60ebd07"
   },
   "source": [
    "### biLSTM from Scratch\n",
    "\n",
    "**( Let's use our 'vanilla' LSTM_cell)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "dffa46a8-a5c4-485c-81b2-8f43d6792046"
   },
   "outputs": [],
   "source": [
    "class BiLSTM_model(nn.Module):\n",
    "    def __init__(self, input_dim: int, embed_dim: int, hidden_dim: int, output_dim: int):\n",
    "        super().__init__()\n",
    "        self.num_directions = 2\n",
    "        self.embedding = nn.Embedding(input_dim, embed_dim, padding_idx=pad_idx)\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.forward_lstm   =  new_LSTM_cell(embed_dim, hidden_dim, lstm_type = 'vanilla')\n",
    "        self.backward_lstm  =  new_LSTM_cell(embed_dim, hidden_dim, lstm_type = 'vanilla')\n",
    "        \n",
    "        # These should be torch Parameters\n",
    "        self.W_h = nn.Parameter(torch.Tensor(hidden_dim*self.num_directions, hidden_dim*self.num_directions ))\n",
    "        self.b_h = nn.Parameter(torch.Tensor(hidden_dim*self.num_directions))\n",
    "        \n",
    "        self.fc  = nn.Linear(hidden_dim*self.num_directions, output_dim)\n",
    "    \n",
    "        self.init_weights()\n",
    "    \n",
    "    def init_weights(self):\n",
    "        stdv = 1.0 / math.sqrt(self.hidden_dim)\n",
    "        for weight in self.parameters():\n",
    "            weight.data.uniform_(-stdv, stdv)\n",
    "    \n",
    "    def forward(self, text, text_lengths):\n",
    "        embedded      = self.embedding(text)\n",
    "        embedded_flip =  torch.flip(embedded, [1]) \n",
    "        \n",
    "        output_forward, (hn_forward, cn_forward)    = self.forward_lstm(embedded, init_states=None)\n",
    "        output_backward, (hn_backward, cn_backward) = self.backward_lstm(embedded_flip, init_states=None)\n",
    "        \n",
    "        concat_hn = torch.cat( (hn_forward, hn_backward), dim=1 ) \n",
    "        ht        = torch.sigmoid( concat_hn @ self.W_h + self.b_h)\n",
    "\n",
    "        return self.fc(ht)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "a8d3eace-4d78-4546-9819-9fb30ec29499",
    "tags": []
   },
   "source": [
    "## 2. Attention Mechanism\n",
    "The attention mechanism was first born to help memorize long source sentences in neural machine translation (NMT). Rather than building a single context vector out of the encoder’s last hidden state, the attention mechanism creates shortcuts between the context vector and the entire source input. The weights of these shortcut connections are customizable for each output element.\n",
    "While the context vector has access to the entire input sequence, we don’t need to worry about forgetting. The alignment between the source and target is learned and controlled by the context vector. Essentially the context vector consumes three pieces of information:\n",
    "\n",
    "    - encoder hidden states\n",
    "    - decoder hidden states\n",
    "    - alignment between source and target\n",
    "    \n",
    "This is the same mechanism that we have learned in class, which is actually called 'Cross Attention'.\n",
    "\n",
    "However, in this assignment we are making a classification model so we only have the encoder hidden states and our target would be the class decision."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "e6ddeef0-9bf1-40f6-a01a-d32d849bb150"
   },
   "source": [
    "### 2.1 General Attention Mechanism"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "14acd0fb-3dd2-4528-aa1e-98802543c95f"
   },
   "source": [
    "First, we will be creating an LSTM + General Attention model for classification.\n",
    "\n",
    "Our General Attention mechanism is going to capture how the last encoder hidden state (aka. the 'queries') 'relates' to the other hidden states in the sequence (a.k.a. the 'keys'). ( how much our classification decision is related to each of the hidden states)\n",
    "Then we will scale the output (a.k.a. the 'values') according to the Attention Weights (computed from the Alignment Scores), in order to retain focus on words that are relevant to the query. In doing so, it produces an attention output that we will input to a fully connected layer for the result of our classification task.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2600dd8d-4f52-4901-8427-940777e0f683"
   },
   "source": [
    "**These are the steps we need to implement :**\n",
    "\n",
    "We will pass our data through LSTM first then pass the outputs of LSTM to the General Attention mechanism.\n",
    "    \n",
    "**1. Get the components we need for our Attention Mechanism ('query', 'keys' and 'values')**\n",
    "- Get the last encoder hidden states ($\\mathbf{h}_N$) = last hidden state of last LSTM layer\n",
    "        - Hint : can be found in 'hn'\n",
    "        - Should be of shape [bs, hidden dim * num_directions]\n",
    "        - a.k.a. 'query'\n",
    "- Get the hidden states of every time step from the last layer of LSTM ($\\mathbf{H}$)\n",
    "        - Hint : can be found in 'output' \n",
    "        - Should be of shape  [bs, seq len, hidden_dim * num_directions]\n",
    "        - a.k.a. 'keys'\n",
    "        - This will be matched with our h_t to get the Attention Scores.\n",
    "- Get the hidden states of every time step from the last layer of LSTM ($\\mathbf{H}$)\n",
    "        - Hint : can be found in 'output' \n",
    "        - Should be of shape  [bs, seq len, hidden_dim * num_directions]\n",
    "        - a.k.a. 'values'\n",
    "        - This will be weighted by Attention Weights to get the Context.\n",
    "** In our case, we are implementing Attention in Classification model so our 'keys' and 'values' are the same thing\n",
    "\n",
    "**2.  Calculate Alignment Scores:**\n",
    "\n",
    "Calculate the Alignment Scores by matching the **'query'** with each of the **'keys'**. This matching operation is computed as the **dot product** of our specific 'query' with each of the hidden states or the 'key' vector. This is to get the scores of how 'related' the 'query' is to each 'key' or each hidden state.\n",
    "\n",
    "$\\mathbf{e}_t = [\\mathbf{h}_N^T \\ \\mathbf{h}_1,  \\mathbf{h}_N^T \\ \\mathbf{h}_2,  ..., \\mathbf{h}_N^T \\ \\mathbf{h}_N] \\in \\mathbb{R}^N $\n",
    "\n",
    "where\n",
    "\n",
    "$ \\mathbf{h}_1, ..., \\mathbf{h}_N \\in \\mathbb{R}^h ; \\mathbf{H} \\in \\mathbb{R}^{N,h}$\n",
    "\n",
    "Hint : We can multiply our 'query' with all of the 'keys' at once by using the matrix form of the 'query' ($\\mathbf{H}$) ( we have to keep shape of batch size at the first dimension so **torch.bmm** might come in handy !)\n",
    "\n",
    "Hint2: Alignment Scores should be of shape :  [batch_size, seq_len, 1]\n",
    "\n",
    "**3. Calculate Attention Weights :**\n",
    "\n",
    "We pass the Alignment Scores through a **softmax** operation to convert the scores into probabilities called the 'Attention Weights'\n",
    "This method is called **soft-attention** which help make the model smooth and differentiable.\n",
    "\n",
    "$\\alpha_t = \\text{softmax}(\\mathbf{e}_t) \\in \\mathbb{R}^N$\n",
    "\n",
    "Hint : our softmaxed Attention Weights should still have the same shape as Alignment Score\n",
    "\n",
    "**4. Calculate Context Vector :**\n",
    "\n",
    "Use the Attention Weight to scale the output **'values'** to get the 'context vector'. In this example, the 'values' is the same as the 'keys' which is the hidden states of every time step from the last layer of LSTM. A context vector is a **weighted sum** of the value vectors, V_ki.\n",
    "\n",
    "$ \\mathbf{c}_t = \\mathbf{H}^T \\ \\alpha_t \\in \\mathbb{R}^h $\n",
    "\n",
    "Hint : Again, we can use the matrix form to get the weighted sum in one operation.\n",
    "The resulting context should be of shape [bs, hidden_size * num_directions]\n",
    "\n",
    "**5. Finally, we use this Context Vector as the output of our Attention Mechanism**\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "e3611a36-2cf8-4e7e-8d9d-cdfc1c410bb1"
   },
   "source": [
    "### LSTM + General Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "9afe0fd8-8726-4a2d-9244-a41455180b66"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class LSTM_GAtt(nn.Module):\n",
    "    def __init__(self, input_dim: int, embed_dim: int, hidden_dim: int, output_dim: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(input_dim, embed_dim, padding_idx=pad_idx)\n",
    "        \n",
    "        # let's use pytorch's LSTM\n",
    "        self.lstm = nn.LSTM(embed_dim, \n",
    "                           hidden_dim, \n",
    "                           num_layers=num_layers, \n",
    "                           bidirectional=bidirectional, \n",
    "                           dropout=dropout,\n",
    "                           batch_first=True)\n",
    "        \n",
    "        # Linear Layer for binary classification \n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        \n",
    "    def attention_net(self, lstm_output, hn):\n",
    "        \n",
    "        h_t      = hn.unsqueeze(2)\n",
    "        H_keys   = torch.clone(lstm_output)\n",
    "        H_values = torch.clone(lstm_output)\n",
    "        \n",
    "        alignment_score   = torch.bmm(H_keys, h_t).squeeze(2) # SHAPE : (bs, seq_len, 1)\n",
    "        \n",
    "        soft_attn_weights = F.softmax(alignment_score, 1) # SHAPE : (bs, seq_len, 1)\n",
    "        \n",
    "        context           = torch.bmm(H_values.transpose(1, 2), soft_attn_weights.unsqueeze(2)).squeeze(2) # SHAPE : (bs, hidden_size * num_directions)\n",
    "        \n",
    "        return context\n",
    "\n",
    "    def forward(self, text, text_lengths):\n",
    "\n",
    "        embedded = self.embedding(text) # SHAPE : (batch_size, seq_len, embed_dim)\n",
    "\n",
    "        lstm_output, (hn, cn) = self.lstm(embedded)\n",
    "        \n",
    "        # This is how we concatenate the forward hidden and backward hidden from Pytorch's BiLSTM\n",
    "        hn = torch.cat((hn[-2,:,:], hn[-1,:,:]), dim = 1)\n",
    "\n",
    "        attn_output = self.attention_net(lstm_output, hn)\n",
    "        \n",
    "        return self.fc(attn_output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "aeed6151-1a0d-4b6b-b025-8ca646368da6"
   },
   "source": [
    "### 2.2 Self Attention Mechanism\n",
    "\n",
    "Self-attention, also known as intra-attention, is an attention mechanism relating different positions of a single sequence in order to compute a representation of the same sequence.\n",
    "The self-attention mechanism allows the inputs to interact with each other (“self”) and find out who they should pay more attention to (“attention”). The outputs are aggregates of these interactions and attention scores.\n",
    "\n",
    "It has been shown to be very useful in machine reading, abstractive summarization, or image description generation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "730a7774-6e13-49a0-8edc-84005f0bd284"
   },
   "source": [
    "You might have noticed from the previous part that the are 3 main vector/matrix in the attention mechanism, which are 'queries', 'keys' and 'value'.\n",
    "Self-attention also need the same elements but we only have 'self' for the model to consider so 'queries', 'keys' and 'value' is all made from our input. Other steps are very similar to General Attention.\n",
    "\n",
    "Same with the previous part, we will pass our data through LSTM first then pass the outputs of LSTM to the Self Attention mechanism.\n",
    "\n",
    "**1. Get the components we need for our Attention Mechanism**\n",
    "\n",
    "Make **3 copies** of $ \\mathbf{H} $ (hidden states of every time step from the last layer of LSTM)\n",
    "\n",
    "        - Hint : can be found in 'output' \n",
    "        - Should be of shape  [bs, seq len, hidden_dim * num_directions]\n",
    "\n",
    "**2. Initialize 3 Linear Layers :**\n",
    "\n",
    "    - Initialize 3 Linear Layer called 'lin_Q', 'lin_K', 'lin_V'\n",
    "    \n",
    "    - input_dim = hidden_dim * num_direction\n",
    "    - output_dim = hidden_dim * num_direction\n",
    "    \n",
    "**3. Pass each copy of lstm_output through each of the Linear Layer.**\n",
    "    \n",
    "Pass each copy of $\\mathbf{H}$ through each of the Linear Layer so that we have learnable weights for generating the queries, keys and values\n",
    "\n",
    "$ \\mathbf{Q} = \\mathbf{H}^T \\ \\mathbf{W}_q + \\mathbf{b}_q \\in \\mathbb{R}^{N,h} $\n",
    "\n",
    "$ \\mathbf{K} = \\mathbf{H}^T \\ \\mathbf{W}_k + \\mathbf{b}_k \\in \\mathbb{R}^{N,h} $\n",
    "\n",
    "$ \\mathbf{V} = \\mathbf{H}^T \\ \\mathbf{W}_v + \\mathbf{b}_v \\in \\mathbb{R}^{N,h} $\n",
    "\n",
    "*Hint: Expected SHAPE : (bs, seq_len, n_hidden * num_directions)\n",
    "\n",
    "**4. Calculate Alignment Scores:**\n",
    "\n",
    "    - Matching the 'query' with the 'keys'.\n",
    "    \n",
    "$\\text{AlignmentScore} = \\mathbf{Q} \\ \\mathbf{K}^T \\in \\mathbb{R}^{N,N}$ \n",
    "\n",
    "    - Hint: Our 'query' and 'keys' are both matrix so you might want to use 'torch's matrix multiplication'.\n",
    "    - Expected SHAPE : (bs, seq_len, seq_len) because we want to match each time step in self with each time step of itself\n",
    "    \n",
    "**5. Padding Mask**\n",
    "\n",
    "Since there are many padding tokens in our input sequence. It would be inefficient to leave them as is. Please implement 'pad_mask' which will replace the Alignment Scores with -1e9 where the input sequence is the padding token.\n",
    "\n",
    "**Skipping this step will not affect the next parts :)\n",
    "\n",
    "**6. Calculate Attention Weights :**\n",
    "\n",
    "    - Pass the Alignment Scores through Softmax\n",
    "    \n",
    "$\\text{Attention Weights} = \\text{softmax}(\\text{AlignmentScore}) \\in \\mathbb{R}^{N,N} $ \n",
    "\n",
    "**7. Calculate the Context Vector :**\n",
    "\n",
    "    - Multiply the Attention Weights with the 'values' to get the Context vector of SHAPE : (bs, seq_len, hidden_dim * num_directions)\n",
    "    - Then do 'Sequence Length Reduction' to aggregate the dimension of seq_len into 1.\n",
    "    - You can choose between averaging or sum.\n",
    "    - Finally, Context vector should have SHAPE : (bs, hidden_dim * num_directions)\n",
    "    \n",
    "$\\text{Context Vector} = \\text{Attention Weights} \\ \\mathbf{V} \\in \\mathbb{R}^{h} $ \n",
    "    \n",
    "    \n",
    "**8. Finally, we use this Context Vector as the output of our Attention Mechanism**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "92fc68b1-bc4c-4045-8e16-3f5478301f7c"
   },
   "source": [
    "### LSTM + Self Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "02ab926b-b666-480c-a76d-5fa1712d2a04"
   },
   "outputs": [],
   "source": [
    "# This attention mask will be apply after Q @ K^T thus the shape will be batch, seq_len, seq_len\n",
    "def get_pad_mask(text):  #[batch, seq_len]\n",
    "    batch_size, seq_len = text.size()\n",
    "    # eq(zero) is lstm output over PAD token\n",
    "    pad_mask = text.data.eq(0).unsqueeze(1)  # torch.eq Computes element-wise equality # batch_size x 1 x seq_len; we unsqueeze so we can make expansion below\n",
    "    return pad_mask.expand(batch_size, seq_len, seq_len)  # batch_size x seq_len x seq_len\n",
    "\n",
    "class LSTM_SelfAtt(nn.Module):\n",
    "    def __init__(self, input_dim, embed_dim, hidden_dim, output_dim, len_reduction):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(input_dim, embed_dim, padding_idx=pad_idx)\n",
    "        \n",
    "        # let's use pytorch's LSTM\n",
    "        self.lstm = nn.LSTM(embed_dim, \n",
    "                           hidden_dim, \n",
    "                           num_layers=num_layers, \n",
    "                           bidirectional=bidirectional, \n",
    "                           dropout=dropout,\n",
    "                           batch_first=True)\n",
    "        \n",
    "        self.softmax       = nn.LogSoftmax(dim=1)\n",
    "\n",
    "        self.lin_Q = nn.Linear(hidden_dim * 2, hidden_dim * 2)\n",
    "        self.lin_K = nn.Linear(hidden_dim * 2, hidden_dim * 2)\n",
    "        self.lin_V = nn.Linear(hidden_dim * 2, hidden_dim * 2)\n",
    "        \n",
    "        self.len_reduction = len_reduction\n",
    "        \n",
    "        # Linear Layer for binary classification \n",
    "        self.fc    = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        \n",
    "    def self_attention_net(self, lstm_output, pad_mask):\n",
    "        \n",
    "        Q = self.lin_Q(torch.clone(lstm_output)) # SHAPE : (bs, seq_len, n_hidden * num_directions)\n",
    "        K = self.lin_K(torch.clone(lstm_output)) # SHAPE : (bs, seq_len, n_hidden * num_directions)\n",
    "        V = self.lin_V(torch.clone(lstm_output)) # SHAPE : (bs, seq_len, n_hidden * num_directions)\n",
    "                \n",
    "        alignment_score = torch.matmul(Q, K.transpose(1, 2)) # SHAPE : (bs, seq_len, seq_len)\n",
    "                \n",
    "        # Apply padding mask \n",
    "        if self.mask:\n",
    "            alignment_score.masked_fill_(pad_mask, -1e9) # Fills elements of self tensor with value where mask is one.\n",
    "        \n",
    "        soft_attn_weights = self.softmax(alignment_score)\n",
    "        \n",
    "        context = torch.matmul(soft_attn_weights, V) # SHAPE : (bs, seq_len, hidden_dim * num_directions)\n",
    "        \n",
    "        if self.len_reduction == \"mean\":\n",
    "            return torch.mean(context, dim=1)\n",
    "        elif self.len_reduction == \"sum\":\n",
    "            return torch.sum(context, dim=1)\n",
    "        elif self.len_reduction == \"last\":\n",
    "            return context[:, -1, :]\n",
    "        \n",
    "    def forward(self, text, text_lengths, mask=True):\n",
    "        self.mask = mask\n",
    "        pad_mask = get_pad_mask(text)\n",
    "        \n",
    "        embedded = self.embedding(text) # SHAPE : (batch_size, seq_len, embed_dim)\n",
    "\n",
    "        lstm_output, (hn, cn) = self.lstm(embedded)\n",
    "        \n",
    "        # This is how we concatenate the forward hidden and backward hidden from Pytorch's BiLSTM\n",
    "        hn = torch.cat((hn[-2,:,:], hn[-1,:,:]), dim = 1)\n",
    "\n",
    "        attn_output = self.self_attention_net(lstm_output, pad_mask)\n",
    "        \n",
    "        return self.fc(attn_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4f7516fc-3e49-4527-a2fa-cf179c50e778"
   },
   "source": [
    "#### Reference & Further Readings:\n",
    "\n",
    "LSTM :\n",
    "\n",
    "- https://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
    "- https://medium.com/@raghavaggarwal0089/bi-lstm-bc3d68da8bd0\n",
    "\n",
    "Attention : \n",
    "\n",
    "- https://arxiv.org/pdf/1409.0473.pdf\n",
    "- https://towardsdatascience.com/illustrated-self-attention-2d627e33b20a\n",
    "- https://machinelearningmastery.com/the-attention-mechanism-from-scratch/#:~:text=In%20essence%2C%20when%20the%20generalized,the%20others%20in%20the%20sequence.\n",
    "- https://blog.floydhub.com/attention-mechanism/#bahdanau-att\n",
    "- https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html\n",
    "- https://www.analyticsvidhya.com/blog/2019/11/comprehensive-guide-attention-mechanism-deep-learning/\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "A4_LSTMAttention-Solution.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "teaching_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "toc-autonumbering": false,
  "vscode": {
   "interpreter": {
    "hash": "becc4c8e5ad229b2591d820334d85e3db0111492344629bf57f272470dce75a5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
