{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Vision Transformer (ViT)](https://arxiv.org/pdf/2010.11929v2.pdf)\n",
    "\n",
    "The Vision Transformer (ViT) is a transformer targeted at vision processing tasks. It has achieved state-of-the-art performance in image classification and (with some modification) other tasks. The ViT concept for image classification is as follows:\n",
    "\n",
    "<img src=\"./figures/vit.gif\" title=\"ViT\" />\n",
    "\n",
    "### How does ViT work?\n",
    "\n",
    "The steps of ViT are as follows:\n",
    "\n",
    "1. Split input image into patches\n",
    "2. Flatten the patches\n",
    "3. Produce linear embeddings from the flattened patches\n",
    "4. Add position embeddings\n",
    "5. Feed the sequence preceeded by a `[class]` token as input to a standard transformer encoder\n",
    "6. Pretrain the model to ouptut image labels for the `[class]` token (fully supervised on a huge dataset such as ImageNet-22K)\n",
    "7. Fine-tune on the downstream dataset for the specific image classification task"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ViT architecture\n",
    "\n",
    "ViT is a Transformer encoder. In detail, it looks like this:\n",
    "\n",
    "<img src=\"./figures/ViTArchitecture.png\" title=\"ViT architecture\" />\n",
    "\n",
    "In the figure we see four main parts:\n",
    "<ol style=\"list-style-type:lower-alpha\">\n",
    "    <li> The high-level architecture of the model.</li>\n",
    "    <li> The Transformer module.</li>\n",
    "    <li> The multiscale self-attention (MSA) head.</li>\n",
    "    <li> An individual self-attention (SA) head.</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Set GPU device\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "os.environ['http_proxy']  = 'http://192.41.170.23:3128'\n",
    "os.environ['https_proxy'] = 'http://192.41.170.23:3128'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/guntsv/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2.0 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import time\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchvision.datasets.mnist import MNIST\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('Using device', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "# Loading data\n",
    "transform = ToTensor()\n",
    "\n",
    "full_dataset = MNIST(root='../Lightning/', train=True, download=True, transform=transform)\n",
    "# Define the sizes for the training and validation sets\n",
    "train_size = int(0.8 * len(full_dataset))  # 80% for training\n",
    "val_size = len(full_dataset) - train_size   # 20% for validation\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "train_set, val_set = random_split(full_dataset, [train_size, val_size])\n",
    "test_set = MNIST(root='../Lightning/', train=False, download=True, transform=transform)\n",
    "test_set = MNIST(root='../Lightning/', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_set, shuffle=True, batch_size=16)\n",
    "val_loader = DataLoader(val_set, shuffle=False, batch_size=16)\n",
    "test_loader = DataLoader(test_set, shuffle=False, batch_size=16)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-head Self Attention (MSA) Model\n",
    "\n",
    "As with the basic transformer above, to build the ViT model, we need to create a MSA module and put it\n",
    "together with the other elements.\n",
    "\n",
    "For a single image, self attention means that each patch's representation\n",
    "is updated based on its input token's similarity with those of the other patches.\n",
    "As before, we perform a linear mapping of each patch to three distinct vectors $q$, $k$, and $v$ (query, key, value).\n",
    "\n",
    "For each patch, we need to compute the dot product of its $q$ vector with all of the $k$ vectors, divide by the square root of the dimension\n",
    "of the vectors, then apply softmax to the result. The resulting matrix is called the matrix of attention cues.\n",
    "We multiply the attention cues with the $v$ vectors associated with the different input tokens and sum them all up.\n",
    "\n",
    "The input for each patch is transformed to a new value based on its similarity (after the linear mapping to $q$, $k$, and $v$) with other patches.\n",
    "\n",
    "However, the whole procedure is carried out $H$ times on $H$ sub-vectors of our current 8-dimensional patches, where $H$ is the number of heads.\n",
    "\n",
    "Once all results are obtained, they are concatenated together then passed through a linear layer.\n",
    "\n",
    "The MSA model looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSA(nn.Module):\n",
    "    def __init__(self, d, n_heads=2):\n",
    "        super(MSA, self).__init__()\n",
    "        self.d = d\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "        assert d % n_heads == 0, f\"Can't divide dimension {d} into {n_heads} heads\"\n",
    "\n",
    "        d_head = int(d / n_heads)\n",
    "        self.q_mappings = [nn.Linear(d_head, d_head) for _ in range(self.n_heads)]\n",
    "        self.k_mappings = [nn.Linear(d_head, d_head) for _ in range(self.n_heads)]\n",
    "        self.v_mappings = [nn.Linear(d_head, d_head) for _ in range(self.n_heads)]\n",
    "        self.d_head = d_head\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, sequences):\n",
    "        # Sequences has shape (N, seq_length, token_dim)\n",
    "        # We go into shape    (N, seq_length, n_heads, token_dim / n_heads)\n",
    "        # And come back to    (N, seq_length, item_dim)  (through concatenation)\n",
    "        result = []\n",
    "        for sequence in sequences:\n",
    "            seq_result = []\n",
    "            for head in range(self.n_heads):\n",
    "                q_mapping = self.q_mappings[head]\n",
    "                k_mapping = self.k_mappings[head]\n",
    "                v_mapping = self.v_mappings[head]\n",
    "\n",
    "                seq = sequence[:, head * self.d_head: (head + 1) * self.d_head]\n",
    "                q, k, v = q_mapping(seq), k_mapping(seq), v_mapping(seq)\n",
    "\n",
    "                attention = self.softmax(q @ k.T / (self.d_head ** 0.5))\n",
    "                seq_result.append(attention @ v)\n",
    "            result.append(torch.hstack(seq_result))\n",
    "        return torch.cat([torch.unsqueeze(r, dim=0) for r in result])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Position encoding\n",
    "\n",
    "The position encoding allows the model to understand where each patch is in the original image. While it is theoretically possible to learn\n",
    "such positional embeddings, the original Vaswani et al. Transformer uses a fixed position embedding representation that adds\n",
    "low-frequency values to the first dimension and higher-frequency values to the later dimensions, resulting in a code that is\n",
    "more similar for nearby tokens than far away tokens. For each token, we add to its j-th coordinate the value\n",
    "\n",
    "$$ p_{i,j} =\n",
    "\\left\\{\\begin{matrix}\n",
    "\\sin (\\frac{i}{10000^{j/d_{embdim}}})\\\\ \n",
    "\\cos (\\frac{i}{10000^{j/d_{embdim}}})\n",
    "\\end{matrix}\\right.\n",
    "$$\n",
    "\n",
    "We can visualize the position encoding matrix thusly:\n",
    "\n",
    "<img src=\"img/peimages.png\" title=\"\" style=\"width: 800px;\" />\n",
    "\n",
    "Here is an implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_positional_embeddings(sequence_length, d, device=\"cpu\"):\n",
    "    result = torch.ones(sequence_length, d)\n",
    "    for i in range(sequence_length):\n",
    "        for j in range(d):\n",
    "            result[i][j] = np.sin(i / (10000 ** (j / d))) if j % 2 == 0 else np.cos(i / (10000 ** ((j - 1) / d)))\n",
    "    return result.to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ViT Model\n",
    "Create the ViT model as below. The explaination is later."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MLP contains two layers with a GELU non-linearity.\n",
    "\n",
    "$\\bf{z}_0 = [\\bf{x}_\\text{class}; \\, \\bf{x}^1_p \\bf{E}; \\, \\bf{x}^2_p \\bf{E}; \\cdots; \\, \\bf{x}^{N}_p \\bf{E}] + \\bf{E}_{pos}, \\bf{E} \\in \\mathbb{R}^{(P^2 \\cdot C) \\times D} , \\bf{E}_{pos}  \\in \\mathbb{R}^{(N + 1) \\times D} $\n",
    "\n",
    "$\\bf{z^\\prime}_\\ell = \\operatorname{MSA}(\\operatorname{LN}(\\bf{z}_{\\ell-1})) + \\bf{z}_{\\ell-1}, \\ell=1\\ldots L $\n",
    "\n",
    "$\\bf{z}_\\ell = \\operatorname{MLP}(\\operatorname{LN}(\\bf{z^\\prime}_{\\ell})) + \\bf{z^\\prime}_{\\ell}, \\ell=1\\ldots L $\n",
    "\n",
    "$\\bf{y} = \\operatorname{LN}(\\bf{z}_L^0)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViT(nn.Module):\n",
    "    def __init__(self, input_shape, n_patches=7, hidden_d=8, n_heads=2, out_d=10):\n",
    "        # Super constructor\n",
    "        super(ViT, self).__init__()\n",
    "\n",
    "        # Input and patches sizes\n",
    "        self.input_shape = input_shape\n",
    "        self.n_patches = n_patches\n",
    "        self.n_heads = n_heads\n",
    "        assert input_shape[1] % n_patches == 0, \"Input shape not entirely divisible by number of patches\"\n",
    "        assert input_shape[2] % n_patches == 0, \"Input shape not entirely divisible by number of patches\"\n",
    "        self.patch_size = (input_shape[1] / n_patches, input_shape[2] / n_patches)\n",
    "        self.hidden_d = hidden_d\n",
    "\n",
    "        # 1) Linear mapper\n",
    "        self.input_d = int(input_shape[0] * self.patch_size[0] * self.patch_size[1])\n",
    "        self.linear_mapper = nn.Linear(self.input_d, self.hidden_d)\n",
    "\n",
    "        # 2) Classification token\n",
    "        self.class_token = nn.Parameter(torch.rand(1, self.hidden_d))\n",
    "\n",
    "        # 3) Positional embedding\n",
    "        # (In forward method)\n",
    "\n",
    "        # 4a) Layer normalization 1\n",
    "        self.ln1 = nn.LayerNorm((self.n_patches ** 2 + 1, self.hidden_d))\n",
    "\n",
    "        # 4b) Multi-head Self Attention (MSA) and classification token\n",
    "        self.msa = MSA(self.hidden_d, n_heads)\n",
    "\n",
    "        # 5a) Layer normalization 2\n",
    "        self.ln2 = nn.LayerNorm((self.n_patches ** 2 + 1, self.hidden_d))\n",
    "\n",
    "    # 5b) Encoder MLP\n",
    "        self.enc_mlp = nn.Sequential(\n",
    "            nn.Linear(self.hidden_d, self.hidden_d),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # 6) Classification MLP\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(self.hidden_d, out_d),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "\n",
    "    def forward(self, images):\n",
    "        # Dividing images into patches\n",
    "        n, c, w, h = images.shape\n",
    "        patches = images.reshape(n, self.n_patches ** 2, self.input_d)\n",
    "\n",
    "        # Running linear layer for tokenization\n",
    "        tokens = self.linear_mapper(patches)\n",
    "\n",
    "        # Adding classification token to the tokens\n",
    "        tokens = torch.stack([torch.vstack((self.class_token, tokens[i])) for i in range(len(tokens))])\n",
    "\n",
    "        # Adding positional embedding\n",
    "        tokens += get_positional_embeddings(self.n_patches ** 2 + 1, self.hidden_d, device).repeat(n, 1, 1)\n",
    "\n",
    "        # TRANSFORMER ENCODER BEGINS ###################################\n",
    "        # NOTICE: MULTIPLE ENCODER BLOCKS CAN BE STACKED TOGETHER ######\n",
    "        # Running Layer Normalization, MSA and residual connection\n",
    "        self.msa(self.ln1(tokens.to(\"cpu\")).to(device))\n",
    "        out = tokens + self.msa(self.ln1(tokens))\n",
    "\n",
    "        # Running Layer Normalization, MLP and residual connection\n",
    "        out = out + self.enc_mlp(self.ln2(out))\n",
    "        # TRANSFORMER ENCODER ENDS   ###################################\n",
    "\n",
    "        # Getting the classification token only\n",
    "        out = out[:, 0]\n",
    "\n",
    "        return self.mlp(out)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Patchifying and the linear mapping\n",
    "\n",
    "The transformer encoder was developed with sequence data in mind, such as English sentences. However, an image is not a sequence. Thus, we break it into multiple sub-images and map each sub-image to a vector.\n",
    "\n",
    "We do so by simply reshaping our input, which has size $(N, C, H, W)$ (in our example $(N, 1, 28, 28)$), to size (N, #Patches, Patch dimensionality), where the dimensionality of a patch is adjusted accordingly.\n",
    "\n",
    "In MNIST, we break each $(1, 28, 28)$ into 7x7 patches (hence, each of size 4x4). That is, we are going to obtain 7x7=49 sub-images out of a single image.\n",
    "\n",
    "$$(N,1,28,28) \\rightarrow (N,P\\times P, H \\times C/P  \\times W \\times C/P) \\rightarrow (N, 7\\times 7, 4\\times 4) \\rightarrow (N, 49, 16)$$\n",
    "\n",
    "<img src=\"./figures/patch.png\" title=\"an image is split into patches\" />"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Adding the classification token\n",
    "\n",
    "When information about all other tokens will be present here, we will be able to classify the image using only this special token. The initial value of the special token (the one fed to the transformer encoder) is a parameter of the model that needs to be learned.\n",
    "\n",
    "We can now add a parameter to our model and convert our (N, 49, 8) tokens tensor to an (N, 50, 8) tensor (we add the special token to each sequence).\n",
    "\n",
    "Passing from (N,49,8) â†’ (N,50,8) is probably sub-optimal. Also, notice that the classification token is put as the first token of each sequence. This will be important to keep in mind when we will then retrieve the classification token to feed to the final MLP."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Positional encoding\n",
    "\n",
    "See above, as we mentioned.\n",
    "\n",
    "#### Step 4: LN, MSA, and Residual Connection\n",
    "\n",
    "The step is to apply layer normalization to the tokens, then apply MSA, and add a residual connection (add the input we had before applying LN).\n",
    "- **Layer normalization** is a popular block that, given an input, subtracts its mean and divides by the standard deviation.\n",
    "- **MSA**: same as the vanilla transformer.\n",
    "- **A residual connection** consists in just adding the original input to the result of some computation. This, intuitively, allows a network to become more powerful while also preserving the set of possible functions that the model can approximate.\n",
    "\n",
    "The residual connection is added at the original (N, 50, 8) tensor to the (N, 50, 8) obtained after LN and MSA."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: LN, MLP, and Residual Connection\n",
    "All that is left to the transformer encoder is just a simple residual connection between what we already have and what we get after passing the current tensor through another LN and an MLP.\n",
    "\n",
    "#### Step 6: Classification MLP\n",
    "Finally, we can extract just the classification token (first token) out of our N sequences, and use each token to get N classifications.\n",
    "Since we decided that each token is an 8-dimensional vector, and since we have 10 possible digits, we can implement the classification MLP as a simple 8x10 matrix, activated with the SoftMax function.\n",
    "\n",
    "The output of our model is now an (N, 10) tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ViT(\n",
    "    input_shape=(1, 28, 28), \n",
    "    n_patches=7, \n",
    "    hidden_d=20, \n",
    "    n_heads=2, \n",
    "    out_d=10)\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ViT(\n",
       "  (linear_mapper): Linear(in_features=16, out_features=20, bias=True)\n",
       "  (ln1): LayerNorm((50, 20), eps=1e-05, elementwise_affine=True)\n",
       "  (msa): MSA(\n",
       "    (softmax): Softmax(dim=-1)\n",
       "  )\n",
       "  (ln2): LayerNorm((50, 20), eps=1e-05, elementwise_affine=True)\n",
       "  (enc_mlp): Sequential(\n",
       "    (0): Linear(in_features=20, out_features=20, bias=True)\n",
       "    (1): ReLU()\n",
       "  )\n",
       "  (mlp): Sequential(\n",
       "    (0): Linear(in_features=20, out_features=10, bias=True)\n",
       "    (1): Softmax(dim=-1)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "num_epochs = 5\n",
    "lr = 0.01\n",
    "\n",
    "#training hyperparameters\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, optimizer, criterion, device=\"cpu\"):\n",
    "    \n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    train_correct, total = 0, 0\n",
    "\n",
    "    for batch in tqdm(train_loader):\n",
    "        x, y = batch\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        y_hat = model(x)\n",
    "        loss = criterion(y_hat, y) / len(x)\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_correct += torch.sum(torch.argmax(y_hat, dim=1) == y).item()\n",
    "        total += len(x)\n",
    "\n",
    "    return train_loss, train_correct/total\n",
    "\n",
    "def evaluate(model, loader, criterion):\n",
    "\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    test_correct, total = 0, 0\n",
    "    \n",
    "    for batch in tqdm(loader):\n",
    "        x, y = batch\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        y_hat = model(x)\n",
    "        loss = criterion(y_hat, y) / len(x)\n",
    "        test_loss += loss.item()\n",
    "\n",
    "        test_correct += torch.sum(torch.argmax(y_hat, dim=1) == y).item()\n",
    "        total += len(x)\n",
    "\n",
    "    return test_loss, test_correct/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5489b2bbaeca432abb50d9b906fd5500",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ddf29113e534168a79242a5c1699d00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/750 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Time: 0m 45s\n",
      "\tTrain Loss: 329.664 | Train Acc: 70.44%\n",
      "\t Val. Loss: 74.927 | Test  Acc: 86.62%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f799326d508042b881569c5da803fbd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d57ef8ba0b3347f49b279dcb46bf1c4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/750 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 02 | Time: 0m 44s\n",
      "\tTrain Loss: 295.282 | Train Acc: 88.73%\n",
      "\t Val. Loss: 73.084 | Test  Acc: 90.32%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "186c5941a5164dffa6a0bcd9465cef5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e8e740014f04431b533332886a7b8c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/750 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 03 | Time: 0m 46s\n",
      "\tTrain Loss: 292.464 | Train Acc: 90.20%\n",
      "\t Val. Loss: 73.334 | Test  Acc: 89.64%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eae465238f6a4ef8a66910be4c80b9b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8dc9c92782c34f95a6cacea261280dc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/750 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 04 | Time: 0m 50s\n",
      "\tTrain Loss: 291.517 | Train Acc: 90.64%\n",
      "\t Val. Loss: 72.934 | Test  Acc: 90.53%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b39d8e0741b34f48beec63b10243b201",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99369698ed724886992424d682446f40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/750 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 05 | Time: 0m 58s\n",
      "\tTrain Loss: 290.619 | Train Acc: 91.14%\n",
      "\t Val. Loss: 73.689 | Test  Acc: 88.96%\n"
     ]
    }
   ],
   "source": [
    "best_valid_loss = float('inf')\n",
    "num_epochs = 5\n",
    "clip       = 1\n",
    "\n",
    "save_path = f'models/{model.__class__.__name__}.pt'\n",
    "\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    train_loss, train_acc = train(model, train_loader, optimizer, criterion, device)\n",
    "    valid_loss, test_acc = evaluate(model, val_loader, criterion)\n",
    "\n",
    "    #for plotting\n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), save_path)\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} | Test  Acc: {test_acc * 100:.2f}%')\n",
    "    #lower perplexity is better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'loss')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure(figsize=(5, 3))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.plot(train_losses, label = 'train loss')\n",
    "ax.plot(valid_losses, label = 'valid loss')\n",
    "plt.legend()\n",
    "ax.set_xlabel('updates')\n",
    "ax.set_ylabel('loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Test. Loss: 60.827 | Test Acc: 90.40%\n"
     ]
    }
   ],
   "source": [
    "# model.load_state_dict(torch.load(save_path))\n",
    "test_loss, test_acc = evaluate(model, test_loader, criterion)\n",
    "print(f'\\t Test. Loss: {test_loss:.3f} | Test Acc: {test_acc * 100:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
