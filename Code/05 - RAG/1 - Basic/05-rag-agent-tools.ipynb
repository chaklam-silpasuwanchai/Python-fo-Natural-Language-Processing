{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0a006acd-03a3-428f-b20b-02d240c7f49b",
   "metadata": {},
   "source": [
    "# Natural Language Processing\n",
    "\n",
    "# Agents\n",
    "- The core idea of agents is to use a language model to choose `a sequence of actions` to take. \n",
    "- In chains, a sequence of actions is hardcoded (in code). \n",
    "- In agents, a language model is used as a reasoning engine to determine `which actions to take and in which order.`\n",
    "\n",
    "**LLM**: The language model powering the agent.\n",
    "\n",
    "**Tools**: A function that performs a specific duty. This can be things like: Google Search, Database lookup, Python REPL, other chains. The interface for a tool is currently a function that is expected to have a string as an input, with a string as an output.\n",
    "\n",
    "**Agent**: The agent to use. This should be a string that references a support agent class. Because this notebook focuses on the simplest, highest level API, this only covers using the standard supported agents. If you want to implement a custom agent, see the documentation for custom agents (coming soon).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69b2810d-b5d4-455f-be3d-9ee010a0b756",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install langchain-community\n",
    "# !pip3 install langchain-core\n",
    "# !pip3 install langchain-experimental\n",
    "# !pip3 install langchain==0.0.354\n",
    "# !pip3 install langchainhub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c9c48cd-584f-433f-afbd-1329121b6bfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.0.354'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import langchain\n",
    "langchain.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d315661a-f9a1-4853-bab1-723af8d878d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "# Set GPU device\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "os.environ['http_proxy']  = 'http://192.41.170.23:3128'\n",
    "os.environ['https_proxy'] = 'http://192.41.170.23:3128'\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8bfea397-933f-4717-a9f1-278d723e67af",
   "metadata": {},
   "source": [
    "## 1. LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "649c5e7f-39be-44af-8bd0-a1e2423cbf48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import AutoTokenizer, pipeline, AutoModelForSeq2SeqLM\n",
    "from transformers import BitsAndBytesConfig\n",
    "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "\n",
    "model_id = \"models/fastchat-t5-3b-v1.0\"\n",
    "# model_id = \"models/TinyLlama-1.1B-Chat-v1.0\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_id,\n",
    ")\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "bitsandbyte_config = BitsAndBytesConfig(\n",
    "        load_in_4bit = True,\n",
    "        bnb_4bit_quant_type = \"nf4\",\n",
    "        bnb_4bit_compute_dtype = torch.float16,\n",
    "        bnb_4bit_use_double_quant = True\n",
    "    )\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config = bitsandbyte_config,\n",
    "    device_map = 'auto',\n",
    "    load_in_8bit = True,\n",
    ")\n",
    "\n",
    "pipe = pipeline(\n",
    "    task=\"text2text-generation\",\n",
    "    # task=\"text-generation\",\n",
    "    model=model, \n",
    "    tokenizer=tokenizer, \n",
    "    max_new_tokens=256, \n",
    "    model_kwargs = { \n",
    "        \"temperature\":0, \n",
    "        \"repetition_penalty\": 1.5\n",
    "    }, \n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline = pipe)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b967e064-b7e9-4164-a7ab-d3bf80dd9c60",
   "metadata": {},
   "source": [
    "## 2. Tools\n",
    "Tools are functions that an agent can invoke. There are two important design considerations around tools:\n",
    "1. Giving the agent access to the right tools\n",
    "2. Describing the tools in a way that is most helpful to the agent\n",
    "   \n",
    "Without thinking through both, you won't be able to build a working agent. If you don't give the agent access to a correct set of tools, it will never be able to accomplish the objectives you give it. If you don't describe the tools well, the agent won't know how to use them properly.\n",
    "\n",
    "LangChain provides a wide set of built-in tools, but also makes it easy to define your own (including custom descriptions). [Explore the tools integrations section](https://python.langchain.com/docs/integrations/tools/)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b0383a41-9941-4db9-98a8-333b4fdb5173",
   "metadata": {},
   "source": [
    "### Retriever Tool\n",
    "Now we need to create a tool for our retriever. The main things we need to pass in are a name for the retriever as well as a description. These will both be used by the language model, so they should be informative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "167b45f0-7708-4fb0-b56f-8442381cc6b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load INSTRUCTOR_Transformer\n",
      "max_seq_length  512\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "\n",
    "nlp_document = './docs/pdf/SpeechandLanguageProcessing_3rd_07jan2023.pdf'\n",
    "loader = PyMuPDFLoader(nlp_document)\n",
    "documents = loader.load()\n",
    "# len(documents)\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import torch\n",
    "from langchain_community.embeddings import HuggingFaceInstructEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 700,\n",
    "    chunk_overlap = 100\n",
    ")\n",
    "docs = text_splitter.split_documents(documents) \n",
    "# len(docs)\n",
    "\n",
    "model_name = 'hkunlp/instructor-base'\n",
    "\n",
    "embedding_model = HuggingFaceInstructEmbeddings(\n",
    "    model_name = model_name,              \n",
    "    model_kwargs = {\n",
    "        'device': torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    },\n",
    ")\n",
    "\n",
    "vectordb = FAISS.from_documents(docs, embedding_model)\n",
    "retriever = vectordb.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "caee388d-ed5a-43ba-b379-ad1f5244765a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tool(name='search_nlp_stanford', description='Searches and returns documents regarding the nlp-stanford.', args_schema=<class 'langchain.tools.retriever.RetrieverInput'>, func=<bound method BaseRetriever.get_relevant_documents of VectorStoreRetriever(tags=['FAISS', 'HuggingFaceInstructEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x7f6eb82f7d60>)>, coroutine=<bound method BaseRetriever.aget_relevant_documents of VectorStoreRetriever(tags=['FAISS', 'HuggingFaceInstructEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x7f6eb82f7d60>)>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.agents.agent_toolkits import create_retriever_tool\n",
    "\n",
    "retriever_tool = create_retriever_tool(\n",
    "    retriever = retriever,\n",
    "    name = \"search_nlp_stanford\",\n",
    "    description = \"Searches and returns documents regarding the nlp-stanford.\",\n",
    ")\n",
    "\n",
    "retriever_tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e8c7b89-8595-4d63-8afc-31e92a6919a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='formers are not based on recurrent connections (which can be hard to parallelize),\\nwhich means that transformers can be more efﬁcient to implement at scale.\\nTransformers map sequences of input vectors (x1,...,xn) to sequences of output\\nvectors (y1,...,yn) of the same length. Transformers are made up of stacks of trans-\\nformer blocks, each of which is a multilayer network made by combining simple\\nlinear layers, feedforward networks, and self-attention layers, the key innovation of\\nself-attention\\ntransformers. Self-attention allows a network to directly extract and use information\\nfrom arbitrarily large contexts without the need to pass it through intermediate re-', metadata={'source': './docs/pdf/SpeechandLanguageProcessing_3rd_07jan2023.pdf', 'file_path': './docs/pdf/SpeechandLanguageProcessing_3rd_07jan2023.pdf', 'page': 219, 'total_pages': 636, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': \"D:20230107092057-08'00'\", 'modDate': \"D:20230107092057-08'00'\", 'trapped': ''}),\n",
       " Document(page_content='transformer\\nThe transformer offers new mechanisms (self-attention and positional encod-\\nings) that help represent time and help focus on how words relate to each other over\\nlong distances. We’ll see how to apply this model to the task of language modeling,\\nand then we’ll see how a transformer pretrained on language modeling can be used\\nin a zero shot manner to perform other NLP tasks.\\n10.1\\nSelf-Attention Networks: Transformers\\nIn this section we introduce the architecture of transformers. Like the LSTMs of\\ntransformers\\nChapter 9, transformers can handle distant information. But unlike LSTMs, trans-\\nformers are not based on recurrent connections (which can be hard to parallelize),', metadata={'source': './docs/pdf/SpeechandLanguageProcessing_3rd_07jan2023.pdf', 'file_path': './docs/pdf/SpeechandLanguageProcessing_3rd_07jan2023.pdf', 'page': 219, 'total_pages': 636, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': \"D:20230107092057-08'00'\", 'modDate': \"D:20230107092057-08'00'\", 'trapped': ''}),\n",
       " Document(page_content='multaneously. For example, distinct syntactic, semantic, and discourse relationships\\ncan hold between verbs and their arguments in a sentence. It would be difﬁcult for\\na single transformer block to learn to capture all of the different kinds of parallel\\nrelations among its inputs. Transformers address this issue with multihead self-\\nattention layers. These are sets of self-attention layers, called heads, that reside in\\nmultihead\\nself-attention\\nlayers\\nparallel layers at the same depth in a model, each with its own set of parameters.', metadata={'source': './docs/pdf/SpeechandLanguageProcessing_3rd_07jan2023.pdf', 'file_path': './docs/pdf/SpeechandLanguageProcessing_3rd_07jan2023.pdf', 'page': 224, 'total_pages': 636, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': \"D:20230107092057-08'00'\", 'modDate': \"D:20230107092057-08'00'\", 'trapped': ''}),\n",
       " Document(page_content='Beyond this simple change, all of the other elements of the transformer archi-\\ntecture remain the same for bidirectional encoder models. Inputs to the model are\\nsegmented using subword tokenization and are combined with positional embed-\\ndings before being passed through a series of standard transformer blocks consisting\\nof self-attention and feedforward layers augmented with residual connections and\\nlayer normalization, as shown in Fig. 11.4.\\nSelf-Attention Layer\\nLayer Normalize\\nFeedforward Layer\\nLayer Normalize\\nTransformer\\nBlock\\nyn\\nx1\\nx2\\nx3\\nxn\\n…\\nResidual\\nconnection\\nResidual\\nconnection\\n+\\n+\\nFigure 11.4\\nA transformer block showing all the layers.', metadata={'source': './docs/pdf/SpeechandLanguageProcessing_3rd_07jan2023.pdf', 'file_path': './docs/pdf/SpeechandLanguageProcessing_3rd_07jan2023.pdf', 'page': 238, 'total_pages': 636, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': \"D:20230107092057-08'00'\", 'modDate': \"D:20230107092057-08'00'\", 'trapped': ''})]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever_tool('Transformers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a2f9933-f367-4fc9-8680-a038b6dc9064",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #### alternative ways\n",
    "# from langchain.agents import Tool\n",
    "# from langchain.chains import RetrievalQA\n",
    "\n",
    "# knowledge_base = RetrievalQA.from_chain_type(\n",
    "#     llm=llm, chain_type=\"stuff\", retriever=retriever, verbose = True\n",
    "# )\n",
    "\n",
    "# retriever_tool = Tool(\n",
    "#     name=\"search_nlp_stanford\",\n",
    "#     func=knowledge_base.run,\n",
    "#     description=\"Searches and returns documents regarding the nlp-stanford.\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7b930bb6-2d4e-4a2e-b38d-e1df9db28c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retriever_tool('Transformers')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "498435c5-aa16-401b-99ae-a324157a2d4a",
   "metadata": {},
   "source": [
    "### Search Tool\n",
    "This notebook shows off usage of various search tools.\n",
    "[Explore more tools](https://python.langchain.com/docs/integrations/tools)\n",
    "\n",
    "[get SERPAPI_API_KEY click](https://serpapi.com/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e0d31c69-2f2c-40f8-894f-e11dbb5475b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install google-search-results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ae84a68a-768c-436c-8b33-d0d2d519ac08",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['SERPAPI_API_KEY'] = \"6068c2ee7f7da49ffbbd077d9847f80afe5c8e946c9d959aec66fc7e7b944eab\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "730713b7-3d77-430e-ad7b-9952be5cebc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Tool(name='Search', description='A search engine. Useful for when you need to answer questions about current events. Input should be a search query.', func=<bound method SerpAPIWrapper.run of SerpAPIWrapper(search_engine=<class 'serpapi.google_search.GoogleSearch'>, params={'engine': 'google', 'google_domain': 'google.com', 'gl': 'us', 'hl': 'en'}, serpapi_api_key='6068c2ee7f7da49ffbbd077d9847f80afe5c8e946c9d959aec66fc7e7b944eab', aiosession=None)>, coroutine=<bound method SerpAPIWrapper.arun of SerpAPIWrapper(search_engine=<class 'serpapi.google_search.GoogleSearch'>, params={'engine': 'google', 'google_domain': 'google.com', 'gl': 'us', 'hl': 'en'}, serpapi_api_key='6068c2ee7f7da49ffbbd077d9847f80afe5c8e946c9d959aec66fc7e7b944eab', aiosession=None)>)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.agents.load_tools import load_tools\n",
    "tool_names = [\"serpapi\"]\n",
    "\n",
    "list_load_tools = load_tools(tool_names)\n",
    "list_load_tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "98cbec23-4edd-4b94-abc2-16e85e2f213c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tool(name='Search', description='A search engine. Useful for when you need to answer questions about current events. Input should be a search query.', func=<bound method SerpAPIWrapper.run of SerpAPIWrapper(search_engine=<class 'serpapi.google_search.GoogleSearch'>, params={'engine': 'google', 'google_domain': 'google.com', 'gl': 'us', 'hl': 'en'}, serpapi_api_key='6068c2ee7f7da49ffbbd077d9847f80afe5c8e946c9d959aec66fc7e7b944eab', aiosession=None)>, coroutine=<bound method SerpAPIWrapper.arun of SerpAPIWrapper(search_engine=<class 'serpapi.google_search.GoogleSearch'>, params={'engine': 'google', 'google_domain': 'google.com', 'gl': 'us', 'hl': 'en'}, serpapi_api_key='6068c2ee7f7da49ffbbd077d9847f80afe5c8e946c9d959aec66fc7e7b944eab', aiosession=None)>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "google_search_tools = list_load_tools[0]\n",
    "google_search_tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "24f2aba8-bf45-4cb0-a8d4-e7f01326b69c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[\"Gemini is the third astrological sign in the zodiac. Under the tropical zodiac, the sun transits this sign between about May 21 to June 21. Gemini is represented by the twins, Castor and Pollux, known as the Dioscuri in Greek mythology. It\\'s known as a positive, mutable sign.\", \\'Gemini type: Astrological sign.\\', \\'Gemini symbol: Twin.\\', \\'Gemini element: Air.\\', \\'Gemini ruling_planet: Mercury.\\', \\'Gemini zodiac_color: yellow.\\', \\'Gemini constellation: Gemini.\\', \\'Gemini duration_tropical_western: May 20 – June 20 (2024, UT1).\\', \\'Gemini fall: South Node.\\', \\'The secure way to buy, sell, store, and convert crypto. Millions use Gemini to diversify their portfolios. Get started.\\', \\'Gemini is our most capable and general model, built to be multimodal and optimized for three different sizes: Ultra, Pro and Nano.\\', \\'Gemini (♊︎) is the third astrological sign in the zodiac. Under the tropical zodiac, the sun transits this sign between about May 21 to June 21. Gemini is ...\\', \\'Playful and intellectually curious, Gemini is constantly juggling a variety of passions, hobbies, careers, and friend groups. They are the ...\\', \\'Gemini, an Air sign ruled by Mercury, can represents two personalities. Sociable and communicative, they excel in creative fields, value communication in ...\\', \"People with a Gemini Sun sign are intelligent and perceptive. They can size up a person\\'s character in a matter of seconds and always know who\\'s bluffing.\", \\'Gemini is the first model to outperform human experts on MMLU (Massive Multitask Language Understanding), one of the most popular methods to test the ...\\', \"Gemini is a neighborhood American restaurant located in Chicago\\'s beautiful and historic Lincoln Park. 2075 N Lincoln Ave, Chicago, IL 60614. 773-525-2522.\", \\'Gemini, (Latin: “Twins”) in astronomy, zodiacal constellation lying in the northern sky between Cancer and Taurus, at about 7 hours right ...\\', \\'The International Gemini Observatory consists of twin 8.1-meter diameter optical/infrared telescopes located on two of the best observing sites on the planet.\\']'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "google_search_tools('Gemini')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "edbe7398-5ffa-40d0-9c43-56d21647ee99",
   "metadata": {},
   "source": [
    "### Calculator Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bd0ce6b9-32ca-49f4-8af4-a424fabb6058",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tool(name='Calculator', description='useful for when you need to answer questions about math', func=<bound method Chain.run of LLMMathChain(verbose=True, llm_chain=LLMChain(prompt=PromptTemplate(input_variables=['question'], template='Translate a math problem into a expression that can be executed using Python\\'s numexpr library. Use the output of running this code to answer the question.\\n\\nQuestion: ${{Question with math problem.}}\\n```text\\n${{single line mathematical expression that solves the problem}}\\n```\\n...numexpr.evaluate(text)...\\n```output\\n${{Output of running the code}}\\n```\\nAnswer: ${{Answer}}\\n\\nBegin.\\n\\nQuestion: What is 37593 * 67?\\n```text\\n37593 * 67\\n```\\n...numexpr.evaluate(\"37593 * 67\")...\\n```output\\n2518731\\n```\\nAnswer: 2518731\\n\\nQuestion: 37593^(1/5)\\n```text\\n37593**(1/5)\\n```\\n...numexpr.evaluate(\"37593**(1/5)\")...\\n```output\\n8.222831614237718\\n```\\nAnswer: 8.222831614237718\\n\\nQuestion: {question}\\n'), llm=HuggingFacePipeline(pipeline=<transformers.pipelines.text2text_generation.Text2TextGenerationPipeline object at 0x7f6f8bf0fdc0>)))>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import LLMMathChain\n",
    "from langchain.agents import Tool\n",
    "\n",
    "calculator = LLMMathChain.from_llm(llm=llm, verbose=True)\n",
    "\n",
    "calculator_tool = Tool(\n",
    "    name=\"Calculator\",\n",
    "    func=calculator.run,\n",
    "    description=\"useful for when you need to answer questions about math\",\n",
    ")\n",
    "calculator_tool"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a6e4f2eb-0ded-4e2a-b752-6b926ed9ea26",
   "metadata": {},
   "source": [
    "### SQL Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6bf7052e-b6a6-427f-84c6-5e039934329b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.utilities import SQLDatabase\n",
    "from langchain_experimental.sql import SQLDatabaseChain\n",
    "from langchain.agents import Tool\n",
    "\n",
    "database = \"sqlite:///docs/sql/Chinook.db\"\n",
    "\n",
    "db = SQLDatabase.from_uri(database)\n",
    "\n",
    "db_chain = SQLDatabaseChain.from_llm(\n",
    "    llm = llm, \n",
    "    db = db, \n",
    "    verbose=True,\n",
    ")\n",
    "    \n",
    "db_tool = Tool(\n",
    "    name = 'ChinookSearch',\n",
    "    func = db_chain.run,\n",
    "    description=\"useful for when you need to find chinook database\",\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e52aa9a0-f4f2-431c-80f1-c05e731e13fe",
   "metadata": {},
   "source": [
    "## 3. Agent\n",
    "This is the chain responsible for deciding what step to take next, which powered by a language model and a prompt. The inputs to this chain are:\n",
    "\n",
    "- `Tools`: Descriptions of available tools\n",
    "- `User input`: The high level objective\n",
    "- `Intermediate steps`: Any (action, tool output) pairs previously executed in order to achieve the user input\n",
    "\n",
    "The output is the next action(s) to take or the final response to send to the user (AgentActions or AgentFinish). An action specifies a tool and the input to that tool.\n",
    "\n",
    "Different agents have different prompting styles for reasoning, different ways of encoding inputs, and different ways of parsing the output. [Explore Agent Types here](https://python.langchain.com/docs/modules/agents/agent_types/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fd99ade2-5abe-45b5-938d-93b4d8d22a28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Tool(name='search_nlp_stanford', description='Searches and returns documents regarding the nlp-stanford.', args_schema=<class 'langchain.tools.retriever.RetrieverInput'>, func=<bound method BaseRetriever.get_relevant_documents of VectorStoreRetriever(tags=['FAISS', 'HuggingFaceInstructEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x7f6eb82f7d60>)>, coroutine=<bound method BaseRetriever.aget_relevant_documents of VectorStoreRetriever(tags=['FAISS', 'HuggingFaceInstructEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x7f6eb82f7d60>)>),\n",
       " Tool(name='Search', description='A search engine. Useful for when you need to answer questions about current events. Input should be a search query.', func=<bound method SerpAPIWrapper.run of SerpAPIWrapper(search_engine=<class 'serpapi.google_search.GoogleSearch'>, params={'engine': 'google', 'google_domain': 'google.com', 'gl': 'us', 'hl': 'en'}, serpapi_api_key='6068c2ee7f7da49ffbbd077d9847f80afe5c8e946c9d959aec66fc7e7b944eab', aiosession=None)>, coroutine=<bound method SerpAPIWrapper.arun of SerpAPIWrapper(search_engine=<class 'serpapi.google_search.GoogleSearch'>, params={'engine': 'google', 'google_domain': 'google.com', 'gl': 'us', 'hl': 'en'}, serpapi_api_key='6068c2ee7f7da49ffbbd077d9847f80afe5c8e946c9d959aec66fc7e7b944eab', aiosession=None)>),\n",
       " Tool(name='ChinookSearch', description='useful for when you need to find chinook database', func=<bound method Chain.run of SQLDatabaseChain(verbose=True, llm_chain=LLMChain(prompt=PromptTemplate(input_variables=['input', 'table_info', 'top_k'], template='You are a SQLite expert. Given an input question, first create a syntactically correct SQLite query to run, then look at the results of the query and return the answer to the input question.\\nUnless the user specifies in the question a specific number of examples to obtain, query for at most {top_k} results using the LIMIT clause as per SQLite. You can order the results to return the most informative data in the database.\\nNever query for all columns from a table. You must query only the columns that are needed to answer the question. Wrap each column name in double quotes (\") to denote them as delimited identifiers.\\nPay attention to use only the column names you can see in the tables below. Be careful to not query for columns that do not exist. Also, pay attention to which column is in which table.\\nPay attention to use date(\\'now\\') function to get the current date, if the question involves \"today\".\\n\\nUse the following format:\\n\\nQuestion: Question here\\nSQLQuery: SQL Query to run\\nSQLResult: Result of the SQLQuery\\nAnswer: Final answer here\\n\\nOnly use the following tables:\\n{table_info}\\n\\nQuestion: {input}'), llm=HuggingFacePipeline(pipeline=<transformers.pipelines.text2text_generation.Text2TextGenerationPipeline object at 0x7f6f8bf0fdc0>)), database=<langchain_community.utilities.sql_database.SQLDatabase object at 0x7f6ecd856110>)>)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tools = [retriever_tool, google_search_tools, db_tool]\n",
    "tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d61b0d35-ebc1-4da3-aa6a-1be92f8fbcbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search_nlp_stanford\n",
      "Searches and returns documents regarding the nlp-stanford.\n",
      "======================================================================================================================================================\n",
      "Search\n",
      "A search engine. Useful for when you need to answer questions about current events. Input should be a search query.\n",
      "======================================================================================================================================================\n",
      "ChinookSearch\n",
      "useful for when you need to find chinook database\n",
      "======================================================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "for tool in tools:\n",
    "    print (tool.name)\n",
    "    print (tool.description)\n",
    "    print (\"=\"*150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2f2f537d-11ba-4a7f-a666-599fba3f811a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.prompts import MessagesPlaceholder\n",
    "# from langchain_core.messages import SystemMessage\n",
    "# from langchain.agents.openai_functions_agent.base import OpenAIFunctionsAgent\n",
    "\n",
    "# system_message = SystemMessage(\n",
    "#     content=(\n",
    "#     \"\"\"I'm your friendly NLP chatbot named ChakyBot, here to assist Chaky and Gun with any questions they have about Natural Language Processing (NLP). \n",
    "#     If you're curious about how probability works in the context of NLP, feel free to ask any questions you may have. \n",
    "#     Whether it's about probabilistic models, language models, or any other related topic, \n",
    "#     I'm here to help break down complex concepts into easy-to-understand explanations.\n",
    "#     Just let me know what you're wondering about, and I'll do my best to guide you through it!\"\"\"\n",
    "#     )\n",
    "# )\n",
    "\n",
    "# prompt = OpenAIFunctionsAgent.create_prompt(\n",
    "#     system_message=system_message,\n",
    "# )\n",
    "\n",
    "# prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ef0e422a-9872-46b0-9948-f40a3829e0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.agents.openai_functions_agent.base import OpenAIFunctionsAgent\n",
    "# from langchain.agents import AgentExecutor\n",
    "\n",
    "# # We will use the OpenAIFunctionsAgent\n",
    "# agent = OpenAIFunctionsAgent(\n",
    "#     llm=llm, \n",
    "#     tools=tools, \n",
    "#     prompt=prompt,\n",
    "#     verbose=True,\n",
    "# )\n",
    "\n",
    "# agent_executor = AgentExecutor(\n",
    "#     agent=agent,\n",
    "#     tools=tools,\n",
    "#     memory=memory,\n",
    "#     verbose=True,\n",
    "#     return_intermediate_steps=True,\n",
    "# )\n",
    "\n",
    "# # Importantly, we pass in return_intermediate_steps=True since we are recording that with our memory object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d10ce2a7-ac56-42cb-9f68-7000994b4470",
   "metadata": {},
   "outputs": [],
   "source": [
    "# result = agent_executor({\"input\": \"Who are you by the way?\"})\n",
    "# result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4ba2a8cb-ff24-4514-ab9e-caf38e1ffc19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# result = agent_executor({\"input\": \"What is transformers?\"})\n",
    "# result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4052e0f-6087-4c31-b835-79670a1297c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain import hub\n",
    "\n",
    "# # Get the prompt to use - you can modify this!\n",
    "# prompt = hub.pull(\"hwchase17/openai-functions-agent\")\n",
    "\n",
    "# from langchain.agents import create_openai_functions_agent\n",
    "\n",
    "# agent = create_openai_functions_agent(\n",
    "#     llm, \n",
    "#     tools, \n",
    "#     prompt\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1ca5637-1d88-4b7b-9cda-d8668a4ebb5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.agents import AgentExecutor\n",
    "\n",
    "# agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "\n",
    "# agent_executor.invoke({\"input\": \"hi!\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf8b687",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
