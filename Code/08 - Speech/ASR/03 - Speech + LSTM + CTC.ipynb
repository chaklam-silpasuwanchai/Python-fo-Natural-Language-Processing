{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speech Recognition + LSTM + CTC + Torch Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Encoder network: processes the input sequence (e.g., acoustic features)\n",
    "    using a bidirectional LSTM and reduces the dimension with a linear layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers=1):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True, bidirectional=True)\n",
    "        self.fc   = nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: (batch, T, input_dim)\n",
    "        out, _ = self.lstm(x)   # out: (batch, T, 2 * hidden_dim)\n",
    "        out    = self.fc(out)      # out: (batch, T, hidden_dim)\n",
    "        return out\n",
    "\n",
    "class Predictor(nn.Module):\n",
    "    \"\"\"\n",
    "    Predictor network: a language model that predicts the next token based on previous tokens.\n",
    "    Uses an embedding layer and a unidirectional LSTM.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_layers=1):\n",
    "        super(Predictor, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.lstm      = nn.LSTM(embed_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        \n",
    "    def forward(self, y):\n",
    "        # y: (batch, U) target token sequence\n",
    "        embedded = self.embedding(y)  # (batch, U, embed_dim)\n",
    "        out, _   = self.lstm(embedded)    # (batch, U, hidden_dim)\n",
    "        return out\n",
    "\n",
    "class JointNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Joint network: combines encoder and predictor outputs using a specified combination mode.\n",
    "    \n",
    "    The available modes are:\n",
    "      - 'multiplicative' or 'mul': element-wise multiplication.\n",
    "      - 'additive' or 'add': element-wise addition.\n",
    "    \n",
    "    After combining the features, the network applies a tanh nonlinearity, then a fully connected layer\n",
    "    to project to the vocabulary space, and finally returns a probability distribution via softmax.\n",
    "    \"\"\"\n",
    "    MODES = {\n",
    "        'multiplicative': lambda f, g: f * g,\n",
    "        'mul': lambda f, g: f * g,\n",
    "        'additive': lambda f, g: f + g,\n",
    "        'add': lambda f, g: f + g\n",
    "    }\n",
    "    \n",
    "    def __init__(self, hidden_dim, joint_dim, vocab_size, mode='additive'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            hidden_dim (int): Dimension of the encoder and predictor outputs.\n",
    "            joint_dim (int): Dimension of the joint space.\n",
    "            vocab_size (int): Number of tokens in the vocabulary.\n",
    "            mode (str): Combination mode, one of 'multiplicative'/'mul' or 'additive'/'add'.\n",
    "        \"\"\"\n",
    "        super(JointNetwork, self).__init__()\n",
    "        self.join_mode = self.MODES[mode]\n",
    "        self.fc_enc    = nn.Linear(hidden_dim, joint_dim)\n",
    "        self.fc_pred   = nn.Linear(hidden_dim, joint_dim)\n",
    "        self.fc_out    = nn.Linear(joint_dim, vocab_size)\n",
    "        \n",
    "    def forward(self, enc_out, pred_out):\n",
    "        \"\"\"\n",
    "        Combines the encoder and predictor outputs.\n",
    "        \n",
    "        Args:\n",
    "            enc_out (Tensor): Encoder output of shape (batch, T, hidden_dim).\n",
    "            pred_out (Tensor): Predictor output of shape (batch, U, hidden_dim).\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: Vocabulary probability distribution of shape (batch, T, U, vocab_size).\n",
    "        \"\"\"\n",
    "        # Transform and expand dimensions for broadcasting:\n",
    "        f_enc = self.fc_enc(enc_out).unsqueeze(2)    # (batch, T, 1, joint_dim)\n",
    "        f_pred = self.fc_pred(pred_out).unsqueeze(1)    # (batch, 1, U, joint_dim)\n",
    "        \n",
    "        # Combine using the specified mode and apply tanh:\n",
    "        joint = torch.tanh(self.join_mode(f_enc, f_pred))  # (batch, T, U, joint_dim)\n",
    "        \n",
    "        # Project to the vocabulary space and return softmax probabilities:\n",
    "        logits = self.fc_out(joint)  # (batch, T, U, vocab_size)\n",
    "        return torch.softmax(logits, dim=-1)\n",
    "\n",
    "class RNNT(nn.Module):\n",
    "    \"\"\"\n",
    "    RNN-Transducer (RNN-T) model combining the Encoder, Predictor, and JointNetwork.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, vocab_size, encoder_hidden_dim,\n",
    "                 predictor_embed_dim, predictor_hidden_dim,\n",
    "                 joint_dim, encoder_layers=1, predictor_layers=1, joint_mode='additive'):\n",
    "        super(RNNT, self).__init__()\n",
    "        self.encoder   = Encoder(input_dim, encoder_hidden_dim, encoder_layers)\n",
    "        self.predictor = Predictor(vocab_size, predictor_embed_dim, predictor_hidden_dim, predictor_layers)\n",
    "        self.joint     = JointNetwork(encoder_hidden_dim, joint_dim, vocab_size, mode=joint_mode)\n",
    "        \n",
    "    def forward(self, x, y):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (Tensor): Input sequence (e.g., acoustic features) of shape (batch, T, input_dim).\n",
    "            y (Tensor): Target token sequence of shape (batch, U).\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: Vocabulary distribution of shape (batch, T, U, vocab_size).\n",
    "        \"\"\"\n",
    "        enc_out  = self.encoder(x)     # (batch, T, hidden_dim)\n",
    "        pred_out = self.predictor(y)  # (batch, U, hidden_dim)\n",
    "        logits   = self.joint(enc_out, pred_out)  # (batch, T, U, vocab_size)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits shape: torch.Size([2, 50, 20, 30])\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 2\n",
    "T = 50   # Length of input sequence (e.g., number of acoustic frames)\n",
    "U = 20   # Length of target token sequence\n",
    "input_dim = 40      # Dimension of input features\n",
    "vocab_size = 30     # Vocabulary size (including blank token)\n",
    "encoder_hidden_dim = 256\n",
    "predictor_embed_dim = 128\n",
    "predictor_hidden_dim = 256\n",
    "joint_dim = 512\n",
    "\n",
    "# Instantiate the model\n",
    "model = RNNT(\n",
    "    input_dim, \n",
    "    vocab_size, \n",
    "    encoder_hidden_dim,\n",
    "    predictor_embed_dim, \n",
    "    predictor_hidden_dim, \n",
    "    joint_dim\n",
    ")\n",
    "\n",
    "# Dummy input: acoustic features and target sequences\n",
    "x = torch.randn(batch_size, T, input_dim)\n",
    "y = torch.randint(0, vocab_size, (batch_size, U))\n",
    "\n",
    "# Forward pass: obtain logits over the vocabulary\n",
    "logits = model(x, y)\n",
    "print(\"Logits shape:\", logits.shape)  # Expected shape: (batch, T, U, vocab_size)\n",
    "\n",
    "# To compute loss, you would typically use an RNN-T loss function,\n",
    "# for example, from a third-party implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
