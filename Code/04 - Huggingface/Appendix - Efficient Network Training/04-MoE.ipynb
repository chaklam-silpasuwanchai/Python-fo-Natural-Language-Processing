{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mixture-of-Experts (MoE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sparse-Gated Mixture of Experts in LSTM [Shazeer et al. ICLR 2017](https://arxiv.org/pdf/1701.06538)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Gating network based on LSTM\n",
    "class LSTMGatingNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_experts):\n",
    "        super(LSTMGatingNetwork, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, num_experts)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch, seq_len, input_dim)\n",
    "        lstm_out, _ = self.lstm(x)              # lstm_out: (batch, seq_len, hidden_dim)\n",
    "        scores = self.fc(lstm_out)             # scores: (batch, seq_len, num_experts)\n",
    "        weights = torch.softmax(scores, dim=-1) # convert to probabilities per time step\n",
    "        return weights\n",
    "\n",
    "# Mixture-of-Experts model with LSTM gating\n",
    "class LSTMMoE(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_experts, expert_network=None):\n",
    "        super(LSTMMoE, self).__init__()\n",
    "        # Gating network (LSTM-based)\n",
    "        self.gating_network = LSTMGatingNetwork(input_dim, hidden_dim, num_experts)\n",
    "        # Expert networks (if not provided, use simple linear layers as experts)\n",
    "        if expert_network is None:\n",
    "            # Default: each expert is a Linear layer from input_dim -> output_dim\n",
    "            self.experts = nn.ModuleList([nn.Linear(input_dim, output_dim) for _ in range(num_experts)])\n",
    "        else:\n",
    "            # If a custom expert network class is provided, instantiate for each expert\n",
    "            self.experts = nn.ModuleList([expert_network() for _ in range(num_experts)])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch, seq_len, input_dim)\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        # 1. Get gating weights from LSTM gating network\n",
    "        gating_weights = self.gating_network(x)       # shape: (batch, seq_len, num_experts)\n",
    "        # 2. Compute outputs of each expert on the inputs\n",
    "        expert_outputs = [expert(x) for expert in self.experts]  # list of tensors, each (batch, seq_len, output_dim)\n",
    "        expert_outputs = torch.stack(expert_outputs, dim=-1)     # shape: (batch, seq_len, output_dim, num_experts)\n",
    "        # 3. Weight and sum expert outputs using the gating weights\n",
    "        weights_expanded = gating_weights.unsqueeze(-2)          # shape: (batch, seq_len, 1, num_experts)\n",
    "        combined_output = (expert_outputs * weights_expanded).sum(dim=-1)  # (batch, seq_len, output_dim)\n",
    "        return combined_output\n",
    "\n",
    "# Example usage:\n",
    "input_dim, hidden_dim, output_dim, num_experts = 4, 8, 3, 2\n",
    "model = LSTMMoE(input_dim, hidden_dim, output_dim, num_experts)\n",
    "# Dummy input: batch of 1 sequence, length 5, feature dim 4\n",
    "x = torch.randn(1, 5, input_dim)\n",
    "y = model(x)\n",
    "print(\"Input shape:\", x.shape)\n",
    "print(\"Output shape:\", y.shape)\n",
    "print(\"Output:\", y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output (example):\n",
    "# Input shape: torch.Size([1, 5, 4])  \n",
    "# Output shape: torch.Size([1, 5, 3])  \n",
    "# Output: tensor([[[ 0.1991, -0.2271, -0.4974],\n",
    "#          [-0.0026, -0.2181, -0.4217],\n",
    "#          [-0.1261, -0.1725,  0.1611],\n",
    "#          [-0.1749,  0.2343, -0.2493],\n",
    "#          [ 0.2959,  0.3869, -0.7265]]], grad_fn=<SumBackward1>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity. [Fedus et al, ICLR, 2021](https://arxiv.org/pdf/2101.03961)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Switch Transformer style MoE layer\n",
    "class SwitchMoE(nn.Module):\n",
    "    def __init__(self, input_dim, expert_hidden_dim, output_dim, num_experts):\n",
    "        super(SwitchMoE, self).__init__()\n",
    "        self.num_experts = num_experts\n",
    "        # Gating (router) network: a linear layer that scores each expert for a token\n",
    "        self.gate = nn.Linear(input_dim, num_experts)\n",
    "        # Expert networks: each is a feed-forward MLP (two linear layers with ReLU)\n",
    "        self.experts = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(input_dim, expert_hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(expert_hidden_dim, output_dim)\n",
    "            ) for _ in range(num_experts)\n",
    "        ])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch, seq_len, input_dim)\n",
    "        batch_size, seq_len, dim = x.size()\n",
    "        # 1. Compute gating scores for each token and select top-1 expert\n",
    "        gating_scores = self.gate(x)                        # (batch, seq_len, num_experts)\n",
    "        expert_indices = gating_scores.argmax(dim=-1)       # (batch, seq_len) index of chosen expert per token\n",
    "        \n",
    "        # 2. Prepare an output tensor\n",
    "        output = torch.zeros(batch_size, seq_len, self.experts[0][-1].out_features)\n",
    "        \n",
    "        # 3. Route tokens to their chosen experts and compute expert outputs\n",
    "        # Flatten batch and sequence dimensions for easier indexing\n",
    "        x_flat = x.view(-1, dim)                            # shape: (batch*seq_len, input_dim)\n",
    "        indices_flat = expert_indices.view(-1)              # shape: (batch*seq_len,)\n",
    "        output_flat = torch.zeros(x_flat.size(0), self.experts[0][-1].out_features)\n",
    "        # Process tokens group by expert to avoid loop over each token\n",
    "        for expert_idx in range(self.num_experts):\n",
    "            mask = (indices_flat == expert_idx)\n",
    "            if mask.any():\n",
    "                # select all tokens assigned to this expert\n",
    "                tokens = x_flat[mask]                      # shape: (n_tokens_for_expert, input_dim)\n",
    "                # compute outputs for these tokens using the expert\n",
    "                tokens_out = self.experts[expert_idx](tokens)  # (n_tokens_for_expert, output_dim)\n",
    "                output_flat[mask] = tokens_out             # place outputs in the corresponding positions\n",
    "        # Reshape back to (batch, seq_len, output_dim)\n",
    "        output = output_flat.view(batch_size, seq_len, -1)\n",
    "        return output\n",
    "\n",
    "# Example usage:\n",
    "input_dim = 5\n",
    "output_dim = 5   # usually same as input_dim in transformer for residual connection\n",
    "num_experts = 3\n",
    "expert_hidden_dim = 10  # hidden layer size in each expert FFN\n",
    "\n",
    "model = SwitchMoE(input_dim, expert_hidden_dim, output_dim, num_experts)\n",
    "# Dummy input: batch of 2 sequences, each with 4 tokens (seq_len=4), token feature dim=5\n",
    "x = torch.randn(2, 4, input_dim)\n",
    "y = model(x)\n",
    "print(\"Input shape:\", x.shape)\n",
    "print(\"Output shape:\", y.shape)\n",
    "print(\"Token-to-Expert assignments:\\n\", model.gate(x).argmax(dim=-1))\n",
    "print(\"Output:\\n\", y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output (example):\n",
    "# Input shape: torch.Size([2, 4, 5])  \n",
    "# Output shape: torch.Size([2, 4, 5])  \n",
    "# Token-to-Expert assignments:\n",
    "#  tensor([[1, 1, 1, 0],\n",
    "#         [1, 0, 2, 1]])  \n",
    "# Output:\n",
    "#  tensor([[[ 0.4486, -0.2896, -0.2615, -0.2078,  0.0117],\n",
    "#           [ 0.4096, -0.3731, -0.2567, -0.2418, -0.0269],\n",
    "#           [ 0.3724, -0.5101, -0.2314, -0.2437,  0.0134],\n",
    "#           [-0.1222, -0.6251,  0.2697,  0.2317, -0.2568]],\n",
    "\n",
    "#          [[ 0.5131, -0.3547, -0.3457, -0.1203,  0.1192],\n",
    "#           [-0.0289, -0.3413,  0.2114,  0.0775,  0.0413],\n",
    "#           [-0.2445,  0.0563,  0.1714,  0.2636,  0.3997],\n",
    "#           [ 0.3778, -0.4098, -0.2812, -0.3058, -0.2247]]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
