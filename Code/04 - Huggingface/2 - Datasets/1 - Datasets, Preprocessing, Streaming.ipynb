{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mUQBvyUdHCry"
   },
   "source": [
    "# Natural Language Processing\n",
    "\n",
    "## Datasets, Preprocessing, Streaming\n",
    "\n",
    "## 1. Working on your own file\n",
    "\n",
    "We‚Äôll show you how Datasets can be used to load datasets that aren‚Äôt available on the Hugging Face Hub.\n",
    "\n",
    "### Working with local and remote datasets\n",
    "\n",
    "Datasets provides loading scripts to handle the loading of local and remote datasets.\n",
    "\n",
    "|Data format\t   | Loading script\t    |Example                                                 |\n",
    "| ---              | ---                | ---                                                    |\n",
    "|CSV & TSV         | csv                | `load_dataset(\"csv\", data_files=\"my_file.csv\")`        |\n",
    "|Text files\t       | text\t            | `load_dataset(\"text\", data_files=\"my_file.txt\")`       |\n",
    "|JSON & JSON Lines | json\t            | `load_dataset(\"json\", data_files=\"my_file.jsonl\")`     |\n",
    "|Pickled DataFrames|pandas\t            | `load_dataset(\"pandas\", data_files=\"my_dataframe.pkl\")`|\n",
    "\n",
    "### Loading a local dataset\n",
    "\n",
    "For this example we‚Äôll use some drug effectiveness dataset:\n",
    "\n",
    "You can use the `wget` in your terminal:\n",
    "\n",
    "    wget -P data/ \"https://archive.ics.uci.edu/ml/machine-learning-databases/00462/drugsCom_raw.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#comment this if you are using puffer or tokyo\n",
    "import os\n",
    "os.environ['http_proxy'] = 'http://192.41.170.23:3128'\n",
    "os.environ['https_proxy'] = 'http://192.41.170.23:3128'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since TSV is just a variant of CSV that uses tabs instead of commas as the separator, we can load these files by using the csv loading script and specifying the delimiter argument in the load_dataset() function as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "nazOEfkPHCr8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-9f769bd66dd9ad28\n",
      "Found cached dataset csv (/home/chaklams/.cache/huggingface/datasets/csv/default-9f769bd66dd9ad28/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "580bfdc2d344480197b5022e75add27c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "drug_dataset = load_dataset(\"csv\", data_files=\"data/drugsComTrain_raw.tsv\", delimiter=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, loading local files creates a `DatasetDict` object with a train split. We can see this by inspecting the `drug_dataset` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "h_ggsHbbHCr9",
    "outputId": "aba6f4f0-5c64-4b23-b045-d8c2fe44f962"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Unnamed: 0', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount'],\n",
       "        num_rows: 161297\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drug_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows us the number of rows and the column names associated with the training set.\n",
    "\n",
    "A good practice when doing any sort of data analysis is to grab a small random sample to get a quick feel for the type of data you‚Äôre working with. In ü§ó Datasets, we can create a random sample by chaining the `Dataset.shuffle()` and `Dataset.select()` functions together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Nkna_DnLHCr-",
    "outputId": "ffbad0b3-dfe3-4cc7-d9d7-fdbb3fbb3832"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at /home/chaklams/.cache/huggingface/datasets/csv/default-9f769bd66dd9ad28/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-370401d6b4fc92a0.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Unnamed: 0': [87571, 178045, 80482],\n",
       " 'drugName': ['Naproxen', 'Duloxetine', 'Mobic'],\n",
       " 'condition': ['Gout, Acute', 'ibromyalgia', 'Inflammatory Conditions'],\n",
       " 'review': ['\"like the previous person mention, I&#039;m a strong believer of aleve, it works faster for my gout than the prescription meds I take. No more going to the doctor for refills.....Aleve works!\"',\n",
       "  '\"I have taken Cymbalta for about a year and a half for fibromyalgia pain. It is great\\r\\nas a pain reducer and an anti-depressant, however, the side effects outweighed \\r\\nany benefit I got from it. I had trouble with restlessness, being tired constantly,\\r\\ndizziness, dry mouth, numbness and tingling in my feet, and horrible sweating. I am\\r\\nbeing weaned off of it now. Went from 60 mg to 30mg and now to 15 mg. I will be\\r\\noff completely in about a week. The fibro pain is coming back, but I would rather deal with it than the side effects.\"',\n",
       "  '\"I have been taking Mobic for over a year with no side effects other than an elevated blood pressure.  I had severe knee and ankle pain which completely went away after taking Mobic.  I attempted to stop the medication however pain returned after a few days.\"'],\n",
       " 'rating': [9.0, 3.0, 10.0],\n",
       " 'date': ['September 2, 2015', 'November 7, 2011', 'June 5, 2013'],\n",
       " 'usefulCount': [36, 13, 128]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drug_sample = drug_dataset[\"train\"].shuffle(seed=42).select(range(1000))\n",
    "# Peek at the first few examples\n",
    "drug_sample[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, we‚Äôve loaded our first local dataset! But while this worked for the training set, what we really want is to include both the train and test splits in a single DatasetDict object so we can apply `Dataset.map()` functions across both splits at once. To do this, we can provide a dictionary to the `data_files` argument that maps each split name to a file associated with that split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "3DIHGE2KHCr_",
    "outputId": "ba3b327b-36d4-4f58-8845-158a1b541722"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-781d37ea3f205c6f\n",
      "Found cached dataset csv (/home/chaklams/.cache/huggingface/datasets/csv/default-781d37ea3f205c6f/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ea19661c53942fc89e59a5910b81ebd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data_files = {\"train\": \"data/drugsComTrain_raw.tsv\", \"test\": \"data/drugsComTest_raw.tsv\"}\n",
    "drug_dataset = load_dataset(\"csv\", data_files=data_files, delimiter=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming datasets\n",
    "\n",
    "To enable dataset streaming you just need to pass the `streaming=True` argument to the load_dataset() function. Streaming mode will make sure your PC won't have to load the whole dataset at once..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "7lcoqWEot1W3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-781d37ea3f205c6f\n"
     ]
    }
   ],
   "source": [
    "drug_dataset_streamed = load_dataset(\"csv\", data_files=data_files, delimiter=\"\\t\", streaming=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of the familiar `Dataset`, the object returned with `streaming=True` is an `IterableDataset`. As the name suggests, to access the elements of an `IterableDataset` we need to iterate over it. We can access the first element of our streamed dataset as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datasets.dataset_dict.IterableDatasetDict"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(drug_dataset_streamed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': <datasets.iterable_dataset.IterableDataset at 0x7f676cb10a90>,\n",
       " 'test': <datasets.iterable_dataset.IterableDataset at 0x7f6766e695b0>}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drug_dataset_streamed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "WCrImefDt1W3",
    "outputId": "0fad9792-fe91-4c44-d84e-fb16c260b0d2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chaklams/.local/lib/python3.8/site-packages/datasets/download/streaming_download_manager.py:727: FutureWarning: the 'mangle_dupe_cols' keyword is deprecated and will be removed in a future version. Please take steps to stop the use of 'mangle_dupe_cols'\n",
      "  return pd.read_csv(xopen(filepath_or_buffer, \"rb\", use_auth_token=use_auth_token), **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Unnamed: 0': 206461,\n",
       " 'drugName': 'Valsartan',\n",
       " 'condition': 'Left Ventricular Dysfunction',\n",
       " 'review': '\"It has no side effect, I take it in combination of Bystolic 5 Mg and Fish Oil\"',\n",
       " 'rating': 9.0,\n",
       " 'date': 'May 20, 2012',\n",
       " 'usefulCount': 27}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(drug_dataset_streamed['train']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The elements from a streamed dataset can be processed on the fly using `IterableDataset.map()`, which is useful during training if you need to tokenize the inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "jPvuOb4pt1W4",
    "outputId": "be6cdee6-2afe-4490-ab19-db6db45a69cf"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "tokenized_dataset = drug_dataset_streamed.map(lambda x: tokenizer(x[\"review\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chaklams/.local/lib/python3.8/site-packages/datasets/download/streaming_download_manager.py:727: FutureWarning: the 'mangle_dupe_cols' keyword is deprecated and will be removed in a future version. Please take steps to stop the use of 'mangle_dupe_cols'\n",
      "  return pd.read_csv(xopen(filepath_or_buffer, \"rb\", use_auth_token=use_auth_token), **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Unnamed: 0': 206461,\n",
       " 'drugName': 'Valsartan',\n",
       " 'condition': 'Left Ventricular Dysfunction',\n",
       " 'review': '\"It has no side effect, I take it in combination of Bystolic 5 Mg and Fish Oil\"',\n",
       " 'rating': 9.0,\n",
       " 'date': 'May 20, 2012',\n",
       " 'usefulCount': 27,\n",
       " 'input_ids': [101,\n",
       "  1000,\n",
       "  2009,\n",
       "  2038,\n",
       "  2053,\n",
       "  2217,\n",
       "  3466,\n",
       "  1010,\n",
       "  1045,\n",
       "  2202,\n",
       "  2009,\n",
       "  1999,\n",
       "  5257,\n",
       "  1997,\n",
       "  2011,\n",
       "  16033,\n",
       "  10415,\n",
       "  1019,\n",
       "  11460,\n",
       "  1998,\n",
       "  3869,\n",
       "  3514,\n",
       "  1000,\n",
       "  102],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(tokenized_dataset['train']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also shuffle a streamed dataset using `IterableDataset.shuffle()`, but unlike `Dataset.shuffle()` this only shuffles the elements in a predefined buffer_size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "viW1uxjot1W5",
    "outputId": "9d0e89e9-8cae-4785-9e89-5de1c11ac150"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chaklams/.local/lib/python3.8/site-packages/datasets/download/streaming_download_manager.py:727: FutureWarning: the 'mangle_dupe_cols' keyword is deprecated and will be removed in a future version. Please take steps to stop the use of 'mangle_dupe_cols'\n",
      "  return pd.read_csv(xopen(filepath_or_buffer, \"rb\", use_auth_token=use_auth_token), **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Unnamed: 0': 175494,\n",
       " 'drugName': 'Scopolamine',\n",
       " 'condition': 'Motion Sickness',\n",
       " 'review': '\"The reason I&#039;m giving it a 9 instead of 10 is because of it&#039;s side effects. Otherwise, this patch works really well for an overnight or two day fishing trip (at least in my experience). I thought I&#039;m getting cold and feeling sick after I get off the fishing boat because my throat was getting dry and dryer and dryer. At that time I&#039;m not aware that one of it&#039;s side effects was dry mouth/throat. Aside from that, I didn&#039;t feel anything anymore after couple of days. I&#039;ll use it again for my next fishing trip. I&#039;ll opt for a dry throat verses getting sick on the boat.\"',\n",
       " 'rating': 9.0,\n",
       " 'date': 'August 24, 2010',\n",
       " 'usefulCount': 9}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shuffled_dataset = drug_dataset_streamed.shuffle(buffer_size=10_000, seed=42)\n",
    "next(iter(shuffled_dataset['train']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we selected a random example from the first 10,000 examples in the buffer. Once an example is accessed, its spot in the buffer is filled with the next example in the corpus (i.e., the 10,001st example in the case above). You can also select elements from a streamed dataset using the `IterableDataset.take()` and `IterableDataset.skip()` functions, which act in a similar way to `Dataset.select()`. For example, to select the first 5 examples in the dataset we can do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "U8O8T-SCt1W5",
    "outputId": "d7194dc6-7ab6-4e26-bb61-58feb6d972a6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chaklams/.local/lib/python3.8/site-packages/datasets/download/streaming_download_manager.py:727: FutureWarning: the 'mangle_dupe_cols' keyword is deprecated and will be removed in a future version. Please take steps to stop the use of 'mangle_dupe_cols'\n",
      "  return pd.read_csv(xopen(filepath_or_buffer, \"rb\", use_auth_token=use_auth_token), **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'Unnamed: 0': 206461,\n",
       "  'drugName': 'Valsartan',\n",
       "  'condition': 'Left Ventricular Dysfunction',\n",
       "  'review': '\"It has no side effect, I take it in combination of Bystolic 5 Mg and Fish Oil\"',\n",
       "  'rating': 9.0,\n",
       "  'date': 'May 20, 2012',\n",
       "  'usefulCount': 27},\n",
       " {'Unnamed: 0': 95260,\n",
       "  'drugName': 'Guanfacine',\n",
       "  'condition': 'ADHD',\n",
       "  'review': '\"My son is halfway through his fourth week of Intuniv. We became concerned when he began this last week, when he started taking the highest dose he will be on. For two days, he could hardly get out of bed, was very cranky, and slept for nearly 8 hours on a drive home from school vacation (very unusual for him.) I called his doctor on Monday morning and she said to stick it out a few days. See how he did at school, and with getting up in the morning. The last two days have been problem free. He is MUCH more agreeable than ever. He is less emotional (a good thing), less cranky. He is remembering all the things he should. Overall his behavior is better. \\r\\nWe have tried many different medications and so far this is the most effective.\"',\n",
       "  'rating': 8.0,\n",
       "  'date': 'April 27, 2010',\n",
       "  'usefulCount': 192},\n",
       " {'Unnamed: 0': 92703,\n",
       "  'drugName': 'Lybrel',\n",
       "  'condition': 'Birth Control',\n",
       "  'review': '\"I used to take another oral contraceptive, which had 21 pill cycle, and was very happy- very light periods, max 5 days, no other side effects. But it contained hormone gestodene, which is not available in US, so I switched to Lybrel, because the ingredients are similar. When my other pills ended, I started Lybrel immediately, on my first day of period, as the instructions said. And the period lasted for two weeks. When taking the second pack- same two weeks. And now, with third pack things got even worse- my third period lasted for two weeks and now it&#039;s the end of the third week- I still have daily brown discharge.\\r\\nThe positive side is that I didn&#039;t have any other side effects. The idea of being period free was so tempting... Alas.\"',\n",
       "  'rating': 5.0,\n",
       "  'date': 'December 14, 2009',\n",
       "  'usefulCount': 17},\n",
       " {'Unnamed: 0': 138000,\n",
       "  'drugName': 'Ortho Evra',\n",
       "  'condition': 'Birth Control',\n",
       "  'review': '\"This is my first time using any form of birth control. I&#039;m glad I went with the patch, I have been on it for 8 months. At first It decreased my libido but that subsided. The only downside is that it made my periods longer (5-6 days to be exact) I used to only have periods for 3-4 days max also made my cramps intense for the first two days of my period, I never had cramps before using birth control. Other than that in happy with the patch\"',\n",
       "  'rating': 8.0,\n",
       "  'date': 'November 3, 2015',\n",
       "  'usefulCount': 10},\n",
       " {'Unnamed: 0': 35696,\n",
       "  'drugName': 'Buprenorphine / naloxone',\n",
       "  'condition': 'Opiate Dependence',\n",
       "  'review': '\"Suboxone has completely turned my life around.  I feel healthier, I&#039;m excelling at my job and I always have money in my pocket and my savings account.  I had none of those before Suboxone and spent years abusing oxycontin.  My paycheck was already spent by the time I got it and I started resorting to scheming and stealing to fund my addiction.  All that is history.  If you&#039;re ready to stop, there&#039;s a good chance that suboxone will put you on the path of great life again.  I have found the side-effects to be minimal compared to oxycontin.  I&#039;m actually sleeping better.   Slight constipation is about it for me.  It truly is amazing. The cost pales in comparison to what I spent on oxycontin.\"',\n",
       "  'rating': 9.0,\n",
       "  'date': 'November 27, 2016',\n",
       "  'usefulCount': 37}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_head = drug_dataset_streamed['train'].take(5)\n",
    "list(dataset_head)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, you can use the `IterableDataset.skip()` function to create training and validation splits from a shuffled dataset as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "zTlZdH2Ft1W6"
   },
   "outputs": [],
   "source": [
    "# Skip the first 1,000 examples and include the rest in the training set\n",
    "train_dataset = shuffled_dataset['train'].skip(1000)\n",
    "# Take the first 1,000 examples for the validation set\n",
    "validation_dataset = shuffled_dataset['train'].take(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For preprocessing part, we shall use the non-streamed version, since our data is not that big anyway...but remember to use streaming when your data is like >5GB.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocessing\n",
    "\n",
    "From this sample we can already see a few quirks in our dataset:\n",
    "\n",
    "- The `Unnamed: 0` column looks suspiciously like an anonymized ID for each patient.\n",
    "- The `condition` column includes a mix of uppercase and lowercase labels.\n",
    "- The `reviews` are of varying length and contain a mix of Python line separators (\\r\\n) as well as HTML character codes like &\\#039;.\n",
    "\n",
    "### Rename patient id\n",
    "\n",
    "Let‚Äôs see how we can use Datasets to deal with each of these issues. To test the patient ID hypothesis for the `Unnamed: 0` column, we can use the `Dataset.unique()` function to verify that the number of IDs matches the number of rows in each split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "dTyC5vj4tyxF"
   },
   "outputs": [],
   "source": [
    "for split in drug_dataset.keys():\n",
    "    assert len(drug_dataset[split]) == len(drug_dataset[split].unique(\"Unnamed: 0\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seems to confirm our hypothesis, so let‚Äôs clean up the dataset a bit by renaming the Unnamed: 0 column to something a bit more interpretable. We can use the `DatasetDict.rename_column()` function to rename the column across both splits in one go:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "_BPfOcQatyxF",
    "outputId": "3f813c69-60ab-4d10-d9f1-8ee1eb9aec85"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount'],\n",
       "        num_rows: 161297\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount'],\n",
       "        num_rows: 53766\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drug_dataset = drug_dataset.rename_column(\n",
    "    original_column_name=\"Unnamed: 0\", new_column_name=\"patient_id\"\n",
    ")\n",
    "drug_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lowercasing condition\n",
    "\n",
    "Before we perform lowercasing, we got some problem with our `condition` since some is `None` (which cannot be lowercased). Let‚Äôs first drop these rows using `Dataset.filter()`, which works in a similar way to `Dataset.map()` and expects a function that receives a single example of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "4VDdNb88tyxJ"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/chaklams/.cache/huggingface/datasets/csv/default-781d37ea3f205c6f/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-e1e956c9dd698081.arrow\n",
      "Loading cached processed dataset at /home/chaklams/.cache/huggingface/datasets/csv/default-781d37ea3f205c6f/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-7f566e92a92fcfa3.arrow\n"
     ]
    }
   ],
   "source": [
    "drug_dataset = drug_dataset.filter(lambda x: x[\"condition\"] is not None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the None entries removed, we can now normalize our `condition` column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "DeluzHRjtyxK",
    "outputId": "fc76b700-3ef1-4bf6-8188-d06aa44727fb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/chaklams/.cache/huggingface/datasets/csv/default-781d37ea3f205c6f/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-aec27311b11c5a64.arrow\n",
      "Loading cached processed dataset at /home/chaklams/.cache/huggingface/datasets/csv/default-781d37ea3f205c6f/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-55f2db5ea45774d7.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['left ventricular dysfunction', 'adhd', 'birth control']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def lowercase_condition(example):\n",
    "    return {\"condition\": example[\"condition\"].lower()}\n",
    "\n",
    "drug_dataset = drug_dataset.map(lowercase_condition)\n",
    "# Check that lowercasing worked\n",
    "drug_dataset[\"train\"][\"condition\"][:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It works! Now that we‚Äôve cleaned up the labels, let‚Äôs take a look at cleaning up the reviews themselves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review length\n",
    "\n",
    "Whenever you‚Äôre dealing with customer reviews, a good practice is to check the number of words in each review. A review might be just a single word like ‚ÄúGreat!‚Äù or a full-blown essay with thousands of words, and depending on the use case you‚Äôll need to handle these extremes differently. To compute the number of words in each review, we‚Äôll use a rough heuristic based on splitting each text by whitespace.\n",
    "\n",
    "Let‚Äôs define a simple function that counts the number of words in each review:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "zHrACxvctyxK"
   },
   "outputs": [],
   "source": [
    "def compute_review_length(example):\n",
    "    return {\"review_length\": len(example[\"review\"].split())}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike our `lowercase_condition()` function, `compute_review_length()` returns a dictionary whose key does not correspond to one of the column names in the dataset. In this case, when `compute_review_length()` is passed to `Dataset.map()`, it will be applied to all the rows in the dataset to create a new `review_length` column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "axkRC353tyxK",
    "outputId": "bd79e1d9-52e3-45d9-dd23-c32c6d689266"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/chaklams/.cache/huggingface/datasets/csv/default-781d37ea3f205c6f/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-a38bf2ee84af9ffd.arrow\n",
      "Loading cached processed dataset at /home/chaklams/.cache/huggingface/datasets/csv/default-781d37ea3f205c6f/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-32b756afafc251f7.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'patient_id': 206461,\n",
       " 'drugName': 'Valsartan',\n",
       " 'condition': 'left ventricular dysfunction',\n",
       " 'review': '\"It has no side effect, I take it in combination of Bystolic 5 Mg and Fish Oil\"',\n",
       " 'rating': 9.0,\n",
       " 'date': 'May 20, 2012',\n",
       " 'usefulCount': 27,\n",
       " 'review_length': 17}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drug_dataset = drug_dataset.map(compute_review_length)\n",
    "# Inspect the first training example\n",
    "drug_dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, we can see a review_length column has been added to our training set. We can sort this new column with Dataset.sort() to see what the extreme values look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "wGJkKwBqtyxL",
    "outputId": "01a4313f-16a5-4465-bbb4-12cd93648b7b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached sorted indices for dataset at /home/chaklams/.cache/huggingface/datasets/csv/default-781d37ea3f205c6f/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-5e9a51a8850f19c1.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'patient_id': [103488, 23627, 20558],\n",
       " 'drugName': ['Loestrin 21 1 / 20', 'Chlorzoxazone', 'Nucynta'],\n",
       " 'condition': ['birth control', 'muscle spasm', 'pain'],\n",
       " 'review': ['\"Excellent.\"', '\"useless\"', '\"ok\"'],\n",
       " 'rating': [10.0, 1.0, 6.0],\n",
       " 'date': ['November 4, 2008', 'March 24, 2017', 'August 20, 2016'],\n",
       " 'usefulCount': [5, 2, 10],\n",
       " 'review_length': [1, 1, 1]}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drug_dataset[\"train\"].sort(\"review_length\")[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we suspected, some reviews contain just a single word, which, although it may be okay for sentiment analysis, would not be informative if we want to predict the condition.\n",
    "\n",
    "Note: An alternative way to add new columns to a dataset is with the `Dataset.add_column()` function. This allows you to provide the column as a Python list or NumPy array and can be handy in situations where Dataset.map() is not well suited for your analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let‚Äôs use the `Dataset.filter()` function to remove reviews that contain fewer than 30 words. Similarly to what we did with the `condition` column, we can filter out the very short reviews by requiring that the reviews have a length above this threshold:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "2BUijixttyxL",
    "outputId": "08def8d9-eb3a-49bf-89b0-c5d7b467d37f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/chaklams/.cache/huggingface/datasets/csv/default-781d37ea3f205c6f/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-ed2706c7bc365f9a.arrow\n",
      "Loading cached processed dataset at /home/chaklams/.cache/huggingface/datasets/csv/default-781d37ea3f205c6f/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-aeee41f7dc4896e4.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 138514, 'test': 46108}\n"
     ]
    }
   ],
   "source": [
    "drug_dataset = drug_dataset.filter(lambda x: x[\"review_length\"] > 30)\n",
    "print(drug_dataset.num_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, this has removed around 15% of the reviews from our original training and test sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Escaping review html\n",
    "\n",
    "The last thing we need to deal with is the presence of HTML character codes in our reviews. We can use Python‚Äôs `html` module to unescape these characters, like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "jh8OOh88tyxM",
    "outputId": "4e602e22-b19b-4455-816b-0c3fca61687b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm a transformer called BERT\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import html\n",
    "\n",
    "text = \"I&#039;m a transformer called BERT\"\n",
    "html.unescape(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We‚Äôll use `Dataset.map()` to unescape all the HTML characters in our corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "pbiVpvCdtyxM"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/chaklams/.cache/huggingface/datasets/csv/default-781d37ea3f205c6f/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-00f0922078d99345.arrow\n",
      "Loading cached processed dataset at /home/chaklams/.cache/huggingface/datasets/csv/default-781d37ea3f205c6f/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-e1342b3677faf817.arrow\n"
     ]
    }
   ],
   "source": [
    "drug_dataset = drug_dataset.map(lambda x: {\"review\": html.unescape(x[\"review\"])})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the `Dataset.map()` method is quite useful for processing data ‚Äî and we haven‚Äôt even scratched the surface of everything it can do!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Faster preprocessing\n",
    "\n",
    "### Batched=True\n",
    "\n",
    "The `Dataset.map()` method takes a batched argument that, if set to `True`, causes it to send a batch of examples to the map function at once (the batch size is configurable but defaults to 1,000) which is faster.\n",
    "\n",
    "When you specify `batched=True` the function receives a dictionary with the fields of the dataset, but each value is now a list of values, and not just a single value. The return value of `Dataset.map()` should be the same: a dictionary with the fields we want to update or add to our dataset, and a list of values. For example, here is another way to unescape all HTML characters, but using `batched=True`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "4y-OqJl8tyxM"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/chaklams/.cache/huggingface/datasets/csv/default-781d37ea3f205c6f/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-be1beff3f18025aa.arrow\n",
      "Loading cached processed dataset at /home/chaklams/.cache/huggingface/datasets/csv/default-781d37ea3f205c6f/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-6ef7168e831810d5.arrow\n"
     ]
    }
   ],
   "source": [
    "new_drug_dataset = drug_dataset.map(lambda x: {\"review\": html.unescape(x[\"review\"])}, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you‚Äôre running this code in a notebook, you‚Äôll see that this command executes way faster than the previous one.  This is \n",
    "because we gain some performance by accessing lots of elements at the same time instead of one by one.\n",
    "\n",
    "### Fast tokenizers\n",
    "\n",
    "Using `Dataset.map()` with `batched=True` will be essential to unlock the speed of the ‚Äúfast‚Äù tokenizers that we‚Äôll encounter later, which can quickly tokenize big lists of texts. Let's compare (1) slow vs. fast tokenizier, (2) batched vs no batched, and (3) num_proc vs no num_proc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# #slow tokenizer + batched=True\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\", use_fast=False)\n",
    "\n",
    "# def tokenize_function(examples):\n",
    "#     return tokenizer(examples[\"review\"], truncation=True)\n",
    "\n",
    "# %time tokenized_dataset = drug_dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #slow tokenizer + batched=False\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\", use_fast=False)\n",
    "\n",
    "# def tokenize_function(examples):\n",
    "#     return tokenizer(examples[\"review\"], truncation=True)\n",
    "\n",
    "# %time tokenized_dataset = drug_dataset.map(tokenize_function, batched=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "cUKn7R8ItyxN"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/chaklams/.cache/huggingface/datasets/csv/default-781d37ea3f205c6f/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-aa6d0b174526a4c1.arrow\n",
      "Loading cached processed dataset at /home/chaklams/.cache/huggingface/datasets/csv/default-781d37ea3f205c6f/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-ec4be806df6da2a1.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.9 ms, sys: 3.96 ms, total: 15.9 ms\n",
      "Wall time: 15.5 ms\n"
     ]
    }
   ],
   "source": [
    "#fast tokenizer + batched=True\n",
    "#recommended :)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\", use_fast=True) #default is already True\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"review\"], truncation=True)\n",
    "\n",
    "%time tokenized_dataset = drug_dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #fast tokenizer + batched=False\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\", use_fast=True) #default is already True\n",
    "\n",
    "# def tokenize_function(examples):\n",
    "#     return tokenizer(examples[\"review\"], truncation=True)\n",
    "\n",
    "# %time tokenized_dataset = drug_dataset.map(tokenize_function, batched=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #fast tokenizer + batched=False + num_proc=8\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\", use_fast=True) #default is already True\n",
    "\n",
    "# def tokenize_function(examples):\n",
    "#     return tokenizer(examples[\"review\"], truncation=True)\n",
    "\n",
    "# %time tokenized_dataset = drug_dataset.map(tokenize_function, batched=True, num_proc=8)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A fast tokenizer with the `batched=True` option is 30 times faster than its slow counterpart with no batching ‚Äî this is truly amazing! Fast tokenizer is able to achieve such a speedup because behind the scenes the tokenization code is executed in Rust, which is a language that makes it easy to parallelize code execution.  Python is the language most often used for data science and deep learning applications, but when anything needs to be parallelized to be fast, it has to be written in another language. For instance, the matrix multiplications that are at the core of the model computation are written in CUDA, an optimized C library for GPUs.\n",
    "\n",
    "Parallelization is also the reason for the nearly 6x speedup the fast tokenizer achieves with batching: you can‚Äôt parallelize a single tokenization operation, but when you want to tokenize lots of texts at the same time you can just split the execution across several processes, each responsible for its own texts.\n",
    "\n",
    "`Dataset.map()` also has some parallelization capabilities of its own. Since they are not backed by Rust, they won‚Äôt let a slow tokenizer catch up with a fast one, but they can still be helpful (especially if you‚Äôre using a tokenizer that doesn‚Äôt have a fast version). To enable multiprocessing, use the `num_proc` argument and specify the number of processes to use in your call to Dataset.map()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximum token length \n",
    "\n",
    "Most of the time, pretrained models support certain Here we will tokenize our examples and truncate them to a maximum length of 128, but we will ask the tokenizer to return all the chunks of the texts instead of just the first one. This can be done with `return_overflowing_tokens=True`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "DnL9GFBNtyxO"
   },
   "outputs": [],
   "source": [
    "def tokenize_and_split(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"review\"],\n",
    "        truncation=True,\n",
    "        max_length=128,\n",
    "        return_overflowing_tokens=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'patient_id': 95260,\n",
       " 'drugName': 'Guanfacine',\n",
       " 'condition': 'adhd',\n",
       " 'review': '\"My son is halfway through his fourth week of Intuniv. We became concerned when he began this last week, when he started taking the highest dose he will be on. For two days, he could hardly get out of bed, was very cranky, and slept for nearly 8 hours on a drive home from school vacation (very unusual for him.) I called his doctor on Monday morning and she said to stick it out a few days. See how he did at school, and with getting up in the morning. The last two days have been problem free. He is MUCH more agreeable than ever. He is less emotional (a good thing), less cranky. He is remembering all the things he should. Overall his behavior is better. \\r\\nWe have tried many different medications and so far this is the most effective.\"',\n",
       " 'rating': 8.0,\n",
       " 'date': 'April 27, 2010',\n",
       " 'usefulCount': 192,\n",
       " 'review_length': 141}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drug_dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let‚Äôs test this on one example before using `Dataset.map()` on the whole dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "IGJ2BwZTtyxO",
    "outputId": "16fb5b96-4557-448d-b60d-1c66c6573f3f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 107, 1422, 1488, 1110, 9079, 1194, 1117, 2223, 1989, 1104, 1130, 19972, 11083, 119, 1284, 1245, 4264, 1165, 1119, 1310, 1142, 1314, 1989, 117, 1165, 1119, 1408, 1781, 1103, 2439, 13753, 1119, 1209, 1129, 1113, 119, 1370, 1160, 1552, 117, 1119, 1180, 6374, 1243, 1149, 1104, 1908, 117, 1108, 1304, 172, 14687, 1183, 117, 1105, 7362, 1111, 2212, 129, 2005, 1113, 170, 2797, 1313, 1121, 1278, 12020, 113, 1304, 5283, 1111, 1140, 119, 114, 146, 1270, 1117, 3995, 1113, 6356, 2106, 1105, 1131, 1163, 1106, 6166, 1122, 1149, 170, 1374, 1552, 119, 3969, 1293, 1119, 1225, 1120, 1278, 117, 1105, 1114, 2033, 1146, 1107, 1103, 2106, 119, 1109, 1314, 1160, 1552, 1138, 1151, 2463, 1714, 119, 1124, 1110, 150, 21986, 3048, 1167, 5340, 1895, 1190, 1518, 102], [101, 119, 1124, 1110, 1750, 6438, 113, 170, 1363, 1645, 114, 117, 1750, 172, 14687, 1183, 119, 1124, 1110, 11566, 1155, 1103, 1614, 1119, 1431, 119, 8007, 1117, 4658, 1110, 1618, 119, 1284, 1138, 1793, 1242, 1472, 23897, 1105, 1177, 1677, 1142, 1110, 1103, 1211, 3903, 119, 107, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'overflow_to_sample_mapping': [0, 0]}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = tokenize_and_split(drug_dataset[\"train\"][0])\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we got two `input_ids` list.  Let's try to decode what it is..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] \" My son is halfway through his fourth week of Intuniv. We became concerned when he began this last week, when he started taking the highest dose he will be on. For two days, he could hardly get out of bed, was very cranky, and slept for nearly 8 hours on a drive home from school vacation ( very unusual for him. ) I called his doctor on Monday morning and she said to stick it out a few days. See how he did at school, and with getting up in the morning. The last two days have been problem free. He is MUCH more agreeable than ever [SEP]'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(result['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS]. He is less emotional ( a good thing ), less cranky. He is remembering all the things he should. Overall his behavior is better. We have tried many different medications and so far this is the most effective. \" [SEP]'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(result['input_ids'][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let‚Äôs do this for all elements of the dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "iSQ5bBdItyxO",
    "outputId": "c2f103b6-ae21-4702-a270-df8a22e13a45"
   },
   "outputs": [],
   "source": [
    "#this will give error\n",
    "# tokenized_dataset = drug_dataset.map(tokenize_and_split, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oh no! That didn‚Äôt work! Why not? Looking at the error message will give us a clue: there is a mismatch in the lengths of one of the columns, one being of length 1,463 and the other of length 1,000. If you‚Äôve looked at the `Dataset.map()` documentation, 1000 is the default batch size of `Dataset.map()`, while 1463 comes from chunking.\n",
    "\n",
    "The problem is that we‚Äôre trying to mix two different datasets of different sizes: the drug_dataset columns will have a certain number of examples (the 1,000 in our error), but the tokenized_dataset we are building will have more (the 1,463 in the error message). That doesn‚Äôt work for a `Dataset`, so we need to either remove the columns from the old dataset or make them the same size as they are in the new dataset. We can do the former with the `remove_columns` argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['patient_id',\n",
       " 'drugName',\n",
       " 'condition',\n",
       " 'review',\n",
       " 'rating',\n",
       " 'date',\n",
       " 'usefulCount',\n",
       " 'review_length']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drug_dataset[\"train\"].column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "wqL-014vtyxP"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/chaklams/.cache/huggingface/datasets/csv/default-781d37ea3f205c6f/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-46a3cdc1f946b127.arrow\n",
      "Loading cached processed dataset at /home/chaklams/.cache/huggingface/datasets/csv/default-781d37ea3f205c6f/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-be6754264493cf45.arrow\n"
     ]
    }
   ],
   "source": [
    "tokenized_dataset = drug_dataset.map(\n",
    "    tokenize_and_split, batched=True, remove_columns=drug_dataset[\"train\"].column_names\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now this works without error. We can check that our new dataset has many more elements than the original dataset by comparing the lengths:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'overflow_to_sample_mapping'],\n",
       "        num_rows: 206772\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'overflow_to_sample_mapping'],\n",
       "        num_rows: 68876\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#so now our tokenized dataset has no old columns, but only the tokenized info...\n",
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "lNtcP96FtyxP",
    "outputId": "0150d29a-5ff1-436d-ff53-c4e62959ddc1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(206772, 138514)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check that length got changed....\n",
    "len(tokenized_dataset[\"train\"]), len(drug_dataset[\"train\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We mentioned that we can also deal with the mismatched length problem by making the old columns the same size as the new ones. To do this, we will need the `overflow_to_sample_mapping` field the tokenizer returns when we set `return_overflowing_tokens=True`. It gives us a mapping from a new feature index to the index of the sample it originated from. Using this, we can associate each key present in our original dataset with a list of values of the right size by repeating the values of each example as many times as it generates new features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "CwKKyqNftyxP"
   },
   "outputs": [],
   "source": [
    "def tokenize_and_split(examples):\n",
    "    result = tokenizer(\n",
    "        examples[\"review\"],\n",
    "        truncation=True,\n",
    "        max_length=128,\n",
    "        return_overflowing_tokens=True,\n",
    "    )\n",
    "    # Extract mapping between new and old indices\n",
    "    sample_map = result.pop(\"overflow_to_sample_mapping\")\n",
    "    for key, values in examples.items():\n",
    "        result[key] = [values[i] for i in sample_map]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see it works with Dataset.map() without us needing to remove the old columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "QZarJmOVtyxQ",
    "outputId": "2c7fbf70-e227-493e-d4a1-06e7392f2673"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/chaklams/.cache/huggingface/datasets/csv/default-781d37ea3f205c6f/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-9bb0cca85ec5a0dd.arrow\n",
      "Loading cached processed dataset at /home/chaklams/.cache/huggingface/datasets/csv/default-781d37ea3f205c6f/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-553abfd9825bbc81.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 206772\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 68876\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset = drug_dataset.map(tokenize_and_split, batched=True)\n",
    "tokenized_dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get the same number of training features as before, but here we‚Äôve kept all the old fields. If you need them for some post-processing after applying your model, you might want to use this approach.\n",
    "\n",
    "## 4. Using Pandas\n",
    "\n",
    "You‚Äôve now seen how ü§ó Datasets can be used to preprocess a dataset in various ways. Although the processing functions of ü§ó Datasets will cover most of your model training needs, there may be times when you‚Äôll need to switch to Pandas to access more powerful features, like `DataFrame.groupby()` or high-level APIs for visualization. Fortunately, ü§ó Datasets is designed to be interoperable with libraries such as Pandas, NumPy, PyTorch, TensorFlow, and JAX. Let‚Äôs take a look at how this works.\n",
    "\n",
    "To enable the conversion between various third-party libraries, ü§ó Datasets provides a `Dataset.set_format()` function. \n",
    "\n",
    "Under the hood, `Dataset.set_format()` changes the return format for the dataset‚Äôs __getitem__() dunder method. This means that when we want to create a new object like `train_df` from a Dataset in the \"pandas\" format, we need to slice the whole dataset to obtain a `pandas.DataFrame`. You can verify for yourself that the type of drug_dataset[\"train\"] is Dataset, irrespective of the output format.\n",
    "\n",
    "This function only changes the output format of the dataset, so you can easily switch to another format without affecting the underlying data format, which is Apache Arrow. The formatting is done in place. To demonstrate, let‚Äôs convert our dataset to Pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "nd4K0rQVtyxQ"
   },
   "outputs": [],
   "source": [
    "drug_dataset.set_format(\"pandas\") #inplace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datasets.dataset_dict.DatasetDict"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check that drug_dataset is still Dataset\n",
    "type(drug_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now when we access elements of the dataset we get a `pandas.DataFrame` instead of a dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "1Wy5Kg5_tyxQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patient_id</th>\n",
       "      <th>drugName</th>\n",
       "      <th>condition</th>\n",
       "      <th>review</th>\n",
       "      <th>rating</th>\n",
       "      <th>date</th>\n",
       "      <th>usefulCount</th>\n",
       "      <th>review_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>95260</td>\n",
       "      <td>Guanfacine</td>\n",
       "      <td>adhd</td>\n",
       "      <td>\"My son is halfway through his fourth week of ...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>April 27, 2010</td>\n",
       "      <td>192</td>\n",
       "      <td>141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>92703</td>\n",
       "      <td>Lybrel</td>\n",
       "      <td>birth control</td>\n",
       "      <td>\"I used to take another oral contraceptive, wh...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>December 14, 2009</td>\n",
       "      <td>17</td>\n",
       "      <td>134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>138000</td>\n",
       "      <td>Ortho Evra</td>\n",
       "      <td>birth control</td>\n",
       "      <td>\"This is my first time using any form of birth...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>November 3, 2015</td>\n",
       "      <td>10</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>35696</td>\n",
       "      <td>Buprenorphine / naloxone</td>\n",
       "      <td>opiate dependence</td>\n",
       "      <td>\"Suboxone has completely turned my life around...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>November 27, 2016</td>\n",
       "      <td>37</td>\n",
       "      <td>124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>155963</td>\n",
       "      <td>Cialis</td>\n",
       "      <td>benign prostatic hyperplasia</td>\n",
       "      <td>\"2nd day on 5mg started to work with rock hard...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>November 28, 2015</td>\n",
       "      <td>43</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>102654</td>\n",
       "      <td>Aripiprazole</td>\n",
       "      <td>bipolar disorde</td>\n",
       "      <td>\"Abilify changed my life. There is hope. I was...</td>\n",
       "      <td>10.0</td>\n",
       "      <td>March 14, 2015</td>\n",
       "      <td>32</td>\n",
       "      <td>146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>74811</td>\n",
       "      <td>Keppra</td>\n",
       "      <td>epilepsy</td>\n",
       "      <td>\" I Ve had  nothing but problems with the Kepp...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>August 9, 2016</td>\n",
       "      <td>11</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>48928</td>\n",
       "      <td>Ethinyl estradiol / levonorgestrel</td>\n",
       "      <td>birth control</td>\n",
       "      <td>\"I had been on the pill for many years. When m...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>December 8, 2016</td>\n",
       "      <td>1</td>\n",
       "      <td>142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>29607</td>\n",
       "      <td>Topiramate</td>\n",
       "      <td>migraine prevention</td>\n",
       "      <td>\"I have been on this medication almost two wee...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>January 1, 2015</td>\n",
       "      <td>19</td>\n",
       "      <td>148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>75612</td>\n",
       "      <td>L-methylfolate</td>\n",
       "      <td>depression</td>\n",
       "      <td>\"I have taken anti-depressants for years, with...</td>\n",
       "      <td>10.0</td>\n",
       "      <td>March 9, 2017</td>\n",
       "      <td>54</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   patient_id                            drugName  \\\n",
       "0       95260                          Guanfacine   \n",
       "1       92703                              Lybrel   \n",
       "2      138000                          Ortho Evra   \n",
       "3       35696            Buprenorphine / naloxone   \n",
       "4      155963                              Cialis   \n",
       "5      102654                        Aripiprazole   \n",
       "6       74811                              Keppra   \n",
       "7       48928  Ethinyl estradiol / levonorgestrel   \n",
       "8       29607                          Topiramate   \n",
       "9       75612                      L-methylfolate   \n",
       "\n",
       "                      condition  \\\n",
       "0                          adhd   \n",
       "1                 birth control   \n",
       "2                 birth control   \n",
       "3             opiate dependence   \n",
       "4  benign prostatic hyperplasia   \n",
       "5               bipolar disorde   \n",
       "6                      epilepsy   \n",
       "7                 birth control   \n",
       "8           migraine prevention   \n",
       "9                    depression   \n",
       "\n",
       "                                              review  rating  \\\n",
       "0  \"My son is halfway through his fourth week of ...     8.0   \n",
       "1  \"I used to take another oral contraceptive, wh...     5.0   \n",
       "2  \"This is my first time using any form of birth...     8.0   \n",
       "3  \"Suboxone has completely turned my life around...     9.0   \n",
       "4  \"2nd day on 5mg started to work with rock hard...     2.0   \n",
       "5  \"Abilify changed my life. There is hope. I was...    10.0   \n",
       "6  \" I Ve had  nothing but problems with the Kepp...     1.0   \n",
       "7  \"I had been on the pill for many years. When m...     8.0   \n",
       "8  \"I have been on this medication almost two wee...     9.0   \n",
       "9  \"I have taken anti-depressants for years, with...    10.0   \n",
       "\n",
       "                date  usefulCount  review_length  \n",
       "0     April 27, 2010          192            141  \n",
       "1  December 14, 2009           17            134  \n",
       "2   November 3, 2015           10             89  \n",
       "3  November 27, 2016           37            124  \n",
       "4  November 28, 2015           43             68  \n",
       "5     March 14, 2015           32            146  \n",
       "6     August 9, 2016           11             35  \n",
       "7   December 8, 2016            1            142  \n",
       "8    January 1, 2015           19            148  \n",
       "9      March 9, 2017           54             80  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(type(drug_dataset[\"train\"][:3]))\n",
    "drug_dataset[\"train\"][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let‚Äôs create a pandas.DataFrame for the whole training set by selecting all the elements of `drug_dataset[\"train\"]`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "l_qMda2DtyxQ"
   },
   "outputs": [],
   "source": [
    "train_df = drug_dataset[\"train\"][:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here we can use all the Pandas functionality that we want. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patient_id</th>\n",
       "      <th>drugName</th>\n",
       "      <th>condition</th>\n",
       "      <th>review</th>\n",
       "      <th>rating</th>\n",
       "      <th>date</th>\n",
       "      <th>usefulCount</th>\n",
       "      <th>review_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>95260</td>\n",
       "      <td>Guanfacine</td>\n",
       "      <td>adhd</td>\n",
       "      <td>\"My son is halfway through his fourth week of ...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>April 27, 2010</td>\n",
       "      <td>192</td>\n",
       "      <td>141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>92703</td>\n",
       "      <td>Lybrel</td>\n",
       "      <td>birth control</td>\n",
       "      <td>\"I used to take another oral contraceptive, wh...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>December 14, 2009</td>\n",
       "      <td>17</td>\n",
       "      <td>134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>138000</td>\n",
       "      <td>Ortho Evra</td>\n",
       "      <td>birth control</td>\n",
       "      <td>\"This is my first time using any form of birth...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>November 3, 2015</td>\n",
       "      <td>10</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>35696</td>\n",
       "      <td>Buprenorphine / naloxone</td>\n",
       "      <td>opiate dependence</td>\n",
       "      <td>\"Suboxone has completely turned my life around...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>November 27, 2016</td>\n",
       "      <td>37</td>\n",
       "      <td>124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>155963</td>\n",
       "      <td>Cialis</td>\n",
       "      <td>benign prostatic hyperplasia</td>\n",
       "      <td>\"2nd day on 5mg started to work with rock hard...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>November 28, 2015</td>\n",
       "      <td>43</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   patient_id                  drugName                     condition  \\\n",
       "0       95260                Guanfacine                          adhd   \n",
       "1       92703                    Lybrel                 birth control   \n",
       "2      138000                Ortho Evra                 birth control   \n",
       "3       35696  Buprenorphine / naloxone             opiate dependence   \n",
       "4      155963                    Cialis  benign prostatic hyperplasia   \n",
       "\n",
       "                                              review  rating  \\\n",
       "0  \"My son is halfway through his fourth week of ...     8.0   \n",
       "1  \"I used to take another oral contraceptive, wh...     5.0   \n",
       "2  \"This is my first time using any form of birth...     8.0   \n",
       "3  \"Suboxone has completely turned my life around...     9.0   \n",
       "4  \"2nd day on 5mg started to work with rock hard...     2.0   \n",
       "\n",
       "                date  usefulCount  review_length  \n",
       "0     April 27, 2010          192            141  \n",
       "1  December 14, 2009           17            134  \n",
       "2   November 3, 2015           10             89  \n",
       "3  November 27, 2016           37            124  \n",
       "4  November 28, 2015           43             68  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And once we‚Äôre done with our Pandas analysis, we can always create a new Dataset object by using the `Dataset.from_pandas()` function as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "k8wbcylYtyxR",
    "outputId": "ef75a577-caab-42de-85ea-0df80ea4dc60"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length'],\n",
       "    num_rows: 138514\n",
       "})"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Creating a validation set\n",
    "\n",
    "To round out the section, let‚Äôs create a validation set to prepare the dataset for training a classifier on. Before doing so, we‚Äôll reset the output format of drug_dataset from \"pandas\" to \"arrow\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "cmZfZgh-tyxS"
   },
   "outputs": [],
   "source": [
    "drug_dataset.reset_format()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although we have a test set we could use for evaluation, it‚Äôs a good practice to leave the test set untouched and create a separate validation set during development. Once you are happy with the performance of your models on the validation set, you can do a final sanity check on the test set. This process helps mitigate the risk that you‚Äôll overfit to the test set and deploy a model that fails on real-world data.\n",
    "\n",
    "ü§ó Datasets provides a `Dataset.train_test_split()` function that is based on the famous functionality from scikit-learn. Let‚Äôs use it to split our training set into train and validation splits (we set the seed argument for reproducibility):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length'],\n",
       "        num_rows: 138514\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length'],\n",
       "        num_rows: 46108\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drug_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached split indices for dataset at /home/chaklams/.cache/huggingface/datasets/csv/default-781d37ea3f205c6f/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-a2aff6acd88adfb9.arrow and /home/chaklams/.cache/huggingface/datasets/csv/default-781d37ea3f205c6f/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-9bf84c86a3e05a56.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length'],\n",
       "        num_rows: 110811\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length'],\n",
       "        num_rows: 27703\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drug_dataset_clean = drug_dataset[\"train\"].train_test_split(train_size=0.8, seed=42)\n",
    "drug_dataset_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length'],\n",
       "        num_rows: 110811\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length'],\n",
       "        num_rows: 27703\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rename the default \"test\" split to \"validation\"\n",
    "drug_dataset_clean[\"validation\"] = drug_dataset_clean.pop(\"test\")\n",
    "drug_dataset_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "wriQ7MK-tyxS",
    "outputId": "09f12931-1e22-42a2-cb1f-853e57e75084"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length'],\n",
       "        num_rows: 110811\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length'],\n",
       "        num_rows: 27703\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length'],\n",
       "        num_rows: 46108\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add the \"test\" set to our `DatasetDict`\n",
    "drug_dataset_clean[\"test\"] = drug_dataset[\"test\"]\n",
    "drug_dataset_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving a dataset\n",
    "\n",
    "Finally, after preprocessing, we may want to save the `Dataset`.  Huggingface provides `Dataset.save_to_disk()` (Apache Arrow), `Dataset.to_csv` (csv) and `Dataset.to_json()` (JSON)\n",
    "\n",
    "For example, let‚Äôs save our cleaned dataset in the Arrow format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "4uAK-EintyxT"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/chaklams/.cache/huggingface/datasets/csv/default-781d37ea3f205c6f/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-b8edfaec310f7fab.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/110811 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/chaklams/.cache/huggingface/datasets/csv/default-781d37ea3f205c6f/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-4792be80ca98a56e.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/27703 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/46108 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "drug_dataset_clean.save_to_disk(\"drug-reviews\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice you got some new dir.  We can load it by using the `load_from_disk()` function as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "yMMLx0DJtyxT",
    "outputId": "aca2db3a-db22-4a0d-9185-7ea218a1f2a0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chaklams/.local/lib/python3.8/site-packages/datasets/dataset_dict.py:1241: FutureWarning: 'fs' was is deprecated in favor of 'storage_options' in version 2.8.0 and will be removed in 3.0.0.\n",
      "You can remove this warning by passing 'storage_options=fs.storage_options' instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length'],\n",
       "        num_rows: 110811\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length'],\n",
       "        num_rows: 27703\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length'],\n",
       "        num_rows: 46108\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "drug_dataset_reloaded = load_from_disk(\"drug-reviews\")\n",
    "drug_dataset_reloaded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the CSV and JSON formats, we have to store each split as a separate file. One way to do this is by iterating over the keys and values in the DatasetDict object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "AFOdACTctyxT"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "518e9bf6327340328c77ab5f56c7e94d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/111 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2fee7773229457ea0924dc78c5eb616",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/28 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49c279624cc0411cb63b1c0dfc68fa08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/47 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for split, dataset in drug_dataset_clean.items():\n",
    "    dataset.to_json(f\"drug-reviews-{split}.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This saves each split in JSON Lines format, where each row in the dataset is stored as a single line of JSON. Here‚Äôs what the first example looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "5jZ1Lyx7tyxU",
    "outputId": "e99a8c87-75ac-4abc-f9b4-7190929fc123"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "{\"patient_id\":89879,\"drugName\":\"Cyclosporine\",\"condition\":\"keratoconjunctivitis sicca\",\"review\":\"\\\"I have used Restasis for about a year now and have seen almost no progress.  For most of my life I've had red and bothersome eyes. After trying various eye drops, my doctor recommended Restasis.  He said it typically takes 3 to 6 months for it to really kick in but it never did kick in.  When I put the drops in it burns my eyes for the first 30 - 40 minutes.  I've talked with my doctor about this and he said it is normal but should go away after some time, but it hasn't. Every year around spring time my eyes get terrible irritated  and this year has been the same (maybe even worse than other years) even though I've been using Restasis for a year now. The only difference I notice was for the first couple weeks, but now I'm ready to move on.\\\"\",\"rating\":2.0,\"date\":\"April 20, 2013\",\"usefulCount\":69,\"review_length\":147}\n"
     ]
    }
   ],
   "source": [
    "!head -n 1 drug-reviews-train.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can load the json file using `load_dataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "vCghkOYgtyxU"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-bdae8a4a887d0ba9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/default to /home/chaklams/.cache/huggingface/datasets/json/default-bdae8a4a887d0ba9/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "beb841da18144eb2b6946f3768c5a08d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3347575719a84e05b9ccb3fb4cb2de53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0423557fe3834cd79880424124073a31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset json downloaded and prepared to /home/chaklams/.cache/huggingface/datasets/json/default-bdae8a4a887d0ba9/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57d39b330ee34bcaa9220253b30aa052",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_files = {\n",
    "    \"train\": \"drug-reviews-train.jsonl\",\n",
    "    \"validation\": \"drug-reviews-validation.jsonl\",\n",
    "    \"test\": \"drug-reviews-test.jsonl\",\n",
    "}\n",
    "drug_dataset_reloaded = load_dataset(\"json\", data_files=data_files)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "What if my dataset isn't on the Hub?",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9 (default, Oct 26 2021, 07:25:54) \n[Clang 13.0.0 (clang-1300.0.29.30)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "0f2c79af21be9d001248940c049b6176cf8bfb45cabf7aa85848f5cea0f590f6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
