{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec (Skipgram )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('1.24.0', '1.12.1')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.__version__, torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.6.2'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib\n",
    "matplotlib.__version__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#specify the sentences / corpus\n",
    "#corpus is defined as a set of documents\n",
    "#document is basically a bunch of sentence(s)\n",
    "corpus = [\"apple banana fruit\", \"banana apple fruit\", \"banana fruit apple\", \n",
    "          \"dog cat animal\", \"cat dog animal\", \"cat animal dog\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['apple', 'banana', 'fruit'],\n",
       " ['banana', 'apple', 'fruit'],\n",
       " ['banana', 'fruit', 'apple'],\n",
       " ['dog', 'cat', 'animal'],\n",
       " ['cat', 'dog', 'animal'],\n",
       " ['cat', 'animal', 'dog']]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1. tokenize\n",
    "#usually you use spaCy / NLTK to tokenize (but we gonna do this later on, we gonna have spaCy)\n",
    "corpus_tokenized = [sent.split(\" \") for sent in corpus]\n",
    "corpus_tokenized  #we called each of this as \"tokens\", NOT words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. numericalize\n",
    "\n",
    "#2.1 get all the unique words\n",
    "#we want to flatten this (basically merge all list)\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "vocabs  = list(set(flatten(corpus_tokenized)))  #vocabs is a term defining all unique words your system know"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.2 assign id to all these vocabs\n",
    "word2index = {v: idx for idx, v in enumerate(vocabs)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2index['dog']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add <UNK>, which is a very normal token exists in the world\n",
    "vocabs.append('<UNK>') #chaky, can it be ##UNK, or UNKKKKKK, or anything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we have a way to know what is the id of <UNK>\n",
    "word2index['<UNK>'] = 6  #usually <UNK> is 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'banana',\n",
       " 1: 'animal',\n",
       " 2: 'cat',\n",
       " 3: 'fruit',\n",
       " 4: 'apple',\n",
       " 5: 'dog',\n",
       " 6: '<UNK>'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create index2word dictionary\n",
    "#2 min    \n",
    "index2word = {v:k for k, v in word2index.items()}\n",
    "\n",
    "index2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['banana', 'animal', 'cat', 'fruit', 'apple', 'dog', '<UNK>']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepare train data\n",
    "You move the window along, and create those tuples as we said in class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['banana', 'apple'],\n",
       " ['banana', 'fruit'],\n",
       " ['apple', 'banana'],\n",
       " ['apple', 'fruit'],\n",
       " ['fruit', 'banana'],\n",
       " ['fruit', 'apple'],\n",
       " ['cat', 'dog'],\n",
       " ['cat', 'animal'],\n",
       " ['dog', 'cat'],\n",
       " ['dog', 'animal'],\n",
       " ['animal', 'cat'],\n",
       " ['animal', 'dog']]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#move along the corpus\n",
    "#to fit with our corpus, we gonna use window_size = 1\n",
    "\n",
    "skipgrams = []\n",
    "\n",
    "#for each corpus\n",
    "for sent in corpus_tokenized:\n",
    "    #for each sent [\"apple\", \"banana\", \"fruit\"]\n",
    "    for i in range(1, len(sent) - 1): #start from 1 to second last\n",
    "        center_word = sent[i]\n",
    "        outside_words = [sent[i-1], sent[i+1]]  #window_size = 1\n",
    "        for o in outside_words:\n",
    "            skipgrams.append([center_word, o])\n",
    "\n",
    "skipgrams\n",
    "        \n",
    "#here we want to create (banana, apple), (banana, fruit) append to some list\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's make what we have made into a function (batch function)\n",
    "#return a batches of data, e.g., =2 --> ['banana', 'apple'], ['banana', 'fruit']\n",
    "#also i want these batches to be id, NOT token   --> [5, 4]\n",
    "\n",
    "def random_batch(batch_size, corpus):\n",
    "    \n",
    "    skipgrams = []\n",
    "\n",
    "    #for each corpus\n",
    "    for sent in corpus_tokenized:\n",
    "        #for each sent [\"apple\", \"banana\", \"fruit\"]\n",
    "        for i in range(1, len(sent) - 1): #start from 1 to second last\n",
    "            center_word = word2index[sent[i]]\n",
    "            outside_words = [word2index[sent[i-1]], word2index[sent[i+1]]]  #window_size = 1\n",
    "            for o in outside_words:\n",
    "                skipgrams.append([center_word, o])\n",
    "                \n",
    "    #only get a batch, not the entire list\n",
    "    random_index = np.random.choice(range(len(skipgrams)), batch_size, replace=False)\n",
    "             \n",
    "    #appending some list of inputs and labels\n",
    "    random_inputs, random_labels = [], []   \n",
    "    for index in random_index:\n",
    "        random_inputs.append([skipgrams[index][0]])  #center words, this will be a shape of (1, ) --> (1, 1) for modeling\n",
    "        random_labels.append([skipgrams[index][1]])\n",
    "        \n",
    "    return np.array(random_inputs), np.array(random_labels)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 1)\n",
      "label=array([[5],\n",
      "       [1],\n",
      "       [5],\n",
      "       [3],\n",
      "       [4],\n",
      "       [3],\n",
      "       [2],\n",
      "       [0],\n",
      "       [0],\n",
      "       [1]])\n"
     ]
    }
   ],
   "source": [
    "input, label = random_batch(10, corpus_tokenized)\n",
    "\n",
    "print(f\"{input.shape}\")\n",
    "print(f\"{label=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model\n",
    "\n",
    "$$J(\\theta) = -\\frac{1}{T}\\sum_{t=1}^{T}\\sum_{\\substack{-m \\leq j \\leq m \\\\ j \\neq 0}}\\log P(w_{t+j} | w_t; \\theta)$$\n",
    "\n",
    "where $P(w_{t+j} | w_t; \\theta) = $\n",
    "\n",
    "$$P(o|c)=\\frac{\\exp(\\mathbf{u_o^{\\top}v_c})}{\\sum_{w=1}^V\\exp(\\mathbf{u_w^{\\top}v_c})}$$\n",
    "\n",
    "where $o$ is the outside words and $c$ is the center word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voc_size = len(vocabs)\n",
    "voc_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['banana', 'animal', 'cat', 'fruit', 'apple', 'dog', '<UNK>']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P(o|c)=\\frac{\\exp(\\mathbf{u_o^{\\top}v_c})}{\\sum_{w=1}^V\\exp(\\mathbf{u_w^{\\top}v_c})}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the model will accept three vectors - u_o, v_c, u_w\n",
    "#u_o - vector for outside words\n",
    "#v_c - vector for center word\n",
    "#u_w - vectors of all vocabs\n",
    "\n",
    "class Skipgram(nn.Module):\n",
    "    \n",
    "    def __init__(self, voc_size, emb_size):\n",
    "        super(Skipgram, self).__init__()\n",
    "        self.embedding_center_word  = nn.Embedding(voc_size, emb_size)  #is a lookup table mapping all ids in voc_size, into some vector of size emb_size\n",
    "        self.embedding_outside_word = nn.Embedding(voc_size, emb_size)\n",
    "    \n",
    "    def forward(self, center_word, outside_word, all_vocabs):\n",
    "        #center_word, outside_word: (batch_size, 1)\n",
    "        #all_vocabs: (batch_size, voc_size)\n",
    "        \n",
    "        #convert them into embedding\n",
    "        center_word_embed  = self.embedding_center_word(center_word)     #(batch_size, 1, emb_size)\n",
    "        outside_word_embed = self.embedding_outside_word(outside_word)   #(batch_size, 1, emb_size)\n",
    "        all_vocabs_embed   = self.embedding_outside_word(all_vocabs)     #(batch_size, voc_size, emb_size)\n",
    "        \n",
    "        #bmm is basically @ or .dot , but across batches (i.e., ignore the batch dimension)\n",
    "        top_term = outside_word_embed.bmm(center_word_embed.transpose(1, 2)).squeeze(2)\n",
    "        #(batch_size, 1, emb_size) @ (batch_size, emb_size, 1) = (batch_size, 1, 1) ===> (batch_size, 1)\n",
    "        \n",
    "        top_term_exp = torch.exp(top_term)  #exp(uo vc)\n",
    "        #(batch_size, 1)\n",
    "        \n",
    "        lower_term = all_vocabs_embed.bmm(center_word_embed.transpose(1, 2)).squeeze(2)\n",
    "         #(batch_size, voc_size, emb_size) @ (batch_size, emb_size, 1) = (batch_size, voc_size, 1) = (batch_size, voc_size)\n",
    "         \n",
    "        lower_term_sum = torch.sum(torch.exp(lower_term), 1) #sum exp(uw vc)\n",
    "        #(batch_size, 1)\n",
    "        \n",
    "        loss_fn = -torch.mean(torch.log(top_term_exp / lower_term_sum))\n",
    "        #(batch_size, 1) / (batch_size, 1) ==mean==> scalar\n",
    "        \n",
    "        return loss_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 7])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#preparing all_vocabs\n",
    "\n",
    "batch_size = 2\n",
    "\n",
    "def prepare_sequence(seq, word2index):\n",
    "    #map(function, list of something)\n",
    "    #map will look at each of element in this list, and apply this function\n",
    "    idxs = list(map(lambda w: word2index[w] if word2index.get(w) is not None else word2index[\"<UNK>\"], seq))\n",
    "    return torch.LongTensor(idxs)\n",
    "\n",
    "all_vocabs = prepare_sequence(list(vocabs), word2index).expand(batch_size, voc_size)\n",
    "all_vocabs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voc_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4],\n",
       "       [2]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input, label = random_batch(batch_size, corpus_tokenized)\n",
    "input #center word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3],\n",
       "       [1]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label #context word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voc_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_size = 2 #usually, this can be 50, 100, or 300\n",
    "model = Skipgram(voc_size, emb_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor = torch.LongTensor(input)  \n",
    "label_tensor = torch.LongTensor(label)  #LongTensor basically means integer...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2318274225857707364, 8031151179464336711])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#this is different\n",
    "torch.LongTensor(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.LongTensor([2])  #put shape (, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2, 3, 4, 5, 6],\n",
       "        [0, 1, 2, 3, 4, 5, 6]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_vocabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this should give one number\n",
    "loss = model(input_tensor, label_tensor, all_vocabs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.1194, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2 #why?  no reason; \n",
    "emb_size   = 2 #why?  no reason; usually 50, 100, 300, but 2 so we can plot (50 can also plot, but need PCA)\n",
    "model      = Skipgram(voc_size, emb_size)\n",
    "\n",
    "optimizer  = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1000 | Loss: 1.341595 | Time: ??\n",
      "Epoch 2000 | Loss: 1.323924 | Time: ??\n",
      "Epoch 3000 | Loss: 1.021360 | Time: ??\n",
      "Epoch 4000 | Loss: 1.224537 | Time: ??\n",
      "Epoch 5000 | Loss: 0.902450 | Time: ??\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5000\n",
    "#for epoch\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    #get random batch\n",
    "    input_batch, label_batch = random_batch(batch_size, corpus)\n",
    "    input_batch = torch.LongTensor(input_batch)\n",
    "    label_batch = torch.LongTensor(label_batch)\n",
    "    \n",
    "    # print(input_batch.shape, label_batch.shape, all_vocabs.shape)\n",
    "    \n",
    "    #loss = model\n",
    "    loss = model(input_batch, label_batch, all_vocabs)\n",
    "    \n",
    "    #backpropagate\n",
    "    loss.backward()\n",
    "    \n",
    "    #update alpha\n",
    "    optimizer.step()\n",
    "    \n",
    "    #print epoch loss\n",
    "    if (epoch + 1) % 1000 == 0:\n",
    "        print(f\"Epoch {epoch+1} | Loss: {loss:.6f} | Time: ??\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Plot the embeddings\n",
    "\n",
    "Is really the related stuff are close to each other, and vice versa?\n",
    "\n",
    "The most fun part:  Will \"banana\" closer to \"fruit\" than \"cat\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['banana', 'animal', 'cat', 'fruit', 'apple', 'dog', '<UNK>']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "banana = torch.LongTensor([word2index['banana']])\n",
    "banana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.4631, 3.4770]], grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "banana_center_embed = model.embedding_center_word(banana)\n",
    "banana_outisde_embed = model.embedding_outside_word(banana)\n",
    "\n",
    "banana_embed = (banana_center_embed + banana_outisde_embed) / 2\n",
    "banana_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embed(word):\n",
    "    try:\n",
    "        index = word2index[word]\n",
    "    except:\n",
    "        index = word2index['<UNK>']\n",
    "    \n",
    "    word = torch.LongTensor([index])\n",
    "\n",
    "    center_embed  = model.embedding_center_word(word)\n",
    "    outside_embed = model.embedding_outside_word(word)\n",
    "    \n",
    "    embed = (center_embed + outside_embed) / 2\n",
    "    \n",
    "    return  embed[0][0].item(), embed[0][1].item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3.050816535949707, 4.016746520996094)\n",
      "(-1.4275349378585815, -4.583487510681152)\n",
      "(0.7686139345169067, -0.1948009431362152)\n"
     ]
    }
   ],
   "source": [
    "#find embedding of fruit, cat\n",
    "print(get_embed('fruit'))\n",
    "print(get_embed('cat'))\n",
    "\n",
    "print(get_embed('chaky'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#help me plot fruit cat banana on matplotlib\n",
    "plt.figure(figsize=(6,3))\n",
    "for i, word in enumerate(vocabs[:20]): #loop each unique vocab\n",
    "    x, y = get_embed(word)\n",
    "    plt.scatter(x, y)\n",
    "    plt.annotate(word, xy=(x, y), xytext=(5, 2), textcoords='offset points')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Cosine similarity\n",
    "\n",
    "How do (from scratch) calculate cosine similarity?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9 (default, Oct 26 2021, 07:25:54) \n[Clang 13.0.0 (clang-1300.0.29.30)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0f2c79af21be9d001248940c049b6176cf8bfb45cabf7aa85848f5cea0f590f6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
