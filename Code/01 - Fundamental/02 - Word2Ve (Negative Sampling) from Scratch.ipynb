{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Word2Vec (Negative Sampling)\n",
        "\n",
        "Let's work on negative-sampling based implementation of word2vec."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Define some very simple data for understanding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "corpus = [\"apple banana fruit\", \"banana apple fruit\", \"banana fruit apple\",\n",
        "                 \"dog cat animal\", \"cat animal dog\", \"cat dog animal\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[['apple', 'banana', 'fruit'],\n",
              " ['banana', 'apple', 'fruit'],\n",
              " ['banana', 'fruit', 'apple'],\n",
              " ['dog', 'cat', 'animal'],\n",
              " ['cat', 'animal', 'dog'],\n",
              " ['cat', 'dog', 'animal']]"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "corpus = [sent.split(\" \") for sent in corpus]\n",
        "corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['cat', 'apple', 'animal', 'dog', 'banana', 'fruit']"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#get word sequences and unique words\n",
        "flatten = lambda l: [item for sublist in l for item in sublist]\n",
        "vocab = list(set(flatten(corpus)))\n",
        "vocab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'cat': 0, 'apple': 1, 'animal': 2, 'dog': 3, 'banana': 4, 'fruit': 5}\n"
          ]
        }
      ],
      "source": [
        "#numericalization\n",
        "word2index = {w: i for i, w in enumerate(vocab)}\n",
        "print(word2index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "6\n"
          ]
        }
      ],
      "source": [
        "#vocab size\n",
        "voc_size = len(vocab)\n",
        "print(voc_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "#append UNK\n",
        "vocab.append('<UNK>')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['cat', 'apple', 'animal', 'dog', 'banana', 'fruit', '<UNK>']"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vocab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "word2index['<UNK>'] = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "#just in case we need to use\n",
        "index2word = {v:k for k, v in word2index.items()} "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Prepare train data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['apple', 'banana', 'fruit']\n",
            "['banana', 'apple', 'fruit']\n",
            "['banana', 'fruit', 'apple']\n",
            "['dog', 'cat', 'animal']\n",
            "['cat', 'animal', 'dog']\n",
            "['cat', 'dog', 'animal']\n"
          ]
        }
      ],
      "source": [
        "for c in corpus:\n",
        "    print(c)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "def random_batch(batch_size, word_sequence):\n",
        "    \n",
        "    # Make skip gram of one size window\n",
        "    skip_grams = []\n",
        "    # loop each word sequence\n",
        "    # we starts from 1 because 0 has no context\n",
        "    # we stop at second last for the same reason\n",
        "    for sent in corpus:\n",
        "        for i in range(1, len(sent) - 1):\n",
        "            target = word2index[sent[i]]\n",
        "            context = [word2index[sent[i - 1]], word2index[sent[i + 1]]]\n",
        "            for w in context:\n",
        "                skip_grams.append([target, w])\n",
        "    \n",
        "    random_inputs = []\n",
        "    random_labels = []\n",
        "    random_index = np.random.choice(range(len(skip_grams)), batch_size, replace=False) #randomly pick without replacement\n",
        "        \n",
        "    for i in random_index:\n",
        "        random_inputs.append([skip_grams[i][0]])  # target, e.g., 2\n",
        "        random_labels.append([skip_grams[i][1]])  # context word, e.g., 3\n",
        "            \n",
        "    return np.array(random_inputs), np.array(random_labels)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Testing the method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input:  [[1]\n",
            " [2]]\n",
            "Target:  [[4]\n",
            " [3]]\n"
          ]
        }
      ],
      "source": [
        "#testing the method\n",
        "batch_size = 2 # mini-batch size\n",
        "input_batch, target_batch = random_batch(batch_size, corpus)\n",
        "\n",
        "print(\"Input: \",  input_batch)\n",
        "print(\"Target: \", target_batch)\n",
        "\n",
        "#we will convert them to tensor during training, so don't worry..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((2, 1), (2, 1))"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "input_batch.shape, target_batch.shape"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Negative Sampling"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Unigram distribution\n",
        "\n",
        "$$P(w)=U(w)^{3/4}/Z$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "Z = 0.001"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "word_count = Counter(flatten(corpus))\n",
        "num_total_words = sum([c for w, c in word_count.items()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "word_count[',']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "18"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "num_total_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "unigram_table = []\n",
        "\n",
        "for vo in vocab:\n",
        "    unigram_table.extend([vo] * int(((word_count[vo]/num_total_words)**0.75)/Z))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Counter({'cat': 260,\n",
              "         'apple': 260,\n",
              "         'animal': 260,\n",
              "         'dog': 260,\n",
              "         'banana': 260,\n",
              "         'fruit': 260})"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Counter(unigram_table)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Negative Sampling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "def prepare_sequence(seq, word2index):\n",
        "    idxs = list(map(lambda w: word2index[w] if word2index.get(w) is not None else word2index[\"<UNK>\"], seq))\n",
        "    return torch.LongTensor(idxs)\n",
        "\n",
        "def negative_sampling(targets, unigram_table, k):\n",
        "    batch_size = targets.size(0)\n",
        "    neg_samples = []\n",
        "    for i in range(batch_size):\n",
        "        nsample = []\n",
        "        target_index = targets[i].item()\n",
        "        while len(nsample) < k: # num of sampling\n",
        "            neg = random.choice(unigram_table)\n",
        "            if word2index[neg] == target_index:\n",
        "                continue\n",
        "            nsample.append(neg)\n",
        "        neg_samples.append(prepare_sequence(nsample, word2index).view(1, -1))\n",
        "    return torch.cat(neg_samples)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Testing the negative sampling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "input_batch  = torch.Tensor(input_batch)\n",
        "target_batch = torch.LongTensor(target_batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([2, 1])"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "target_batch.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[1.],\n",
              "        [2.]])"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "input_batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[1, 0, 0],\n",
              "        [1, 2, 0]])"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "num_neg = 3\n",
        "negative_sampling(target_batch, unigram_table, num_neg)\n",
        "\n",
        "#{'grapes': 0, 'apple': 1, 'animal': 2, 'cat': 3, 'ice': 4, 'orange': 5, 'dog': 6, 'monkey': 7, 'conda': 8, 'fruit': 9, 'banana': 10}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([3])"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "target_batch[1]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Model\n",
        "\n",
        "$$\\mathbf{J}_{\\text{neg-sample}}(\\mathbf{v}_c,o,\\mathbf{U})=-\\log(\\sigma(\\mathbf{u}_o^T\\mathbf{v}_c))-\\sum_{k=1}^K\\log(\\sigma(-\\mathbf{u}_k^T\\mathbf{v}_c))$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SkipgramNegSampling(nn.Module):\n",
        "    \n",
        "    def __init__(self, vocab_size, emb_size):\n",
        "        super(SkipgramNegSampling, self).__init__()\n",
        "        self.embedding_v = nn.Embedding(vocab_size, emb_size) # center embedding\n",
        "        self.embedding_u = nn.Embedding(vocab_size, emb_size) # out embedding\n",
        "        self.logsigmoid = nn.LogSigmoid()\n",
        "                    \n",
        "    def forward(self, center_words, target_words, negative_words):\n",
        "        center_embeds = self.embedding_v(center_words) # [batch_size, 1, emb_size]\n",
        "        target_embeds = self.embedding_u(target_words) # [batch_size, 1, emb_size]\n",
        "        neg_embeds    = -self.embedding_u(negative_words) # [batch_size, num_neg, emb_size]\n",
        "        \n",
        "        positive_score = target_embeds.bmm(center_embeds.transpose(1, 2)).squeeze(2)\n",
        "        #[batch_size, 1, emb_size] @ [batch_size, emb_size, 1] = [batch_size, 1, 1] = [batch_size, 1]\n",
        "        \n",
        "        negative_score = neg_embeds.bmm(center_embeds.transpose(1, 2))\n",
        "        #[batch_size, k, emb_size] @ [batch_size, emb_size, 1] = [batch_size, k, 1]\n",
        "        \n",
        "        loss = self.logsigmoid(positive_score) + torch.sum(self.logsigmoid(negative_score), 1)\n",
        "                \n",
        "        return -torch.mean(loss)\n",
        "    \n",
        "    def prediction(self, inputs):\n",
        "        embeds = self.embedding_v(inputs)\n",
        "        \n",
        "        return embeds"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "batch_size     = 2 # mini-batch size\n",
        "embedding_size = 2 #so we can later plot\n",
        "model          = SkipgramNegSampling(voc_size, embedding_size)\n",
        "num_neg        = 10 # num of negative sampling\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1000 | cost: 7.393001 | time: 0m 0s\n",
            "Epoch: 2000 | cost: 4.989662 | time: 0m 0s\n",
            "Epoch: 3000 | cost: 3.122581 | time: 0m 0s\n",
            "Epoch: 4000 | cost: 3.685686 | time: 0m 0s\n",
            "Epoch: 5000 | cost: 2.721482 | time: 0m 0s\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "# Training\n",
        "num_epochs = 5000\n",
        "for epoch in range(num_epochs):\n",
        "    \n",
        "    start = time.time()\n",
        "    \n",
        "    input_batch, target_batch = random_batch(batch_size, corpus)\n",
        "    \n",
        "    #input_batch: [batch_size, 1]\n",
        "    input_batch = torch.LongTensor(input_batch)\n",
        "    \n",
        "    #target_batch: [batch_size, 1]\n",
        "    target_batch = torch.LongTensor(target_batch)\n",
        "    \n",
        "    #negs_batch:   [batch_size, num_neg]\n",
        "    negs_batch = negative_sampling(target_batch, unigram_table, num_neg)\n",
        "    \n",
        "    optimizer.zero_grad()\n",
        "        \n",
        "    loss = model(input_batch, target_batch, negs_batch)\n",
        "    \n",
        "    end = time.time()\n",
        "    \n",
        "    epoch_mins, epoch_secs = epoch_time(start, end)\n",
        "    \n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if (epoch + 1) % 1000 == 0:\n",
        "        print(f\"Epoch: {epoch + 1} | cost: {loss:.6f} | time: {epoch_mins}m {epoch_secs}s\")\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Plotting the embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['cat', 'apple', 'animal', 'dog', 'banana', 'fruit', '<UNK>']"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#list of vocabs\n",
        "vocab[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "word = vocab[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#numericalization\n",
        "id = word2index[word]\n",
        "id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([0])"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "id_tensor = torch.LongTensor([id])\n",
        "id_tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor([[-1.0292,  0.4411]], grad_fn=<EmbeddingBackward0>),\n",
              " tensor([[ 1.3423, -2.1218]], grad_fn=<EmbeddingBackward0>))"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#get the embedding by averaging\n",
        "v_embed = model.embedding_v(id_tensor)\n",
        "u_embed = model.embedding_u(id_tensor)\n",
        "\n",
        "v_embed, u_embed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(-0.8404, grad_fn=<SelectBackward0>)"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#average to get the word embedding\n",
        "word_embed = (v_embed + u_embed) / 2\n",
        "word_embed[0][1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [],
      "source": [
        "#let's write a function to get embedding given a word\n",
        "def get_embed(word):\n",
        "    id_tensor = torch.LongTensor([word2index[word]])\n",
        "    v_embed = model.embedding_v(id_tensor)\n",
        "    u_embed = model.embedding_u(id_tensor) \n",
        "    word_embed = (v_embed + u_embed) / 2 \n",
        "    x, y = word_embed[0][0].item(), word_embed[0][1].item()\n",
        "\n",
        "    return x, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 600x300 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.figure(figsize=(6,3))\n",
        "for i, word in enumerate(vocab[:20]): #loop each unique vocab\n",
        "    x, y = get_embed(word)\n",
        "    plt.scatter(x, y)\n",
        "    plt.annotate(word, xy=(x, y), xytext=(5, 2), textcoords='offset points')\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Cosine similarity\n",
        "\n",
        "Formally the [Cosine Similarity](https://en.wikipedia.org/wiki/Cosine_similarity) $s$ between two vectors $p$ and $q$ is defined as:\n",
        "\n",
        "$$s = \\frac{p \\cdot q}{||p|| ||q||}, \\textrm{ where } s \\in [-1, 1] $$ \n",
        "\n",
        "If $p$ and $q$ is super similar, the result is 1 otherwise 0."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['cat', 'apple', 'animal', 'dog', 'banana', 'fruit', '<UNK>']"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vocab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [],
      "source": [
        "#let's try similarity between first and second, and second and third\n",
        "cat          = get_embed('cat')\n",
        "fruit        = get_embed('fruit')\n",
        "animal       = get_embed('animal')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cat vs. fruit:  -0.7370975750771399\n",
            "cat vs. animal:  0.68735597853717\n",
            "cat vs. cat:  1.0\n"
          ]
        }
      ],
      "source": [
        "#numpy version\n",
        "from numpy import dot\n",
        "from numpy.linalg import norm\n",
        "\n",
        "def cos_sim(a, b):\n",
        "    cos_sim = dot(a, b)/(norm(a)*norm(b))\n",
        "    return cos_sim\n",
        "    \n",
        "print(f\"cat vs. fruit: \",        cos_sim(cat, fruit))\n",
        "print(f\"cat vs. animal: \",       cos_sim(cat, animal))\n",
        "print(f\"cat vs. cat: \",          cos_sim(cat, cat))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cat vs. fruit:  -0.73709757507714\n",
            "cat vs. animal:  0.6873559785371702\n",
            "cat vs. cat:  1\n"
          ]
        }
      ],
      "source": [
        "#scipy version\n",
        "from scipy import spatial\n",
        "\n",
        "def cos_sim(a, b):\n",
        "    cos_sim = 1 - spatial.distance.cosine(a, b)  #distance = 1 - similarlity, because scipy only gives distance\n",
        "    return cos_sim\n",
        "\n",
        "print(f\"cat vs. fruit: \",        cos_sim(cat, fruit))\n",
        "print(f\"cat vs. animal: \",       cos_sim(cat, animal))\n",
        "print(f\"cat vs. cat: \",          cos_sim(cat, cat))"
      ]
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "dsai",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "0f2c79af21be9d001248940c049b6176cf8bfb45cabf7aa85848f5cea0f590f6"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
